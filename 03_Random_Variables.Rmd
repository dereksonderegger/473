
# Random Variables and Distributions

## Defining Random Variables and Discrete Distributions

A random variable is a _function_ that takes outcomes in the sample space $\mathcal{S}$ and maps them to numeric values in $\mathbb{R}$. Often times we abbreviate random variable as RV.

The idea is that random events such as flipping Heads, a medical test showing the patient has a disease, Chris Froome winning the Tour de France, or rolling a Leaning Jowler in _Pass the Pigs_ are all random events but to do math on them, we need to turn them into numbers.

In cases where the sample space $\mathcal{S}$ is already numeric, the random variable can just be the identity, but in other cases, we might have to be more careful.  For example, if my experiment is flipping a fair coin $n=4$ times, I could define the random variable $X=$ number of heads and $X$ could take on any of the values $x \in \{0,1,2,3,4\}$.  I could similarly define 

$$Y= \begin{cases} 
  0 \;\;\; \textrm{ if number of heads } < 2 \\ 
  1 \;\;\; \textrm{ if number of heads } > 2 
\end{cases}$$ 
                   
A RV function doesn't have to be one-to-one and it doesn't have to map to the entire set of real values.

Because events in the sample space $\mathcal{S}$ have an associated probability, then it is natural to define an event, say $B$ to be all the outcomes $s \in \mathcal{S}$ such that $X(s) = x$ and then define $Pr(X=x) = Pr(B)$.

Notation: We will refer to the random variable using the capital letters, (e.g. $X$, $Y$, $Z$, $W$) and the possible values they take on using lower case letters. With this notation, the RV $X$ could take on values $x \in \{0,1,2,3,4\}$

1. We consider flipping a fair coin $n=4$ times.
    a) What is the set of outcomes?  As usual, we will define an event for each outcome.
    b) What is the probability of each outcome?
    c) For the RV $X$ defined above, what outcomes define the event $B$ such that $s \in B \implies X(s)=2$?
    d) What is $Pr(B)$? Therefore what is $Pr(X=2)$?
    e) For the RV $Y$ defined above, what outcomes define the event $A$ such that $s \in A \implies Y(s)=1$?
    f) What is $Pr(A)$? Therefore what is $Pr(Y=1)$?
    
For each value that $X$ or $Y$ could take on, we could figure out the probability of the associated event.  We define the random variables _distribution_ as a description of what values the RV can take on and $Pr(X \in C)$, for any interval $C = \{c: a\le c \le b\}$ for any $a<b$.  This is actually a very awkward definition and we will examine more convenient ways to specify these same probabilities.

**Discrete** random variables are RVs that can only take on a finite or countably infinite set of values.

**Continuous** random variables are RVs that can take on an unaccountably infinite set of values.

We now define the _probability function_ of a discrete RV $X$ as 
$$f(x) = Pr(X = x)$$
and the closure of the set $\{x: \textrm{ such that } f(x) > 0\}$ is referred to as the _support of $X$_. Notice that this function is defined for all $x\in \mathbb{R}$, but for only a countable number of cases is $f(x)>0$.

2. Suppose that RV $X$ can take on the values $\{x_1,x_2,\dots,x_K\}$. Prove that $$\sum_{k=1}^K f(x_k) = 1$$

3. **Bernoulli Distribution**.  Suppose that the random variable $W$ takes on the values $0$ and $1$ with the probabilities $Pr(W=1) = p$ and $Pr(W=0) = 1-p$.  Then we say that $W$ has a Bernoulli distribution with probability of success $p$, which I might write as $W \sim Bernoulli(p)$. Show that for any interval $C =\{c: a\le c \le b\}$ in $\mathbb{R}$, you can find $Pr(W \in C)$.

4. **Uniform Distribution on Integers**. Suppose that we have integers $a$ and $b$ such that $a < b$.  Suppose that the RV $X$ is equally likely to any integer $a,\dots,b$. What is $f(x)$? _(Make sure your definition applies to any $x\in\mathbb{R}$)_

5. **Binomial Distribution** Suppose that we have an experiment that consists of $n$ independent Bernoulli($p$) trials. We are interested in the distribution of $X=$ # of successful trials. That is $X\sim Binomial(n,p)$.
    a) For any integer $x \in \{0,1,\dots,n\}$, what is $Pr(X=x)$?
    b) Define $f(x)$ for all values of $x \in \mathbb{R}$.

6. Give two examples of random variables which have Bernoulli($p=1/2$) distributions.  These two RVs should not the same RVs, but they have the same distribution.  That is to say, RVs have distributions, but distributions are not RVs.

7. Suppose that two fair six-sided dice are rolled and the RV of interest is the absolute value of the difference between the dice. Give the probability distribution along with an illuminating graph.

8. Suppose that a box contains 7 red balls and 3 green balls. If five balls are selected at random, without replacement, determine the probability function of $X$ where $X$ is the number of red balls selected.

9. Suppose that a random variable $X$ has a discrete distribution with the following probability function:
$$f(x) = \begin{cases} 
  \frac{c}{2^x} \;\textrm{ for } x = 0, 1, 2, \dots \\
  0 \;\;\;\; \textrm{otherwise}
\end{cases}$$
Find the value for $c$ that forces the requirement that $$\sum_{x=0}^\infty f(x) = 1$$ _Hint: This is a particular power series. Go to any Calculus book (or internet) for an appropriate result. Make sure you introduce the result and show why the result applies to $f(x)$ in your solution._

For the binomial distribution (and many distributions we will consider this semester), it would be nice to not have to calculate various probabilities by hand. Most mathematical software packages include some way to calculate these probabilities.  

|  System     |     Documentation Link or site link                                                |
|:-----------:|:-----------------------------------------------------------------------------------|
| Matlab      |  https://www.mathworks.com/help/stats/working-with-probability-distributions.html  |
| Mathematica |  http://reference.wolfram.com/language/howto/WorkWithStatisticalDistributions.html |
| R           |  https://dereksonderegger.github.io/570L/3-statistical-tables.html                 |
| Web App     |  https://ismay.shinyapps.io/ProbApp/                                               |

10. Each time I ride home or to work, there is a $p=0.1$ probability that I will get stopped by a train.  Let $X$ be the number of times I'm stopped by the train in the next 10 trips. Assume that the probability I'm stopped on trip $i$ is independent of all other trips $j$.
    a) What is the distribution of $X$? Remember, to specify the distribution by name, you must specify the name and the value of all parameters.
    b) What is $Pr(X =6)$?
    c) What is $Pr(X < 6)$?
    d) What is $Pr(X \ge 6)$?


## Continuous Distributions

We wish to define something similar to the probability function for continuous RVs. However, because there are an uncountable number of values that the RV could take on, we have to be careful and define probability on intervals of the form $[a,b]$. We define the _probability density function_ (usually denoted pdf) as the function $f(x)$ such that 
$$Pr( a \le X \le b) = \int_a^b f(x) dx$$

We further define the support of the distribution as the closure of the set ${x: f(x)>0}$. This is the second time we've defined the support as the _closure_ of the set.  In the discrete case, it didn't really matter, but here it does. If we define the pdf on the set $[0,1]$ versus $(0,1)$, we want the support to contain the end points of the interval, that is the support is the closure of $(0,1)$ which is $[0,1]$.

1. Show that $Pr( X = a ) = 0$ for any $a$.  _Hint: What happens as $b-a$ gets small?  Take the limit._

2. Prove that $\int_{-\infty}^\infty f(x) dx = 1$.

Further notice that $f(x)\ge0$ for all $x$ because otherwise that would imply that there are events with negative probabilities.

3. **Continuous Uniform**.  Suppose that the random variable $X$ will be a randomly chosen real value from the interval $[a,b]$. Suppose that for any sub interval $d=[d_1,d_2]$ where $a \le d_1 \le d_2 \le b$, that $Pr( X \in d)$ is proportional to the length of the interval $d$.  What is the pdf of the distribution of $X$?

It is unfortunate that your book doesn't introduce indicator functions at this point in time. We can define the following:
$$I( \textrm{logical test} ) = 
    \begin{cases} 1 \;\;\; \textrm{ if logical test is true } \\
                  0 \;\;\; \textrm{ if logical test is false } \end{cases}$$

3. Suppose that the pdf of a random variable $X$ is $$f(x) = \frac{4}{3}\left(1-x^3\right) \;I(0<x<1)$$
    a) Create a graph of the pdf.
    b) Find $Pr\left( X\le \frac{1}{2} \right)$?
    c) Find $Pr\left( \frac{1}{4} < X < \frac{3}{4} \right)$
    d) Find $Pr\left( X > \frac{3}{4} \right)$

4. Suppose the random variable $X$ has pdf 
$$f(x) = c\cdot e^{-5x}I(x\ge 0) = \begin{cases} c\cdot e^{-5 x} \;\;\; \textrm{ if } x \ge 0 \\
                                          0   \;\;\;\;\;\;\;\;\;\;\;\;\; \textrm{otherwise}
                            \end{cases}$$
    a) What is the value of $c$ that makes this a valid pdf?  That is, what value of $c$ makes this function integrate to 1?
    b) Graph this function. What is $f(0)$? Many students dislike that $f(0)>1$ is greater than one. Why isn't this a problem?
    c) What is the probability $Pr(X \le 0.5)$?
    
5. Suppose that the pdf of the random variable $X$ is
$$f(x)= c \cdot x \;\;I(0\le x \le 3)$$
    a) What value of $c$ makes this a valid pdf?
    b) Find a value of $t$ such that $Pr( X \le t ) = 0.25)$
    c) Find a value of $t$ such that $Pr( X > t) = 0.5)$

6. Given the same random variable $X$ described in problem 3.2.5, consider the random variable $Y$ which is the simply the nearest integer to $X$. What is the pf of $Y$?



We now have defined both the probability function (pf) for discrete random variables and probability density function (pdf) for continuous random variables.  We now try to make a physics analogy to describe the difference between the two. In both cases we want to think about probability as physical mass.  

**Discrete Case** In this case, all the probability mass is concentrated at precise points.  So we have little nuggets of probability mass, at discrete points.  If I want to know, say $Pr(X \le 3)$ then we have to sum the probability across all the discrete locations such that $X \le 3$.

**Continuous Case** In this case, the probability mass is spread out across $\mathbb{R}$ and the concentration of mass is not uniform, some spots have more concentrated mass.  In this case, we don't have a definiate amount of mass at a particular point, but we do have a description of how dense the mass is at any point.  In this case if we want to know $Pr(X \le 3)$ then we have to break up $\mathbb{R}$ into a bunch of intervals and then combine the length of the interval along with information about the average density of each interval.

```{r, message=FALSE, warning=FALSE, echo=FALSE, fig.height=3}
library(ggplot2); library(dplyr);
curve.data <- data.frame( x = seq(-3, 7, by=0.01) ) %>%
  mutate( density = dnorm(x, mean=2, sd=1))
hist.data <- data.frame( x = seq(-3.25, 2.75, by=0.5) ) %>%
  mutate(density = dnorm(x, mean=2, sd=1))

ggplot(curve.data, aes(x=x, y=density)) + 
  geom_line() +
  geom_bar(data=hist.data, stat='identity', width=.5, alpha=.3, color='red') +
  scale_x_continuous(breaks=-3:7, minor=NULL)  
```
    
Each bar has some probability mass (mass is the length of the interval times the average density along the interval) and then we just sum up the bars.  If we take the limit as we make the bars narrower, then we end up with 
$$Pr(X \le 3) = \int_{-\infty}^3 f(x) \, dx$$

You might be a little freaked out because typically you would think about mass being the _area_ or _volume_ times the density, but we need to start in the 1-dimension case before we address the 2-dimension and 3-dimension cases.


## Cumulative Distribution Function


