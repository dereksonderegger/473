<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>STA 473 - Probability</title>
  <meta name="description" content="STA 473 - Probability">
  <meta name="generator" content="bookdown 0.5 and GitBook 2.6.7">

  <meta property="og:title" content="STA 473 - Probability" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="dereksonderegger/474" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="STA 473 - Probability" />
  
  
  

<meta name="author" content="Derek L. Sonderegger">


<meta name="date" content="2018-01-17">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="4-expectations.html">
<link rel="next" href="A-gamma-function.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="1-introduction-to-probability.html"><a href="1-introduction-to-probability.html"><i class="fa fa-check"></i><b>1</b> Introduction to Probability</a><ul>
<li class="chapter" data-level="1.1" data-path="1-introduction-to-probability.html"><a href="1-introduction-to-probability.html#history-of-probability"><i class="fa fa-check"></i><b>1.1</b> History of Probability</a></li>
<li class="chapter" data-level="1.2" data-path="1-introduction-to-probability.html"><a href="1-introduction-to-probability.html#interpretations-of-probability"><i class="fa fa-check"></i><b>1.2</b> Interpretations of Probability</a></li>
<li class="chapter" data-level="1.3" data-path="1-introduction-to-probability.html"><a href="1-introduction-to-probability.html#experiments-and-events"><i class="fa fa-check"></i><b>1.3</b> Experiments and Events</a></li>
<li class="chapter" data-level="1.4" data-path="1-introduction-to-probability.html"><a href="1-introduction-to-probability.html#review-of-set-theory-ds-1.4"><i class="fa fa-check"></i><b>1.4</b> Review of Set Theory (D&amp;S 1.4)</a></li>
<li class="chapter" data-level="1.5" data-path="1-introduction-to-probability.html"><a href="1-introduction-to-probability.html#definition-of-probability-ds-1.5"><i class="fa fa-check"></i><b>1.5</b> Definition of Probability (D&amp;S 1.5)</a></li>
<li class="chapter" data-level="1.6" data-path="1-introduction-to-probability.html"><a href="1-introduction-to-probability.html#finite-sample-spaces-ds-1.6"><i class="fa fa-check"></i><b>1.6</b> Finite Sample Spaces (D&amp;S 1.6)</a></li>
<li class="chapter" data-level="1.7" data-path="1-introduction-to-probability.html"><a href="1-introduction-to-probability.html#ordered-counting-ds-1.7"><i class="fa fa-check"></i><b>1.7</b> Ordered Counting (D&amp;S 1.7)</a></li>
<li class="chapter" data-level="1.8" data-path="1-introduction-to-probability.html"><a href="1-introduction-to-probability.html#combinations-ds-1.8"><i class="fa fa-check"></i><b>1.8</b> Combinations (D&amp;S 1.8)</a></li>
<li class="chapter" data-level="1.9" data-path="1-introduction-to-probability.html"><a href="1-introduction-to-probability.html#multinomial-coefficients-ds-1.9"><i class="fa fa-check"></i><b>1.9</b> Multinomial Coefficients (D&amp;S 1.9)</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-conditional-probability.html"><a href="2-conditional-probability.html"><i class="fa fa-check"></i><b>2</b> Conditional Probability</a><ul>
<li class="chapter" data-level="2.1" data-path="2-conditional-probability.html"><a href="2-conditional-probability.html#defining-conditional-probability-ds-2.1"><i class="fa fa-check"></i><b>2.1</b> Defining Conditional Probability (D&amp;S 2.1)</a></li>
<li class="chapter" data-level="2.2" data-path="2-conditional-probability.html"><a href="2-conditional-probability.html#independence"><i class="fa fa-check"></i><b>2.2</b> Independence</a></li>
<li class="chapter" data-level="2.3" data-path="2-conditional-probability.html"><a href="2-conditional-probability.html#bayes-theorem"><i class="fa fa-check"></i><b>2.3</b> Bayes’ Theorem</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html"><i class="fa fa-check"></i><b>3</b> Random Variables and Distributions</a><ul>
<li class="chapter" data-level="3.1" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#defining-random-variables-and-discrete-distributions"><i class="fa fa-check"></i><b>3.1</b> Defining Random Variables and Discrete Distributions</a></li>
<li class="chapter" data-level="3.2" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#continuous-distributions"><i class="fa fa-check"></i><b>3.2</b> Continuous Distributions</a></li>
<li class="chapter" data-level="3.3" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#cumulative-distribution-function"><i class="fa fa-check"></i><b>3.3</b> Cumulative Distribution Function</a></li>
<li class="chapter" data-level="3.4" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#bivariate-distributions"><i class="fa fa-check"></i><b>3.4</b> Bivariate Distributions</a><ul>
<li class="chapter" data-level="3.4.1" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#bivariate-discrete"><i class="fa fa-check"></i><b>3.4.1</b> Bivariate Discrete</a></li>
<li class="chapter" data-level="3.4.2" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#bivariate-continuous"><i class="fa fa-check"></i><b>3.4.2</b> Bivariate Continuous</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#marginal-distributions"><i class="fa fa-check"></i><b>3.5</b> Marginal Distributions</a><ul>
<li class="chapter" data-level="3.5.1" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#discrete-case"><i class="fa fa-check"></i><b>3.5.1</b> Discrete Case</a></li>
<li class="chapter" data-level="3.5.2" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#continuous-case"><i class="fa fa-check"></i><b>3.5.2</b> Continuous Case</a></li>
<li class="chapter" data-level="3.5.3" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#independence-1"><i class="fa fa-check"></i><b>3.5.3</b> Independence</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#conditional-distributions"><i class="fa fa-check"></i><b>3.6</b> Conditional Distributions</a></li>
<li class="chapter" data-level="3.7" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#functions-of-random-variables"><i class="fa fa-check"></i><b>3.7</b> Functions of Random Variables</a><ul>
<li class="chapter" data-level="3.7.1" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#cdf-method"><i class="fa fa-check"></i><b>3.7.1</b> CDF Method</a></li>
<li class="chapter" data-level="3.7.2" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#pdf-method"><i class="fa fa-check"></i><b>3.7.2</b> pdf Method</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#multivariate-transformations"><i class="fa fa-check"></i><b>3.8</b> Multivariate Transformations</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-expectations.html"><a href="4-expectations.html"><i class="fa fa-check"></i><b>4</b> Expectations</a><ul>
<li class="chapter" data-level="4.1" data-path="4-expectations.html"><a href="4-expectations.html#expectation-of-a-rv"><i class="fa fa-check"></i><b>4.1</b> Expectation of a RV</a></li>
<li class="chapter" data-level="4.2" data-path="4-expectations.html"><a href="4-expectations.html#properties-of-expectations"><i class="fa fa-check"></i><b>4.2</b> Properties of Expectations</a></li>
<li class="chapter" data-level="4.3" data-path="4-expectations.html"><a href="4-expectations.html#variance"><i class="fa fa-check"></i><b>4.3</b> Variance</a></li>
<li class="chapter" data-level="4.4" data-path="4-expectations.html"><a href="4-expectations.html#moments-and-moment-generating-functions"><i class="fa fa-check"></i><b>4.4</b> Moments and Moment Generating Functions</a></li>
<li class="chapter" data-level="4.5" data-path="4-expectations.html"><a href="4-expectations.html#mean-vs-median"><i class="fa fa-check"></i><b>4.5</b> Mean vs Median</a></li>
<li class="chapter" data-level="4.6" data-path="4-expectations.html"><a href="4-expectations.html#covariance-and-correlation"><i class="fa fa-check"></i><b>4.6</b> Covariance and Correlation</a></li>
<li class="chapter" data-level="4.7" data-path="4-expectations.html"><a href="4-expectations.html#conditional-expectation"><i class="fa fa-check"></i><b>4.7</b> Conditional Expectation</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-common-distributions.html"><a href="5-common-distributions.html"><i class="fa fa-check"></i><b>5</b> Common Distributions</a><ul>
<li class="chapter" data-level="5.1" data-path="5-common-distributions.html"><a href="5-common-distributions.html#introduction"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="5-common-distributions.html"><a href="5-common-distributions.html#bernoulli-and-binomial"><i class="fa fa-check"></i><b>5.2</b> Bernoulli and Binomial</a></li>
<li class="chapter" data-level="5.3" data-path="5-common-distributions.html"><a href="5-common-distributions.html#hypergeometric"><i class="fa fa-check"></i><b>5.3</b> Hypergeometric</a></li>
<li class="chapter" data-level="5.4" data-path="5-common-distributions.html"><a href="5-common-distributions.html#poisson"><i class="fa fa-check"></i><b>5.4</b> Poisson</a></li>
<li class="chapter" data-level="5.5" data-path="5-common-distributions.html"><a href="5-common-distributions.html#geometric-and-negative-binomial"><i class="fa fa-check"></i><b>5.5</b> Geometric and Negative Binomial</a></li>
<li class="chapter" data-level="5.6" data-path="5-common-distributions.html"><a href="5-common-distributions.html#normal"><i class="fa fa-check"></i><b>5.6</b> Normal</a></li>
<li class="chapter" data-level="5.7" data-path="5-common-distributions.html"><a href="5-common-distributions.html#uniform"><i class="fa fa-check"></i><b>5.7</b> Uniform</a></li>
<li class="chapter" data-level="5.8" data-path="5-common-distributions.html"><a href="5-common-distributions.html#exponential-and-gamma"><i class="fa fa-check"></i><b>5.8</b> Exponential and Gamma</a></li>
<li class="chapter" data-level="5.9" data-path="5-common-distributions.html"><a href="5-common-distributions.html#beta"><i class="fa fa-check"></i><b>5.9</b> Beta</a></li>
<li class="chapter" data-level="5.10" data-path="5-common-distributions.html"><a href="5-common-distributions.html#bivariate-normal"><i class="fa fa-check"></i><b>5.10</b> Bivariate Normal</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="A-gamma-function.html"><a href="A-gamma-function.html"><i class="fa fa-check"></i><b>A</b> Gamma Function</a></li>
<li class="chapter" data-level="B" data-path="B-useful-series-results.html"><a href="B-useful-series-results.html"><i class="fa fa-check"></i><b>B</b> Useful Series Results</a><ul>
<li class="chapter" data-level="B.1" data-path="B-useful-series-results.html"><a href="B-useful-series-results.html#ex"><i class="fa fa-check"></i><b>B.1</b> <span class="math inline">\(e^x\)</span></a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="C-tedious-results.html"><a href="C-tedious-results.html"><i class="fa fa-check"></i><b>C</b> Tedious results</a><ul>
<li class="chapter" data-level="C.1" data-path="C-tedious-results.html"><a href="C-tedious-results.html#normal-distribution"><i class="fa fa-check"></i><b>C.1</b> Normal distribution</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">STA 473 - Probability</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="common-distributions" class="section level1">
<h1><span class="header-section-number">Chapter 5</span> Common Distributions</h1>
<div id="introduction" class="section level2">
<h2><span class="header-section-number">5.1</span> Introduction</h2>
<p>No Problems. Really!</p>
</div>
<div id="bernoulli-and-binomial" class="section level2">
<h2><span class="header-section-number">5.2</span> Bernoulli and Binomial</h2>

<div class="definition">
<span id="def:unnamed-chunk-23" class="definition"><strong>Definition 5.1  </strong></span>Define a Bernoulli random variable as a variable that takes on the value 0 with probability <span class="math inline">\(1-p\)</span> and the value 1 with probability <span class="math inline">\(p\)</span> where <span class="math inline">\(0\le p \le 1\)</span>. We will write this as <span class="math inline">\(X\sim Bernoulli( p )\)</span>.
</div>
<p> The pf of a Bernoulli random variable is written as <span class="math display">\[f(x) = p^x (1-p)^{1-x} \;\; \cdot I(x \in \{0,1\})\]</span></p>
<ol style="list-style-type: decimal">
<li>Show that if <span class="math inline">\(X\sim Bernoulli(p)\)</span> then
<ol style="list-style-type: lower-alpha">
<li>The expectation of <span class="math inline">\(X\)</span> is <span class="math inline">\(E(X) = p\)</span></li>
<li>The variance of <span class="math inline">\(X\)</span> is <span class="math inline">\(Var(X) = p(1-p)\)</span></li>
<li>The moment generating function of <span class="math inline">\(X\)</span> is <span class="math inline">\(\psi(t)=pe^t + (1-p)\)</span></li>
</ol></li>
</ol>

<div class="definition">
<span id="def:unnamed-chunk-24" class="definition"><strong>Definition 5.2  </strong></span>Define a Binomial random variable as the sum of <span class="math inline">\(n\)</span> independent and identically distributed Bernoulli(p) random variables. We will write <span class="math inline">\(X \sim Bin(n,p)\)</span>
</div>
<p> The pf of a Binomial random variable is <span class="math display">\[f(x)= {n \choose x}p^x (1-p)^{n-x} \;\;\cdot I(x\in \{0,1,2,\dots,n\})\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>Show that if <span class="math inline">\(X \sim Bin(n,p)\)</span> then
<ol style="list-style-type: lower-alpha">
<li>The expectation of <span class="math inline">\(X\)</span> is <span class="math inline">\(E(X) = np\)</span></li>
<li>The variance of <span class="math inline">\(X\)</span> is <span class="math inline">\(Var(X) = np(1-p)\)</span></li>
<li>The moment generating function of <span class="math inline">\(X\)</span> is <span class="math inline">\(\left( pe^t + (1-p) \right)^n\)</span></li>
</ol></li>
</ol>
</div>
<div id="hypergeometric" class="section level2">
<h2><span class="header-section-number">5.3</span> Hypergeometric</h2>
<p>We next consider a distribution that is the sum of <em>dependent</em> Bernoulli random variables. Consider the scenario where there are <span class="math inline">\(A\)</span> balls that are <em>amber</em> and <span class="math inline">\(B\)</span> balls that are <em>blue</em>. The balls are thoroughly mixed and we will select <span class="math inline">\(n\)</span> of those <em>without replacement</em>. Of interest is <span class="math inline">\(Y\)</span>, the number of amber balls drawn. It is helpful to think of randomly arranging all <span class="math inline">\(A+B\)</span> balls into some order and then selecting the first <span class="math inline">\(n\)</span> balls. Define <span class="math inline">\(X_i=1\)</span> if the <span class="math inline">\(i\)</span>th ball is amber and <span class="math inline">\(X_i=0\)</span> if the <span class="math inline">\(i\)</span>th ball is blue. Finally we note that <span class="math inline">\(X_i\)</span> is <em>not</em> independent of <span class="math inline">\(X_j\)</span> and that <span class="math display">\[Y=\sum_{i=1}^n X_i\]</span></p>
<ol style="list-style-type: decimal">
<li>Derive the pf of <span class="math inline">\(Y \sim Hypergeometric(A,B,n)\)</span>.
<ol style="list-style-type: lower-alpha">
<li>How many ways are there to draw, without replacement, <span class="math inline">\(n\)</span> balls from <span class="math inline">\(A+B\)</span> when order doesn’t matter?</li>
<li>How many ways are there to draw <span class="math inline">\(x\)</span> amber balls and <span class="math inline">\(n-x\)</span> blue balls (assuming <span class="math inline">\(x\le A\)</span> and <span class="math inline">\(n-x \le B\)</span>)?</li>
<li>Give the pf of a Hypergeometric(A,B,n) distribution.</li>
</ol></li>
<li><p>Notice that absent any information about the other <span class="math inline">\(X_j\)</span> balls, <span class="math inline">\(X_i \sim Bernoulli \Big( \frac{A}{A+B} \Big)\)</span>. Use this information to derive the expectation of <span class="math inline">\(Y\)</span>.</p></li>
<li>Because <span class="math inline">\(X_i\)</span> is negatively correlated with <span class="math inline">\(X_j\)</span>, we can’t easily derive the variance of <span class="math inline">\(Y\)</span>. Instead we will be <em>obnoxiously</em> clever.
<ol style="list-style-type: lower-alpha">
<li>Let <span class="math inline">\(n=A+B\)</span> so that we are selecting all the balls. Argue that <span class="math inline">\(Var(Y) = 0\)</span>.</li>
<li>Because of the symmetry of the random assignments of balls, then <span class="math inline">\(Cov(X_i,X_j)\)</span> is the same for all <span class="math inline">\(i\ne j\)</span>. <em>There isn’t anything thing to do here, but this part of our arguement is critical.</em></li>
<li>We know that for any set of random variables <span class="math display">\[Var(Y) = \sum_{i=1}^n Var(X_i) + \sum_{i \ne j} Cov(X_i,X_j)\]</span> Use this information, along with parts (a) and (b), to solve for the <span class="math inline">\(Cov(X_i, X_j)\)</span>. <em>The hard part here is figuring out how many covariance terms there are.</em></li>
<li>Finally, show that <span class="math display">\[Var(Y) = \frac{nAB}{(A+B)^2}\cdot \frac{A+B-n}{A+B-1}\]</span></li>
</ol></li>
<li><p>Suppose that we have <span class="math inline">\(Y \sim Hypergeometric(A,B,n)\)</span> and separately we have <span class="math inline">\(W \sim Bernoulli \Big( n, \frac{A}{A+B} \Big)\)</span>. Show that for <span class="math inline">\(n &gt; 1\)</span> that <span class="math inline">\(Var(W) &gt; Var(Y)\)</span>.</p></li>
<li>My daughter recently mixed <span class="math inline">\(20\)</span> M&amp;Ms and <span class="math inline">\(30\)</span> Skittles in a bowl and left them for me to find.
<ol style="list-style-type: lower-alpha">
<li>What is the probability that I select <strong>only</strong> M&amp;Ms when I select 6 pieces of candy?</li>
<li>What is the expected number of M&amp;Ms in the next 6 pieces (from the 50)?</li>
</ol></li>
</ol>
</div>
<div id="poisson" class="section level2">
<h2><span class="header-section-number">5.4</span> Poisson</h2>
<p>The Poisson distribution is used to model the number of events that happen in some unit time. The critical idea is that the number of events that occur in any two disjoint time periods are independent, regardless of the length of the period. By splitting the time unit into <em>many</em> sub-intervals, each of which that could only have 0 or 1 event and considering those sub-intervals as independent Bernoulli RVs, it is possible to justify the following probability function <span class="math display">\[f(x) = \frac{e^{-\lambda} \lambda^x}{x!} \;\;\cdot I(x \in \{0,1,2,\dots\} )\]</span> where the parameter <span class="math inline">\(\lambda\)</span> represents the mean number of events that should happen per time interval of some specific size.</p>
<ol style="list-style-type: decimal">
<li>Suppose that <span class="math inline">\(X\sim Poisson(\lambda)\)</span>. Show that
<ol style="list-style-type: lower-alpha">
<li>This is a valid pf by showing that <span class="math inline">\(f(x) \ge 0\)</span> for all <span class="math inline">\(x\)</span> and that <span class="math inline">\(\sum_{x=0}^\infty f(x) = 1\)</span>. <em>Hint, look at the series expansion of <span class="math inline">\(e^\lambda\)</span></em>.</li>
<li>The expectation of <span class="math inline">\(X\)</span> is <span class="math inline">\(E(X)=\lambda\)</span>.</li>
<li>The variance of <span class="math inline">\(X\)</span> is <span class="math inline">\(Var(X)=\lambda\)</span>. <em>Hint, derive <span class="math inline">\(E[ X(X-1) ]\)</span> and use that to figure out <span class="math inline">\(E[X^2]\)</span>.</em></li>
<li>The moment generating function of <span class="math inline">\(X\)</span> is <span class="math display">\[\psi(t)=e^{\lambda(e^t-1)}\]</span></li>
</ol></li>
<li>Show that if <span class="math inline">\(X_1,\dots,X_n\)</span> are independent and identically distributed <span class="math inline">\(Poisson(\lambda)\)</span> random variables, which we denote as <span class="math display">\[X_i \stackrel{iid}{\sim} Poisson(\lambda)\]</span> then <span class="math display">\[Y=\sum(X_i) \sim Poisson(n\lambda).\]</span></li>
</ol>
</div>
<div id="geometric-and-negative-binomial" class="section level2">
<h2><span class="header-section-number">5.5</span> Geometric and Negative Binomial</h2>
<p>We will first define the <em>geometric</em> distribution. Here we consider another version of multiple Bernoulli random variables. This time, we consider an experiment where we repeatedly sample from a <span class="math inline">\(Bernoulli(p)\)</span> distribution, where <span class="math inline">\(p\)</span> is the probability the draw was a success, and each draw is independent of all previous draws. We are interested in <em>“the number of failures before the first success.”</em></p>
<ol style="list-style-type: decimal">
<li>We first consider <span class="math inline">\(Y \sim Geometric(p)\)</span>.
<ol style="list-style-type: lower-alpha">
<li>What is the probability that there are no failures? That is, what is the probability that the first success occurs on the first draw. <span class="math display">\[f(0) = Pr( Y = 0 ) = \;\;?\]</span></li>
<li>What is the probability that there is one failure followed by a success? What is the probability that there are <span class="math inline">\(y\)</span> failures before the first success?</li>
<li>Use this to derive the pf of a <span class="math inline">\(Geometric(p)\)</span> distribution.</li>
</ol></li>
<li><p>Show that the Moment Generating Function for the <span class="math inline">\(Geometric(p)\)</span> distribution is <span class="math display">\[\psi(t) = \frac{p}{1-(1-p)e^t}\]</span> <em>Hint, the Geometric distribution is named as such because the Geometric Series result is necessary to show this.</em></p></li>
<li><p>Utilize the mfg to derive the expected value and variance of a <span class="math inline">\(Geometric(p)\)</span> distribution.</p></li>
</ol>
<p>The Negative Binomial distribution extends the idea of “number of failures until the first success” to the number of failures until the <span class="math inline">\(r\)</span>th success.</p>
<ol start="4" style="list-style-type: decimal">
<li>The pf of the <span class="math inline">\(Y \sim Negative \;Binomial(r,p)\)</span> distribution can be derived with the following:
<ol style="list-style-type: lower-alpha">
<li>What is the probability of observing exactly <span class="math inline">\(y\)</span> failures in a row, followed by <span class="math inline">\(r\)</span> successes?</li>
<li>How many ways are there to distribute the <span class="math inline">\(r\)</span> successes among the <span class="math inline">\(r+y\)</span> draws, keeping in mind that the final draw must be a success?</li>
<li>Use the above ideas to derive the pf.</li>
</ol></li>
<li>A second way of thinking about the negative binomial distribution is as the sum of <span class="math inline">\(r\)</span> independent Geometric random variables. Utilize this interpretation to derive the expectation, variance, and moment generating function of a <span class="math inline">\(Negative \;Binomial(r,p)\)</span> distribution.</li>
</ol>
<p><em>Some books parameterize the geometric (and negative binomial) distributions as the total number of draws before the first (or <span class="math inline">\(n\)</span>th) success. Whenever you are looking up properties of this distribution, make sure it is defined how you want it. For example, the wikipedia page for the geometric (and negative binomial) have the information for both definitions.</em></p>
</div>
<div id="normal" class="section level2">
<h2><span class="header-section-number">5.6</span> Normal</h2>
<p>The normal distribution plays a central role in statistics because there are many many asymptotic results that show that some quantity converges to a normal distribution.</p>
<p>The normal distribution is defined by it’s expectation <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span> and has pdf <span class="math display">\[f(x) = \frac{1}{\sqrt{2\pi}\sigma} \exp \left[ -\frac{(x-\mu)^2}{2\sigma^2} \right]\]</span> where <span class="math inline">\(\exp [w] = e^w\)</span> is just a notational convenience. Notice that we have no indicator function, and <span class="math inline">\(f(x) &gt; 0\)</span> for all <span class="math inline">\(x\in \mathbb{R}\)</span>.</p>
<p>One of the most tedious result to derive is that the pdf integrates to one (see the appendix) but can easily be done using the appropriate polar-coordinate transformation.</p>
<p>For our first result, we will derive the moment generating function.</p>
<p><span class="math display">\[\begin{aligned} \psi(t) 
  &amp;= E(e^{tX}) \\
  &amp;= \int_{-\infty}^\infty e^{tx} \; \frac{1}{\sqrt{2\pi}\sigma} \exp \left[ -\frac{(x-\mu)^2}{2\sigma^2} \right] \,dx\\
  &amp;= \int_{-\infty}^\infty \frac{1}{\sqrt{2\pi}\sigma} \exp \left[ tx -\frac{(x-\mu)^2}{2\sigma^2} \right] \,dx\\
  \end{aligned}\]</span> We then expand the square twice in the exponent by adding and subtracting the term <span class="math inline">\(\mu t + \frac{1}{2}\sigma^2 t^2\)</span> and then re-arranging to find</p>
<p><span class="math display">\[\begin{aligned} \psi(t) 
  &amp;= \int_{-\infty}^\infty \frac{1}{\sqrt{2\pi}\sigma} \exp \left[ \mu t + \frac{1}{2} \sigma^2 t^2 - \frac{\Big(x-(\mu+\sigma^2t)\Big)^2}{2\sigma^2} \right] \,dx\\
  &amp;= \exp \left[ \mu t + \frac{1}{2} \sigma^2 t^2 \right]  \int_{-\infty}^\infty \frac{1}{\sqrt{2\pi}\sigma} \exp \left[ - \frac{\Big(x-(\mu+\sigma^2t)\Big)^2}{2\sigma^2} \right] \,dx \\
  &amp;= \exp \left[ \mu t + \frac{1}{2} \sigma^2 t^2 \right]
  \end{aligned}\]</span></p>
<ol style="list-style-type: decimal">
<li><p>Show that if <span class="math inline">\(X \sim N(\mu, \sigma^2)\)</span>, then <span class="math inline">\(E(X) = \mu\)</span>. <em>Hint, use the mgf.</em></p></li>
<li><p>Show that if <span class="math inline">\(X \sim N(\mu, \sigma^2)\)</span>, then <span class="math inline">\(Var(X) = \sigma^2\)</span>. <em>Hint, use the mgf.</em></p></li>
<li><p>Show that if <span class="math inline">\(X \sim N(\mu, \sigma^2)\)</span>, then <span class="math inline">\(Y=aX + b\)</span> has distribution <span class="math inline">\(Y\sim N(a\mu+b, \,a^2 \sigma^2)\)</span>.</p></li>
<li><p>Show that if <span class="math inline">\(X_i \sim N(\mu_i,\, \sigma^2_i)\)</span> for <span class="math inline">\(i \in \{1,2,\dots,n\}\)</span> where <span class="math inline">\(X_i\)</span> are all independent, then <span class="math inline">\(Y=\sum X_i\)</span> has distribution <span class="math inline">\(Y \sim N\left( \sum \mu_i, \sum \sigma^2_i \right)\)</span></p></li>
</ol>
</div>
<div id="uniform" class="section level2">
<h2><span class="header-section-number">5.7</span> Uniform</h2>
<p>The continuous uniform distribution on the interval <span class="math inline">\((a,b)\)</span> where <span class="math inline">\(a&lt;b\)</span> has a density function <span class="math display">\[f(x) = \frac{1}{b-a} \;\cdot I(a &lt; x &lt; b)\]</span> 1. Show that if <span class="math inline">\(X\sim Unif(a,b)\)</span> then <span class="math inline">\(E(X) = \frac{1}{b-a}\)</span></p>
<ol start="2" style="list-style-type: decimal">
<li><p>Show that the moment generating function of a <span class="math inline">\(Uniform(a,b)\)</span> distribution is <span class="math display">\[\psi(t) = \frac{e^{bt} - e^{at}}{t(b-a)}\]</span></p></li>
<li><p>Show that if <span class="math inline">\(U \sim Uniform(0,1)\)</span> then <span class="math inline">\(X=(b-a)U + a\)</span> has a <span class="math inline">\(Uniform(a,b)\)</span> distribution.</p></li>
<li><p>Show that the <span class="math inline">\(Uniform(0,1)\)</span> distribution has expectation <span class="math inline">\(\frac{1}{2}\)</span> and variance <span class="math inline">\(\frac{1}{12}\)</span>.</p></li>
<li><p>Show that the <span class="math inline">\(Uniform(a,b)\)</span> distribution has expectation <span class="math inline">\(\frac{a+b}{2}\)</span> and variance <span class="math inline">\(\frac{(b-a)^2}{12}\)</span></p></li>
</ol>
</div>
<div id="exponential-and-gamma" class="section level2">
<h2><span class="header-section-number">5.8</span> Exponential and Gamma</h2>
</div>
<div id="beta" class="section level2">
<h2><span class="header-section-number">5.9</span> Beta</h2>
</div>
<div id="bivariate-normal" class="section level2">
<h2><span class="header-section-number">5.10</span> Bivariate Normal</h2>

</div>
</div>



            </section>

          </div>
        </div>
      </div>
<a href="4-expectations.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="A-gamma-function.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/dereksonderegger/edit/master/474/05_Common_Distributions.Rmd",
"text": "Edit"
},
"download": [["Probability.pdf", "PDF"], ["Probability.epub", "EPUB"]],
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
