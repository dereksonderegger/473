<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>STA 473 - Probability</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="STA 473 - Probability">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="STA 473 - Probability" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="dereksonderegger/473" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="STA 473 - Probability" />
  
  
  

<meta name="author" content="Derek L. Sonderegger">


<meta name="date" content="2017-04-24">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="3-random-variables-and-distributions.html">
<link rel="next" href="5-common-distributions.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="1-introduction-to-probability.html"><a href="1-introduction-to-probability.html"><i class="fa fa-check"></i><b>1</b> Introduction to Probability</a><ul>
<li class="chapter" data-level="1.1" data-path="1-introduction-to-probability.html"><a href="1-introduction-to-probability.html#history-of-probability"><i class="fa fa-check"></i><b>1.1</b> History of Probability</a></li>
<li class="chapter" data-level="1.2" data-path="1-introduction-to-probability.html"><a href="1-introduction-to-probability.html#interpretations-of-probability"><i class="fa fa-check"></i><b>1.2</b> Interpretations of Probability</a></li>
<li class="chapter" data-level="1.3" data-path="1-introduction-to-probability.html"><a href="1-introduction-to-probability.html#experiments-and-events"><i class="fa fa-check"></i><b>1.3</b> Experiments and Events</a></li>
<li class="chapter" data-level="1.4" data-path="1-introduction-to-probability.html"><a href="1-introduction-to-probability.html#review-of-set-theory-ds-1.4"><i class="fa fa-check"></i><b>1.4</b> Review of Set Theory (D&amp;S 1.4)</a></li>
<li class="chapter" data-level="1.5" data-path="1-introduction-to-probability.html"><a href="1-introduction-to-probability.html#definition-of-probability-ds-1.5"><i class="fa fa-check"></i><b>1.5</b> Definition of Probability (D&amp;S 1.5)</a></li>
<li class="chapter" data-level="1.6" data-path="1-introduction-to-probability.html"><a href="1-introduction-to-probability.html#finite-sample-spaces-ds-1.6"><i class="fa fa-check"></i><b>1.6</b> Finite Sample Spaces (D&amp;S 1.6)</a></li>
<li class="chapter" data-level="1.7" data-path="1-introduction-to-probability.html"><a href="1-introduction-to-probability.html#ordered-counting-ds-1.7"><i class="fa fa-check"></i><b>1.7</b> Ordered Counting (D&amp;S 1.7)</a></li>
<li class="chapter" data-level="1.8" data-path="1-introduction-to-probability.html"><a href="1-introduction-to-probability.html#combinations-ds-1.8"><i class="fa fa-check"></i><b>1.8</b> Combinations (D&amp;S 1.8)</a></li>
<li class="chapter" data-level="1.9" data-path="1-introduction-to-probability.html"><a href="1-introduction-to-probability.html#multinomial-coefficients-ds-1.9"><i class="fa fa-check"></i><b>1.9</b> Multinomial Coefficients (D&amp;S 1.9)</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-conditional-probability.html"><a href="2-conditional-probability.html"><i class="fa fa-check"></i><b>2</b> Conditional Probability</a><ul>
<li class="chapter" data-level="2.1" data-path="2-conditional-probability.html"><a href="2-conditional-probability.html#defining-conditional-probability-ds-2.1"><i class="fa fa-check"></i><b>2.1</b> Defining Conditional Probability (D&amp;S 2.1)</a></li>
<li class="chapter" data-level="2.2" data-path="2-conditional-probability.html"><a href="2-conditional-probability.html#independence"><i class="fa fa-check"></i><b>2.2</b> Independence</a></li>
<li class="chapter" data-level="2.3" data-path="2-conditional-probability.html"><a href="2-conditional-probability.html#bayes-theorem"><i class="fa fa-check"></i><b>2.3</b> Bayes’ Theorem</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html"><i class="fa fa-check"></i><b>3</b> Random Variables and Distributions</a><ul>
<li class="chapter" data-level="3.1" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#defining-random-variables-and-discrete-distributions"><i class="fa fa-check"></i><b>3.1</b> Defining Random Variables and Discrete Distributions</a></li>
<li class="chapter" data-level="3.2" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#continuous-distributions"><i class="fa fa-check"></i><b>3.2</b> Continuous Distributions</a></li>
<li class="chapter" data-level="3.3" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#cumulative-distribution-function"><i class="fa fa-check"></i><b>3.3</b> Cumulative Distribution Function</a></li>
<li class="chapter" data-level="3.4" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#bivariate-distributions"><i class="fa fa-check"></i><b>3.4</b> Bivariate Distributions</a><ul>
<li class="chapter" data-level="3.4.1" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#bivariate-discrete"><i class="fa fa-check"></i><b>3.4.1</b> Bivariate Discrete</a></li>
<li class="chapter" data-level="3.4.2" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#bivariate-continuous"><i class="fa fa-check"></i><b>3.4.2</b> Bivariate Continuous</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#marginal-distributions"><i class="fa fa-check"></i><b>3.5</b> Marginal Distributions</a><ul>
<li class="chapter" data-level="3.5.1" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#discrete-case"><i class="fa fa-check"></i><b>3.5.1</b> Discrete Case</a></li>
<li class="chapter" data-level="3.5.2" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#continuous-case"><i class="fa fa-check"></i><b>3.5.2</b> Continuous Case</a></li>
<li class="chapter" data-level="3.5.3" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#independence-1"><i class="fa fa-check"></i><b>3.5.3</b> Independence</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#conditional-distributions"><i class="fa fa-check"></i><b>3.6</b> Conditional Distributions</a></li>
<li class="chapter" data-level="3.7" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#functions-of-random-variables"><i class="fa fa-check"></i><b>3.7</b> Functions of Random Variables</a><ul>
<li class="chapter" data-level="3.7.1" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#cdf-method"><i class="fa fa-check"></i><b>3.7.1</b> CDF Method</a></li>
<li class="chapter" data-level="3.7.2" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#pdf-method"><i class="fa fa-check"></i><b>3.7.2</b> pdf Method</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#multivariate-transformations"><i class="fa fa-check"></i><b>3.8</b> Multivariate Transformations</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-expectations.html"><a href="4-expectations.html"><i class="fa fa-check"></i><b>4</b> Expectations</a><ul>
<li class="chapter" data-level="4.1" data-path="4-expectations.html"><a href="4-expectations.html#expectation-of-a-rv"><i class="fa fa-check"></i><b>4.1</b> Expectation of a RV</a></li>
<li class="chapter" data-level="4.2" data-path="4-expectations.html"><a href="4-expectations.html#properties-of-expectations"><i class="fa fa-check"></i><b>4.2</b> Properties of Expectations</a></li>
<li class="chapter" data-level="4.3" data-path="4-expectations.html"><a href="4-expectations.html#variance"><i class="fa fa-check"></i><b>4.3</b> Variance</a></li>
<li class="chapter" data-level="4.4" data-path="4-expectations.html"><a href="4-expectations.html#moments-and-moment-generating-functions"><i class="fa fa-check"></i><b>4.4</b> Moments and Moment Generating Functions</a></li>
<li class="chapter" data-level="4.5" data-path="4-expectations.html"><a href="4-expectations.html#mean-vs-median"><i class="fa fa-check"></i><b>4.5</b> Mean vs Median</a></li>
<li class="chapter" data-level="4.6" data-path="4-expectations.html"><a href="4-expectations.html#covariance-and-correlation"><i class="fa fa-check"></i><b>4.6</b> Covariance and Correlation</a></li>
<li class="chapter" data-level="4.7" data-path="4-expectations.html"><a href="4-expectations.html#conditional-expectation"><i class="fa fa-check"></i><b>4.7</b> Conditional Expectation</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-common-distributions.html"><a href="5-common-distributions.html"><i class="fa fa-check"></i><b>5</b> Common Distributions</a><ul>
<li class="chapter" data-level="5.1" data-path="5-common-distributions.html"><a href="5-common-distributions.html#bernoulli-and-binomial"><i class="fa fa-check"></i><b>5.1</b> Bernoulli and Binomial</a></li>
<li class="chapter" data-level="5.2" data-path="5-common-distributions.html"><a href="5-common-distributions.html#hypergeometric"><i class="fa fa-check"></i><b>5.2</b> Hypergeometric</a></li>
<li class="chapter" data-level="5.3" data-path="5-common-distributions.html"><a href="5-common-distributions.html#poisson"><i class="fa fa-check"></i><b>5.3</b> Poisson</a></li>
<li class="chapter" data-level="5.4" data-path="5-common-distributions.html"><a href="5-common-distributions.html#geometric-and-negative-binomial"><i class="fa fa-check"></i><b>5.4</b> Geometric and Negative Binomial</a></li>
<li class="chapter" data-level="5.5" data-path="5-common-distributions.html"><a href="5-common-distributions.html#uniform"><i class="fa fa-check"></i><b>5.5</b> Uniform</a></li>
<li class="chapter" data-level="5.6" data-path="5-common-distributions.html"><a href="5-common-distributions.html#exponential-and-gamma"><i class="fa fa-check"></i><b>5.6</b> Exponential and Gamma</a></li>
<li class="chapter" data-level="5.7" data-path="5-common-distributions.html"><a href="5-common-distributions.html#beta"><i class="fa fa-check"></i><b>5.7</b> Beta</a></li>
<li class="chapter" data-level="5.8" data-path="5-common-distributions.html"><a href="5-common-distributions.html#normal"><i class="fa fa-check"></i><b>5.8</b> Normal</a></li>
<li class="chapter" data-level="5.9" data-path="5-common-distributions.html"><a href="5-common-distributions.html#bivariate-normal"><i class="fa fa-check"></i><b>5.9</b> Bivariate Normal</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="A-useful-functions.html"><a href="A-useful-functions.html"><i class="fa fa-check"></i><b>A</b> Useful functions</a><ul>
<li class="chapter" data-level="A.1" data-path="A-useful-functions.html"><a href="A-useful-functions.html#gamma-function"><i class="fa fa-check"></i><b>A.1</b> Gamma Function</a></li>
<li class="chapter" data-level="A.2" data-path="A-useful-functions.html"><a href="A-useful-functions.html#useful-series-results"><i class="fa fa-check"></i><b>A.2</b> Useful Series Results</a><ul>
<li class="chapter" data-level="A.2.1" data-path="A-useful-functions.html"><a href="A-useful-functions.html#ex"><i class="fa fa-check"></i><b>A.2.1</b> <span class="math inline">\(e^x\)</span></a></li>
<li class="chapter" data-level="A.2.2" data-path="A-useful-functions.html"><a href="A-useful-functions.html#geometric-series"><i class="fa fa-check"></i><b>A.2.2</b> Geometric Series</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">STA 473 - Probability</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="expectations" class="section level1">
<h1><span class="header-section-number">Chapter 4</span> Expectations</h1>
<div id="expectation-of-a-rv" class="section level2">
<h2><span class="header-section-number">4.1</span> Expectation of a RV</h2>
<p>The sample mean is a useful measure of centrality of a set of data and we would like a similar quantity for a distribution.</p>
<p>Suppose we have a sample from a distribution that can take on integer values in the range of <span class="math inline">\(1\)</span> to <span class="math inline">\(5\)</span>. For example suppose we have the data <span class="math inline">\(\{ 1,1,2,3,3,3,4,5 \}\)</span>. Then the sample mean is <span class="math display">\[\begin{aligned} \bar{x} 
  &amp;= \frac{1}{n} \sum_{j=1}^n x_j = \frac{1}{8} (1+1+2+3+3+3+4+5)\\
  &amp;= \sum_{i=1}^5 \hat{p}_i \;i = \left(\frac{2}{8} \cdot 1 \right) + 
                            \left(\frac{1}{8} \cdot 2 \right) +
                            \left(\frac{3}{8} \cdot 3 \right) +
                            \left(\frac{1}{8} \cdot 4 \right) +
                            \left(\frac{1}{8} \cdot 5 \right) 
  \end{aligned}\]</span> where <span class="math inline">\(\hat{p}_i\)</span> values are the observed proportions for each possible value. If we have a really large sample then <span class="math inline">\(\hat{p}_i \approx Pr(X=i)\)</span> and it is natural to define the Expected Value as <span class="math display">\[E(X) = \begin{cases}
    \sum x \, f(x) \;\;\;\;\;\;\;\;\;\;\;\; \textrm{ if } X \textrm{ is discrete} \\
    \int_{-\infty}^\infty x \, f(x) \,dx \;\;\;\;\; \textrm{ if } X \textrm{ is continuous} 
    \end{cases}\]</span></p>
<p>We need to be careful to avoid the <span class="math inline">\(\infty - \infty\)</span> case and note that if <span class="math display">\[\sum_{\textrm{Negative }x} x\,f(x) = -\infty \;\;\;\;\; \textrm{ and } \;\;\;\;\;\sum_{\textrm{Positive }x} x\,f(x) = \infty\]</span> or <span class="math display">\[\int_{-\infty}^0 x\,f(x)\, dx = -\infty \;\;\;\;\;\; \textrm{ and } \;\;\;\;\;\int_{0}^\infty x\,f(x)\,dx = \infty\]</span> then the resulting expectation could be written as <span class="math inline">\(-\infty + \infty\)</span> and that quantity <em>does not exist</em>.</p>
<p><strong>Example</strong> Suppose that the lifetime, <span class="math inline">\(X\)</span>, of an appliance has a pdf <span class="math display">\[f(x) = 2 e^{-2x}\;\cdot I(x&gt;0)\]</span> Then the expectation of <span class="math inline">\(X\)</span> is <span class="math display">\[E(X) = \int_{-\infty}^{\infty} x \,f(x) \, dx = \int_0^\infty x \, 2e^{-2x} \, dx = 2\int_0^\infty x \, e^{-2x} \, dx\]</span> To finish solving this integral, we need to do integration by parts letting <span class="math display">\[\begin{aligned} 
  u =2x    &amp; \;\;\;\;&amp; dv =   e^{-2x}\,dx \\
  du= 2\,dx  &amp;         &amp; v = -\frac{1}{2} e^{-2x} 
  \end{aligned}\]</span> and therefore <span class="math display">\[\begin{aligned} E(X) 
  &amp;= -xe^{-2x} \Big\vert_0^\infty+ \int_0^\infty e^{-2x}\,dx \\
  &amp;= 0 + -\frac{1}{2} e^{-2x} \Big\vert_0^\infty \\
  &amp;= \frac{1}{2}
  \end{aligned}\]</span></p>
<p><strong>Expectations of functions of a RV</strong> Suppose we have a random variable <span class="math inline">\(X\)</span> and some function <span class="math inline">\(Y=r(X)\)</span>, then I might want to know the expectation of the random variable <span class="math inline">\(Y\)</span>. We could just derive the pdf of <span class="math inline">\(Y\)</span> and calculate its expectation, but that is just a bunch of integration and differentiation that cancels out because (<em>in the continuous case and assuming <span class="math inline">\(r(x)\)</span> is invertable</em>) <span class="math display">\[\begin{aligned} E(Y) 
  &amp;= \int y \, g(y) \, dy \\
  &amp;= \int r(x)\, f\big( r(x) \big) \Bigg\vert \frac{dx}{dy} \Bigg\vert \,dy \\
  &amp;= \int r(x)\, f(x)  \,dx 
  \end{aligned}\]</span></p>
<p>We could prove that if the expectation of <span class="math inline">\(Y=r(X)\)</span> exists, then for any function <span class="math inline">\(r(x)\)</span> is equal to <span class="math display">\[E(Y) = E[r(X)] = \begin{cases}
  \sum r(x) \,f(x) \\
  \int r(x) \,f(x) \, dx
  \end{cases}\]</span></p>
<p><strong>Example</strong> Suppose that we have a random variable with pdf <span class="math display">\[f(x) = 3x^2 \, I(0&lt;x&lt;1)\]</span> then <span class="math display">\[E(X) = \int_0^1 x\, f(x) \,dx = \int_0^1 x \, 3x^2 \,dx = \int_0^1 3 x^3 \, dx = \frac{3}{4}x^4 \Bigg\vert_0^1 = \frac{3}{4}\]</span> and if we consider <span class="math inline">\(Y=X^2\)</span> then <span class="math display">\[E(Y) = E(X^2) = \int_0^1 x^2\, f(x)\, dx = \int_0^1 3x^4 = \frac{3}{5}x^5\Bigg\vert_0^1 = \frac{3}{5}\]</span></p>
<p>Notice that <span class="math inline">\(E(X^2) \ne \Bigg( E(X) \Bigg)^2\)</span>. For general <span class="math inline">\(r(X)\)</span>, we typically see that <span class="math inline">\(E(r(X)) \ne r\Bigg( E(X) \Bigg)\)</span>. However for linear functions, it is true.</p>
<ol style="list-style-type: decimal">
<li><p>Suppose that <span class="math inline">\(X\)</span> has a Uniform(<span class="math inline">\(a\)</span>, <span class="math inline">\(b\)</span>) distribution. Find the expected value of <span class="math inline">\(X\)</span>.</p></li>
<li><p>Suppose that <span class="math inline">\(X\)</span> has a Uniform(<span class="math inline">\(a\)</span>, <span class="math inline">\(b\)</span>) distribution. Find the expected value of <span class="math inline">\(Y=\sqrt X\)</span>.</p></li>
<li><p>Suppose that <span class="math inline">\(X\)</span> has a Uniform(<span class="math inline">\(a\)</span>, <span class="math inline">\(b\)</span>) distribution. Find the expected value of <span class="math inline">\(Y=1/X\)</span>.</p></li>
<li><p>A 1-meter stick is broken at a random spot along the stick. Find the expected value of the length of the longer piece.</p></li>
</ol>
<p><strong>Expectations of functions of several variables</strong> Suppose that a multivariate distribution with joint pdf <span class="math inline">\(f(x_1, x_2, \dots, x_n)\)</span> and we define <span class="math inline">\(Y=r(X_1,X_2,\dots,X_n)\)</span> then <span class="math display">\[E(Y) = \iint\dots\int r(x_1, x_2, \dots, x_n)\, f(x_1, x_2, \dots, x_n)\, dx_1 dx_2 \dots dx_n\]</span> <strong>Example</strong> Suppose that we have a bivariate distribution <span class="math display">\[f(x_1, x_2) = 8x_1 x_2 \, \cdot I(0 &lt; x_1 &lt; x_2 &lt; 1)\]</span> and we wish to know the expectation of <span class="math inline">\(Y=X_1+X_2\)</span>. Then <span class="math display">\[\begin{aligned} E( X_1+X_2 ) 
  &amp;= \int_0^1 \int_0^{x_2} (x_1+x_2) \, 8x_1x_x \; dx_1 dx_2 \\
  &amp;= \vdots \\
  &amp;= \frac{4}{3}
  \end{aligned}\]</span></p>
</div>
<div id="properties-of-expectations" class="section level2">
<h2><span class="header-section-number">4.2</span> Properties of Expectations</h2>
<ol style="list-style-type: decimal">
<li><p>Prove that for finite constants <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> and continuous random variable <span class="math inline">\(X\)</span>, we have <span class="math display">\[E(aX + b) = aE(X) + b\]</span></p></li>
<li><p>Show that if continuous random variable <span class="math inline">\(X\)</span> has support on the interval <span class="math inline">\((a,b)\)</span> where <span class="math inline">\(a&lt;b\)</span>, then <span class="math inline">\(a &lt; E(X) &lt; b\)</span>. This is true in the discrete case as well.</p></li>
<li><p>Show that if <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are (possibly not independent!) continuous random variables with joint pdf <span class="math inline">\(f(x_1, x_2)\)</span> that <span class="math inline">\(E( X_1 + X_2 ) = E(X_1) + E(X_2)\)</span>. By induction this result will hold for the sum of a finite number number random variables. Notice the proof for the discrete case is similar with simply replacing integrals with summations.</p></li>
<li><p>Suppose that three random variables <span class="math inline">\(X_1, X_2, X_3\)</span> are sampled from a distribution that has mean <span class="math inline">\(E(X_i)=5\)</span>. Find the expectation of <span class="math inline">\(2X_1 - 3X_2 -X_3 - 5\)</span>. <em>When we say <span class="math inline">\(X_1, X_2, \dots\)</span> are sampled from a distribution, we actually mean that <span class="math inline">\(X_1,X_2,\dots\)</span> are independent and each have marginal distribution as given. So when you heard “We sampled from population …” in your Introduction to Statistics course, they actually were telling you that the observations are independent.</em></p></li>
<li><p>Suppose that three random variables <span class="math inline">\(X_1, X_2, X_3\)</span> are sampled from a uniform distribution on <span class="math inline">\((0,1)\)</span>. What is the expectation of <span class="math inline">\((X_1 - 2X_2 + X_3)^2\)</span>.</p></li>
<li><p><em>Bernoulli Expectation</em> Suppose that the random variable <span class="math inline">\(X_i\)</span> can take on values <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span> and the probability it takes on <span class="math inline">\(1\)</span> is <span class="math inline">\(f(1)=p\)</span>. What is the expected value of <span class="math inline">\(X_i\)</span>?</p></li>
<li><p><em>Binomial Expectation</em> Suppose that we have <span class="math inline">\(n\)</span> identically distributed Bernoulli random variables, each of which having probability of success <span class="math inline">\(f_i(1)=p\)</span>. Letting <span class="math inline">\(Y=\sum_1^n X_i\)</span>, what is the expected value of <span class="math inline">\(Y\)</span>?</p></li>
<li><em>Hypergeometric Expectation</em> Suppose that we have a bag with <span class="math inline">\(N\)</span> balls, of which <span class="math inline">\(M\)</span> are red and <span class="math inline">\(N-M\)</span> are blue. We will draw <span class="math inline">\(n\)</span> balls out (without replacement) and we are interested in the total number of red balls drawn. Let <span class="math inline">\(X_i\)</span> be <span class="math inline">\(1\)</span> if the <span class="math inline">\(i\)</span>th draw was a red ball.
<ol style="list-style-type: lower-alpha">
<li>First consider the case where we draw <span class="math inline">\(n=1\)</span> ball. What is the probability that we draw a red ball on the first draw and therefore what is <span class="math inline">\(E(X_1)\)</span>?</li>
<li>Next consider the case where we draw <span class="math inline">\(n=2\)</span> balls. What is the probability that we draw a red ball on the second draw and therefore what is <span class="math inline">\(E(X_2)\)</span>?</li>
<li>The same pattern holds for the rest of <span class="math inline">\(X_3, X_4, \dots X_n\)</span>. Given that, what is the expected value of <span class="math inline">\(Y=\sum_1^n X_i\)</span>?</li>
</ol></li>
<li><p>Suppose that continuous random variables <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> are independent, each having a marginal pdf <span class="math inline">\(f_i(x_i)\)</span>. Show that <span class="math display">\[E\left( \prod_{i=1}^n X_i \right) = \prod_{i=1}^n E(X_i)\]</span> Notice that this result requires that the variables are independent, whereas the result in 4.2.3 did not require independence.</p></li>
<li><p>A gambler will play a game where he is equally likely to win or lose on a given play. When the gambler wins, her fortune is doubled, but when she loses, it is cut in half. Given that the gambler started the game with a fortune of <span class="math inline">\(c\)</span>, what is the expected fortune after <span class="math inline">\(n\)</span> plays?</p></li>
</ol>
<p>It can be shown that for non-negative, continuous random variables <span class="math display">\[E(X) = \int_0^\infty (1-F(x))\,dx\]</span> and for non-negative discrete random variables <span class="math display">\[E(X) = \sum_{x=1}^\infty Pr( X \ge x )\]</span></p>
<p>The proof in the discrete case is a reordering of <span class="math display">\[E(X) = \sum_{x=0}^\infty x\,f(x) = \sum_x^\infty x\, Pr(X=x)\]</span> to summing one copy of <span class="math inline">\(Pr(X=1)\)</span> and two copies of <span class="math inline">\(Pr(X=2)\)</span> and three copies of <span class="math inline">\(Pr(X=3)\)</span> and so on.</p>
<table>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(E(X)\)</span></td>
<td align="center">=</td>
<td align="center">0</td>
<td align="center"><span class="math inline">\(+ Pr(X=1)\)</span></td>
<td align="center"><span class="math inline">\(+Pr(X=2)\)</span></td>
<td align="center"><span class="math inline">\(+Pr(X=3)\)</span></td>
<td align="center"><span class="math inline">\(\dots\)</span></td>
</tr>
<tr class="even">
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"><span class="math inline">\(+Pr(X=2)\)</span></td>
<td align="center"><span class="math inline">\(+Pr(X=3)\)</span></td>
<td align="center"><span class="math inline">\(\dots\)</span></td>
</tr>
<tr class="odd">
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"><span class="math inline">\(+Pr(X=3)\)</span></td>
<td align="center"><span class="math inline">\(\dots\)</span></td>
</tr>
<tr class="even">
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"><span class="math inline">\(\ddots\)</span></td>
</tr>
</tbody>
</table>
<p>Notice that the first row of this sum is <span class="math inline">\(Pr(X\ge1)\)</span> and the second is <span class="math inline">\(Pr(X\ge 2)\)</span> and that establishes the result.</p>
<ol start="11" style="list-style-type: decimal">
<li><p><em>Geometric Expectation</em> Suppose that each time a person plays a game, they have a probability <span class="math inline">\(p\)</span> of winning. Let the random variable <span class="math inline">\(X\)</span> be the number of games played until the person wins. We have previously shown that <span class="math display">\[f(x) = (1-p)^{x-1}p \;\; I(x\in \{1,2,\dots\})\]</span> <span class="math display">\[Pr(X \le x) = 1 - (1-p)^x\]</span> for <span class="math inline">\(x \in \{1,2,\dots\}\)</span> What is expected number of times a player must play until they win?</p></li>
<li><p><em>Gamma Expectation</em> Suppose that we have a random variable <span class="math inline">\(X\)</span> with a <span class="math inline">\(Gamma(\alpha, \beta)\)</span> distribution and therefore <span class="math display">\[f(x) = \frac{\beta^\alpha}{\Gamma(\alpha)} x^{\alpha-1} e^{-x\beta} \;\cdot I(x&gt;0)\]</span> Show that <span class="math display">\[E(X) = \frac{\alpha}{\beta}\]</span></p></li>
</ol>
</div>
<div id="variance" class="section level2">
<h2><span class="header-section-number">4.3</span> Variance</h2>
<p>Although the mean of a distribution is quite useful, it is not the only measure of interest. A secondary measure of interest is a measure of <em>spread</em> of the distribution. Just as the sample variance is interpreted as the “typical squared distance to the mean” we will define the distribution variance as the “expected squared distance to the mean”.</p>
<p>For notational convenience, let <span class="math inline">\(\mu=E(X)\)</span> and define <span class="math display">\[Var(X) = E\big[ (X-\mu)^2 \big]\]</span></p>
<p>Because expectations don’t necessarily exist, we’ll say that <span class="math inline">\(Var(X)\)</span> does not exist if <span class="math inline">\(E(X)\)</span> does not exist or if <span class="math inline">\(E[(X-\mu)^2]\)</span> does not exist. Notice that the Variance is non-negative because of the square.</p>
<p>Finally, we will define the standard deviation of <span class="math inline">\(X\)</span> as the positive square-root of the variance. That is <span class="math inline">\(StdDev(X) = \bigg\vert \sqrt{Var(X)} \bigg\vert\)</span>.</p>
<p>Notationally all of this is a bit cumbersome and we’ll use <span class="math display">\[E(X) = \mu_X\;\;\;\;\;\;\;\;\;\; Var(X) = \sigma^2_X \;\;\;\;\;\;\;\;\;\; StdDev(X) = \sigma_X\]</span> If we have only a single random variable in a situation, we will suppress the subscript.</p>
<ol style="list-style-type: decimal">
<li>Suppose that RV <span class="math inline">\(X\)</span> has <span class="math inline">\(Var(X)\)</span> that exists, then for constants <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>, show that the RV <span class="math display">\[Y=aX+b\]</span> has variance <span class="math display">\[Var(Y) = a^2 Var(X)\]</span> Notice that shifting the entire distribution of <span class="math inline">\(X\)</span> by some constant <span class="math inline">\(b\)</span> does not affect the <em>spread</em> of the shifted distribution.</li>
</ol>
<p>Next we consider the sum of independent random variables <span class="math inline">\(X_1+X_2\)</span>. <span class="math display">\[\begin{aligned} Var( X_1 + X_2 ) 
  &amp;= E\Bigg[ \bigg[ (X_1 + X_2) - E(X_1+X_2) \bigg]^2 \Bigg] \\
  &amp;= E\Bigg[ \bigg[ X_1 + X_2 - \mu_1 - \mu_2 \bigg]^2 \Bigg] \\
  &amp;= E\Bigg[ \bigg[ (X_1 -\mu_1) + (X_2 - \mu_2)\bigg]^2 \Bigg] \\
  &amp;= E\Bigg[  (X_1 -\mu_1)^2 + 2(X_1-\mu_1)(X_2-\mu_2) + (X_2 - \mu_2)^2  \Bigg] \\
  &amp;= E\big[  (X_1 -\mu_1)^2 \big] + E\big[ 2(X_1-\mu_1)(X_2-\mu_2)\big]  + E\big[(X_2 - \mu_2)^2  \big] \\
  &amp;= Var(X_1) \;\;\;\;\;\;\;\;\;+ 2E\big[ (X_1-\mu_1)(X_2-\mu_2)\big]  + Var(X_2) 
  \end{aligned}\]</span> Because <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are independent then <span class="math display">\[E\big[ (X_1-\mu_1)(X_2-\mu_2)\big] = E[(X_1-\mu_1)] E[(X_2-\mu_2)] = (\mu_1 -\mu_1) (\mu_2-\mu_2) = 0\]</span> Repeating this argument for <span class="math inline">\(n\)</span> independent random variables, we therefore have <span class="math display">\[Var( X_1+X_2+\dots+X_n ) = Var(X_1) + Var(X_2) + \dots + Var(X_n)\]</span> <span class="math display">\[Var\Bigg( \sum_{i=1}^n X_i \Bigg) = \sum_{i=1}^n Var(X_i)\]</span> Notice that this result <em>requires</em> independence so that the cross-product terms are zero!</p>
<ol start="2" style="list-style-type: decimal">
<li><p>Show that <span class="math inline">\(Var(X) = E(X^2) - \mu^2\)</span>. This formula is far more convenient to use, generally.</p></li>
<li><p>Suppose that <span class="math inline">\(X\)</span> has a uniform distribution on the interval <span class="math inline">\([0,1]\)</span>. Compute the variance of <span class="math inline">\(X\)</span>.</p></li>
<li><p>Suppose that <span class="math inline">\(Y\)</span> has a uniform distribution on the interval <span class="math inline">\([a,b]\)</span> where <span class="math inline">\(a&lt;b\)</span>. Compute the variance of <span class="math inline">\(Y\)</span>.</p></li>
<li><p>Suppose that <span class="math inline">\(X\)</span> has expectation <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>. Show that <span class="math display">\[E\bigg[ X(X-1) \bigg]=\mu(\mu-1) + \sigma^2\]</span></p></li>
<li><p>Suppose that <span class="math inline">\(X\)</span> has a <span class="math inline">\(Gamma(\alpha,\beta)\)</span> distribution. Find that the variance of <span class="math inline">\(X\)</span> is <span class="math inline">\(\frac{\alpha}{\beta^2}\)</span>.</p></li>
<li><p>Suppose that <span class="math inline">\(X\)</span> has a <span class="math inline">\(Bernoulli(p)\)</span> distribution, that is <span class="math inline">\(X\)</span> takes on values <span class="math inline">\(1\)</span> or <span class="math inline">\(0\)</span> with probabilities <span class="math inline">\(Pr(X=1)=p\)</span> and <span class="math inline">\(Pr(X=0) = 1-p\)</span>. Find the variance of <span class="math inline">\(X\)</span>.</p></li>
<li><p>Suppose that <span class="math inline">\(Y\)</span> has a <span class="math inline">\(Binomial(n,p)\)</span> distribution. That is that <span class="math display">\[Y=\sum_{i=1}^n X_i\]</span> where <span class="math inline">\(X_i\)</span> are independent <span class="math inline">\(Bernoulli(p)\)</span> random variables. Show that <span class="math display">\[Var(Y) = np(1-p)\]</span></p></li>
</ol>
</div>
<div id="moments-and-moment-generating-functions" class="section level2">
<h2><span class="header-section-number">4.4</span> Moments and Moment Generating Functions</h2>

<div class="definition">
<p><span id="def:unnamed-chunk-8" class="definition"><strong>Definition 4.1 </strong></span>Just as the <span class="math inline">\(E(X)\)</span> defines the center of a distribution and <span class="math inline">\(E[ (X-\mu)^2 ] = E(X^2)-\mu^2\)</span> defines the variance, the quantities <span class="math display">\[M_k = E\big( X^k \big)\;\;\;\;\;\;\textrm{ where } k\in \{1,2,3,\dots\}\]</span></p>
are what we call the <span class="math inline">\(k\)</span>th moment of the distribution. These moments define other attributes of the distribution, but sometimes it is useful to define a similar quantity called the <span class="math inline">\(k\)</span>th <em>central moment</em> <span class="math display">\[m_k = E\big( ( X-\mu ) ^k \big)\;\;\;\;\;\;\textrm{ where } k\in \{1,2,3,\dots\}\]</span>
</div>
<p></p>
<p>These two quantities can define several aspects of the distribution. For example, <span class="math inline">\(M_1=E(X)\)</span> is the distribution mean, while <span class="math inline">\(M_2=E(X^2)\)</span> is related to the variance. Other attributes are related to higher moments (e.g. the distribution skew is related to <span class="math inline">\(m_3\)</span>).</p>
<p>Somewhat obnoxiously, the book defines <span class="math inline">\(M_k\)</span> to exist if and only if <span class="math inline">\(E\Big(|X|^k\Big) &lt; \infty\)</span>. <em>(This is obnoxious because we used a different criteria to say if <span class="math inline">\(E(X)\)</span> existed in section 4.1 and the definition for the <span class="math inline">\(k\)</span>th moment is a more strict requirement.)</em></p>

<div class="theorem">
<span id="thm:unnamed-chunk-9" class="theorem"><strong>Theorem 4.1 </strong></span>For positive integers <span class="math inline">\(j&lt;k\)</span>, if <span class="math inline">\(M_k\)</span> exists, then <span class="math inline">\(M_j\)</span> must also exist.
</div>
 
<div class="proof">
<span class="proof"><em>Proof. </em></span> <span class="math display">\[\begin{aligned} E\Big( |X|^j \Big) 
  &amp;=   \int_{-\infty}^{\infty} |x|^j \,f(x) \,dx \\
  &amp;=   \int_{|x|\le 1} |x|^j \, f(x) \,dx + \int_{|x|&gt;1} |x|^j \, f(x) \, dx \\
  &amp;\le \int_{|x|\le 1} 1 \, f(x)\,dx + \int_{|x|&gt;1} |x|^k \, f(x) \, dx \\
  &amp;\le 1 + M_k\\
  &amp;&lt; \infty \textrm{ by assumption }
  \end{aligned}\]</span>
</div>
<p></p>

<div class="definition">
<span id="def:unnamed-chunk-11" class="definition"><strong>Definition 4.2 </strong></span>Let <span class="math inline">\(X\)</span> be a random variable. For each real number <span class="math inline">\(t\)</span>, define <span class="math display">\[\psi(t) = E\left( e^{tX} \right)\]</span> as the <em>Moment Generating Function of <span class="math inline">\(X\)</span>, which we denote mgf of <span class="math inline">\(X\)</span>.</em>
</div>
<p></p>

<div class="theorem">
<span id="thm:unnamed-chunk-12" class="theorem"><strong>Theorem 4.2 </strong></span>Let <span class="math inline">\(X\)</span> be a random variable whose mgf <span class="math inline">\(\psi(t)\)</span> is finite for some neighborhood about <span class="math inline">\(t=0\)</span>. Then for positive integer <span class="math inline">\(k\)</span>, the <span class="math inline">\(k\)</span>th momement is the <span class="math inline">\(k\)</span>th derivative of <span class="math inline">\(\psi(t)\)</span> evaluated at <span class="math inline">\(t=0\)</span>. That is <span class="math display">\[M_k = E\big(X^k\big) = \psi^{(k)}(0)\]</span>
</div>
 
<div class="proof">
<span class="proof"><em>Proof. </em></span> A full proof is quite technical, but it revolves around showing that it is permissible to interchange the order of integration/summation and differentiation in this case and that therefore: <span class="math display">\[\begin{aligned} \psi^{(n)}(0) 
  &amp;= \frac{d^n}{dt^n} E(e^{tX}) \Bigg\vert_{t=0} \\
  &amp;= E\Bigg[ \Big( \frac{d^n}{dt^n} e^{tX}  \Big) \Bigg\vert_{t=0} \Bigg] \\
  &amp;= E\Big[ X^n e^{tX}\vert_{t=0}\Big] \\
  &amp;= E\big[X^n\big]
  \end{aligned}\]</span>
</div>
<p></p>

<div class="theorem">
<span id="thm:unnamed-chunk-14" class="theorem"><strong>Theorem 4.3 </strong></span>If the mgfs of two random variables are finite and identical for all values of <span class="math inline">\(t\)</span> in a neighborhood of <span class="math inline">\(t=0\)</span>, then the probability distributions of the two variables are the same.
</div>
 
<div class="remark">
<span class="remark"><em>Remark. </em></span> This theorem allows us to compare the mgf of some variable of interest to the set of known mgfs and claim that because the mgfs match, then the variable of interest must follow the matching distribution. This is often a very easy way to show that a variable has a particular distribution and is the reason that we have introduced moment generating functions.
</div>
<p></p>
<ol style="list-style-type: decimal">
<li><p>Suppose that the random variable <span class="math inline">\(X\)</span> has an <span class="math inline">\(Exponential(\beta)\)</span> distribution which is a special case of the Gamma distribution. <span class="math inline">\(Exponential(\beta) = Gamma(1,\beta)\)</span> distribution. The table of distributions in your book shows that the <span class="math inline">\(Var(X)=\frac{1}{\beta^2}\)</span>. Derive the mgf of the Exponential distribution and use it to derive both the expectation and variance.</p></li>
<li><p><strong>Mgf of a linear transformation</strong> Suppose that we have a random variable <span class="math inline">\(X\)</span> with mgf <span class="math inline">\(\psi_1(t)\)</span>. Show that <span class="math inline">\(Y=aX+b\)</span> for real constants <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> has the mgf <span class="math display">\[\psi_2(t) = e^{bt}\psi_1(at)\]</span></p></li>
<li><p>Suppose that <span class="math inline">\(X \sim Exponential(\beta)\)</span>. Show that <span class="math inline">\(Y=2X\)</span> has the mgf of an <span class="math inline">\(Exponential(\beta/2)\)</span> distribution and therefore <span class="math inline">\(Y \sim Exponential(\beta/2)\)</span>.</p></li>
<li><p>Derive the moment generating function of the <span class="math inline">\(Gamma(\alpha, \beta)\)</span> distribution.</p></li>
<li><p>Derive the moment generating function of <span class="math inline">\(Bernoulli(p)\)</span> distribution.</p></li>
<li><p>Suppose that independent random variables <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> each have mgfs <span class="math inline">\(\psi_1(t), \psi_2(t), \dots, \psi_n(t)\)</span>. Show that the mgf of <span class="math inline">\(Y=\sum X_i\)</span> is <span class="math display">\[\psi_Y(t) = \prod_{i=1}^n \psi_i(t)\]</span></p></li>
<li><p>Suppose that <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> are independent <span class="math inline">\(Bernoulli(p)\)</span> random variables. Derive the mgf of <span class="math inline">\(Y=\sum X_i\)</span>. That is, derive the mgf of the Binomial distribution.</p></li>
<li><p>Suppose that random variable <span class="math inline">\(X\)</span> has support on the non-negative integers <span class="math inline">\(0, 1, 2, \dots\)</span>. The probability function of <span class="math inline">\(X\)</span> is <span class="math display">\[f(x)=\frac{\lambda^x}{x!}e^{-\lambda}\,\cdot I(x\in \{0,1,2,\dots\})\]</span> The random variable <span class="math inline">\(X\)</span> has a <span class="math inline">\(Poisson(\lambda)\)</span> distribution. Derive the mgf of this distribution. <em>Hint: what is the summation constant in the pf?</em></p></li>
<li><p>Suppose that <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> are independent random variables each with distribution <span class="math inline">\(Poisson(\lambda)\)</span>. Derive the moment generating function of the distribution of <span class="math inline">\(Y=\sum X_i\)</span> and state its distribution.</p></li>
<li><p>Suppose that <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> are independent random variables each with distribution <span class="math inline">\(Exponential(\beta)=Gamma(1,\beta)\)</span>. Derive the moment generating function of the distribution of <span class="math inline">\(Y=\sum X_i\)</span> and state its distribution.</p></li>
</ol>
</div>
<div id="mean-vs-median" class="section level2">
<h2><span class="header-section-number">4.5</span> Mean vs Median</h2>
<p>We will skip this section.</p>
</div>
<div id="covariance-and-correlation" class="section level2">
<h2><span class="header-section-number">4.6</span> Covariance and Correlation</h2>
<p>In introductory statistics classes, we often define the <em>correlation coefficient</em> which describes if there is a positive or negative linear relationship between two variables.</p>
<p>The sample correlation coefficient is defined as <span class="math display">\[r=\frac{\sum_{i=1}^{n}\left(\frac{x_{i}-\bar{x}}{s_{x}}\right)\left(\frac{y_{i}-\bar{y}}{s_{y}}\right)}{n-1}\]</span></p>
<p>The heart of this formula is the sign of each of the <span class="math inline">\((x_i-\bar{x})(y_i-\bar{y})\)</span> terms. If the x-value is big (greater than <span class="math inline">\(\bar{x}\)</span>) and the y-value is large (greater than <span class="math inline">\(\bar{y}\)</span>), then after multiplication, the result is positive. Likewise if the x-value is small and the y-value is small, both standardized values are negative and therefore after multiplication the result is positive. If a large x-value is paired with a small y-value, then the first value is positive, but the second is negative and so the multiplication result is negative.</p>
<p><img src="Probability_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p>We will define a similar quantity for two random variables.</p>
<p><span class="math display">\[Cov(X,Y) = E\left[ \big(X-E(X)\big)\big(Y-E(Y)\big) \right]\]</span></p>
<ol style="list-style-type: decimal">
<li>Prove that for random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> such that <span class="math inline">\(Var(X)&lt;\infty\)</span> and <span class="math inline">\(Var(Y) &lt; \infty\)</span>, <span class="math display">\[Cov(X,Y) = E(XY) - E(X)E(Y)\]</span></li>
</ol>

<div class="remark">
<span class="remark"><em>Remark. </em></span> This computational formula for the covariance is the same as the computational formula for <span class="math inline">\(Var(X)\)</span> is we let <span class="math inline">\(Y=X\)</span>.<br />
<span class="math display">\[Cov(X,X) = E(XX)-E(X)E(X) = E\left(X^2\right) - \left[ E(X) \right]^2\]</span> In fact, we can consider <span class="math inline">\(Var(X)\)</span> as a special case of <span class="math inline">\(Cov(X,Y)\)</span>.
</div>
<p></p>
<ol start="2" style="list-style-type: decimal">
<li><em>Cauchy-Schwartz Inequality</em>. Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be random variables, each with finite variances. Show that <span class="math inline">\(\left[ E(XY) \right]^2 \le E(X^2)E(Y^2)\)</span>. <em>Hint: Observe that <span class="math inline">\(E\left[ (tX-Y)^2 \right] \ge 0\)</span> for any real value <span class="math inline">\(t\)</span> and therefore</em> <span class="math display">\[t^2E(X^2) -2tE(XY) + E(Y) &gt; 0\]</span> <em>This a polynomial of degree <span class="math inline">\(2\)</span> in <span class="math inline">\(t\)</span> but has no roots. Combine this fact with the quadradic equation to achieve the desired result.</em></li>
</ol>

<div class="definition">
<span id="def:unnamed-chunk-18" class="definition"><strong>Definition 4.3 </strong></span>We can now define the correlation between random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> such that they have finite variances as <span class="math display">\[\rho(X,Y) = \frac{Cov(X,Y)}{\sigma_X \sigma_Y}\]</span>
</div>
<p></p>
<ol start="3" style="list-style-type: decimal">
<li><p>Use the Cauchy-Schwartz Inequality to show that <span class="math display">\[-1 \le \rho(X,Y) \le 1\]</span></p></li>
<li><p>Prove that if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent random variables, each with finite variances, then <span class="math inline">\(Cov(X,Y)=0\)</span>.</p></li>
<li><p>Show that the preceding statement is not an “if and only if” statement by considering the following case. Let be uniformly <span class="math inline">\(X\)</span> distributed on the integers <span class="math inline">\(-1, 0,\)</span> and <span class="math inline">\(1\)</span>. Let <span class="math inline">\(Y=X^2\)</span>. Show that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are not independent but that <span class="math inline">\(Cov(X,Y)=0\)</span>.</p></li>
<li><p>Prove that for random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> and constants <span class="math inline">\(a,b,c\)</span> and <span class="math inline">\(d\)</span> that <span class="math display">\[Cov( aX+b, cY+d) = ac\,Cov(X,Y)\]</span></p></li>
<li><p>Prove that for random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> <span class="math display">\[Var( X + Y) = Var(X) + Var(Y) + 2\,Cov(X,Y)\]</span></p></li>
<li><p>Prove that for random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> and constants <span class="math inline">\(a,b,\)</span> and <span class="math inline">\(c\)</span> that <span class="math display">\[Var( aX + bY + c) = a^2 Var(X) + b^2 Var(Y) + 2ab\,Cov(X,Y)\]</span></p></li>
<li><p>Suppose that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> have a continuous joint distribution with pdf <span class="math display">\[f(x,y) = \frac{1}{3} (x+y) \; I(0&lt;x&lt;1)I(0&lt;y&lt;2)\]</span> Determine the value of <span class="math inline">\(Var( 2X -3Y +8 )\)</span></p></li>
<li><p>Suppose that <span class="math inline">\(X_1,X_2,\dots,X_n\)</span> are independent and identically distributed random variables, each with <span class="math inline">\(E(X_i)=\mu\)</span> and <span class="math inline">\(Var(X_i)=\sigma^2\)</span>. Show that the sample mean <span class="math inline">\(\bar{X} = \frac{1}{n}\sum X_i\)</span> has expectation <span class="math inline">\(E( \bar{X} ) = \mu\)</span> and variance <span class="math inline">\(Var(\bar{X}) = \frac{\sigma^2}{n}\)</span>.</p></li>
</ol>
</div>
<div id="conditional-expectation" class="section level2">
<h2><span class="header-section-number">4.7</span> Conditional Expectation</h2>
<p>In many real-world scenarios, the phenomena of interest is best modeled in a “multilevel” fashion. For example, suppose that we are interested in understanding radon levels within homes and we have radon levels from thousands of houses across the US. Because houses within a county are more similar to each other than houses that are states apart, it makes sense to model the data using a multilevel approach that models each county and then the houses within the county. In this section, we will develop certain mathematical tools that will be useful for these situations.</p>
<p><em>Example</em> Recall the Beta-Binomial hierarchical relationship where we have some observation <span class="math inline">\(X\)</span> that is the number of successes out of <span class="math inline">\(n\)</span> independent Bernoulli<span class="math inline">\((P)\)</span> trials, and the probability of success <span class="math inline">\(P\)</span> was also a random variable but with a Beta(<span class="math inline">\(\alpha\)</span>,<span class="math inline">\(\beta\)</span>) distribution. <span class="math display">\[X|P \sim Binomial(n,P) \;\;\;\;\;\;\textrm{where }\;\;\; P \sim Beta(\alpha, \beta)\]</span> Recall the probability and probability density functions were <span class="math display">\[\begin{aligned} 
  g(x|p) &amp;= {n \choose x} p^x (1-p)^{n-x} \;\cdot I(x\in 0,1,\dots,n)\\
  f(p) &amp;= \frac{\Gamma(\alpha,\beta)}{\Gamma(\alpha)\Gamma(\beta)} p^{\alpha-1}(1-p)^{\beta-1} \; \cdot I(0 &lt; p &lt; 1) 
  \end{aligned}\]</span></p>
<p>If we regard <span class="math inline">\(P=p=0.6\)</span> as a known quantity, then <span class="math display">\[E(X\,|\,P=0.6) = np = n(0.60)\]</span> However, if we don’t know the value of <span class="math inline">\(P\)</span> and continue to consider it a random variable, then <span class="math display">\[E(X|P) = nP\]</span> is just a function of the random variable <span class="math inline">\(P\)</span> and therefore <span class="math inline">\(E(X|P)\)</span> also a random variable. We could ask questions like what is the probability density function of <span class="math inline">\(E(X|P)\)</span> and what is the expectation of <span class="math inline">\(E(X|P)\)</span>.</p>

<div class="definition">
<span id="def:unnamed-chunk-19" class="definition"><strong>Definition 4.4 </strong></span>If <span class="math inline">\(Y\)</span> has a continuous conditional distribution given <span class="math inline">\(X=x\)</span>, define the conditional expectation as <span class="math display">\[E(Y | x) = \int_{-\infty}^\infty y \, g_2(y|x) \,dy.\]</span> If <span class="math inline">\(Y\)</span> has a discrete conditional distribution given <span class="math inline">\(X=x\)</span>, replace the integral with the summation over all values of <span class="math inline">\(y\)</span>.
</div>
<p></p>
<p>Similarly we can define the conditional expectation of some function <span class="math inline">\(h(y)\)</span> of <span class="math inline">\(Y\)</span> as <span class="math display">\[E( h(Y) | x ) = \int_{-\infty}^\infty h(y) \, g_2(y|x) \,dy.\]</span></p>
<ol style="list-style-type: decimal">
<li><p>Suppose that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are continuous random variables where <span class="math inline">\(Y\)</span> has a finite expectation. Prove that <span class="math display">\[E\left[ E(Y|X) \right] = E(Y)\]</span> <em>Hint: Start with the definition of <span class="math inline">\(E(Y)\)</span> using the double integral and joint distribution and then split the joint distribution into the product of the conditional and marginal. Finally recognize the inner integral as <span class="math inline">\(E(Y|X=x)\)</span></em>.</p></li>
<li><p>Suppose that <span class="math inline">\(X|P \sim Binomial(n,P)\)</span> and <span class="math inline">\(P\sim Beta(\alpha=4,\beta=6)\)</span>. Find <span class="math inline">\(E(X)\)</span>. <em>Hint: Though we haven’t proven it (yet!), the expectation of a Beta random variable is <span class="math inline">\(\alpha/(\alpha + \beta)\)</span></em>.</p></li>
<li><p>Suppose that an unknown number of individuals (denoted as <span class="math inline">\(N\)</span>) will be independently randomly chosen for “additional screening” by the TSA. Suppose that we allow <span class="math inline">\(N\sim Poisson(\lambda)\)</span> individuals are selected and that the probability that a selected person is female is <span class="math inline">\(0.4\)</span>. Let <span class="math inline">\(X\)</span> denote the number of females (out of <span class="math inline">\(N\)</span> individuals additionally screened) and so <span class="math inline">\(X|N \sim Binomial(N, p=0.4)\)</span>. Find <span class="math inline">\(E(X)\)</span>. <em>Hint: Though we haven’t proven it yet, the variance of a Poisson random variable is also <span class="math inline">\(\lambda\)</span>.</em></p></li>
</ol>

<div class="definition">
<span id="def:unnamed-chunk-20" class="definition"><strong>Definition 4.5 </strong></span>The conditional variance of <span class="math inline">\(Y|X=x\)</span> is defined similarly to the unconditional case <span class="math display">\[Var(Y|x) = E\bigg[ \Big(Y - E(Y|x) \Big)^2 \bigg] = E\Big[ \big(Y|x\big)^2 \Big] - \Big[ E(Y|x) \Big]^2\]</span>
</div>
<p></p>

<div class="theorem">
<span id="thm:unnamed-chunk-21" class="theorem"><strong>Theorem 4.4 </strong></span>If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are random variables which have finite expectation and variances, then <span class="math display">\[Var(Y) = E\Big[ Var(Y|X) \Big] + Var\Big[ E(Y|X) \Big]\]</span>
</div>
 
<div class="proof">
<span class="proof"><em>Proof. </em></span> We start by noticing <span class="math display">\[Var(Y|X) = E\Big(Y^2 | X\Big) - \Big[ E(Y|X) \Big]^2\]</span> and therefore <span class="math display">\[E \Bigg[ Var(Y|X) \Bigg] = E\Bigg[ E\Big(Y^2 | X\Big) - \Big[ E(Y|X) \Big]^2 \Bigg]\]</span> Furthermore by the definition of variance <span class="math display">\[Var\Bigg[ E(Y|X) \Bigg] = E\Bigg[ \Big( E(Y|X) \Big)^2 \Bigg] - \Bigg[ E\Big( E(Y|X) \Big) \Bigg]^2\]</span> Finally <span class="math display">\[\begin{aligned} Var(Y)
  &amp;= E\left( Y^2 \right) - \left[ E(Y) \right]^2 \\
  &amp;= E\Bigg[ E\Big(Y^2 | X\Big)\Bigg] - \Bigg[ E\Big( E(Y|X) \Big) \Bigg]^2 \\
  &amp;= E\Bigg[ E\Big(Y^2 | X\Big)\Bigg] - E\Bigg[ \Big( E(Y|X) \Big)^2 \Bigg] + E\Bigg[ \Big( E(Y|X) \Big)^2 \Bigg] - \Bigg[ E\Big( E(Y|X) \Big) \Bigg]^2 \\
  &amp;= E\Bigg[ E\Big(Y^2 | X\Big) - \Big( E(Y|X) \Big)^2 \Bigg] + E\Bigg[ \Big( E(Y|X) \Big)^2 \Bigg] - \Bigg[ E\Big( E(Y|X) \Big) \Bigg]^2 \\
  &amp;= E \Bigg[ Var(Y|X) \Bigg] + Var\Bigg[ E(Y|X) \Bigg]
  \end{aligned}\]</span>
</div>
<p></p>
<ol start="4" style="list-style-type: decimal">
<li><p>Suppose that an unknown number of individuals (denoted as <span class="math inline">\(N\)</span>) will be independently randomly chosen for “additional screening” by the TSA. Suppose that we allow <span class="math inline">\(N\sim Poisson(\lambda)\)</span> individuals are selected and that the probability that a selected person is female is <span class="math inline">\(0.4\)</span>. Let <span class="math inline">\(X\)</span> denote the number of females (out of <span class="math inline">\(N\)</span> individuals additionally screened) and so <span class="math inline">\(X|N \sim Binomial(N, p=0.4)\)</span>. Find <span class="math inline">\(Var(X)\)</span>.</p></li>
<li><p>The number of defects per yard in a certain fabric, <span class="math inline">\(Y\)</span>, was known to have a Poisson distribution with parameter <span class="math inline">\(\lambda\)</span>. The parameter <span class="math inline">\(\lambda\)</span> was assumed to be a random variable with a Exponential(1) distribution. Find <span class="math inline">\(E(Y)\)</span> and <span class="math inline">\(Var(Y)\)</span>. Notice that <span class="math inline">\(E(\lambda)=1\)</span> and <span class="math inline">\(1 = Var(Y|\lambda=1) &lt; Var(Y)\)</span>.</p></li>
<li><p>Suppose we have random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> with joint pdf <span class="math display">\[f(x,y) = (x+y) \, \cdot I(0 \le x \le 1)I(0 \le y \le 1)\]</span> Find <span class="math inline">\(E(Y|X)\)</span> and <span class="math inline">\(Var(Y|X)\)</span>. <em>Feel free to set up all the necessary integrals and then use software to calculate them.</em></p></li>
</ol>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="3-random-variables-and-distributions.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="5-common-distributions.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/dereksonderegger/edit/master/473/04_Expectation.Rmd",
"text": "Edit"
},
"download": [["Probability.pdf", "PDF"], ["Probability.epub", "EPUB"]],
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
