<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>STA 473 - Probability</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="STA 473 - Probability">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="STA 473 - Probability" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="dereksonderegger/473" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="STA 473 - Probability" />
  
  
  

<meta name="author" content="Derek L. Sonderegger">


<meta name="date" content="2017-04-11">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="3-random-variables-and-distributions.html">
<link rel="next" href="A-useful-functions.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="1-introduction-to-probability.html"><a href="1-introduction-to-probability.html"><i class="fa fa-check"></i><b>1</b> Introduction to Probability</a><ul>
<li class="chapter" data-level="1.1" data-path="1-introduction-to-probability.html"><a href="1-introduction-to-probability.html#history-of-probability"><i class="fa fa-check"></i><b>1.1</b> History of Probability</a></li>
<li class="chapter" data-level="1.2" data-path="1-introduction-to-probability.html"><a href="1-introduction-to-probability.html#interpretations-of-probability"><i class="fa fa-check"></i><b>1.2</b> Interpretations of Probability</a></li>
<li class="chapter" data-level="1.3" data-path="1-introduction-to-probability.html"><a href="1-introduction-to-probability.html#experiments-and-events"><i class="fa fa-check"></i><b>1.3</b> Experiments and Events</a></li>
<li class="chapter" data-level="1.4" data-path="1-introduction-to-probability.html"><a href="1-introduction-to-probability.html#review-of-set-theory-ds-1.4"><i class="fa fa-check"></i><b>1.4</b> Review of Set Theory (D&amp;S 1.4)</a></li>
<li class="chapter" data-level="1.5" data-path="1-introduction-to-probability.html"><a href="1-introduction-to-probability.html#definition-of-probability-ds-1.5"><i class="fa fa-check"></i><b>1.5</b> Definition of Probability (D&amp;S 1.5)</a></li>
<li class="chapter" data-level="1.6" data-path="1-introduction-to-probability.html"><a href="1-introduction-to-probability.html#finite-sample-spaces-ds-1.6"><i class="fa fa-check"></i><b>1.6</b> Finite Sample Spaces (D&amp;S 1.6)</a></li>
<li class="chapter" data-level="1.7" data-path="1-introduction-to-probability.html"><a href="1-introduction-to-probability.html#ordered-counting-ds-1.7"><i class="fa fa-check"></i><b>1.7</b> Ordered Counting (D&amp;S 1.7)</a></li>
<li class="chapter" data-level="1.8" data-path="1-introduction-to-probability.html"><a href="1-introduction-to-probability.html#combinations-ds-1.8"><i class="fa fa-check"></i><b>1.8</b> Combinations (D&amp;S 1.8)</a></li>
<li class="chapter" data-level="1.9" data-path="1-introduction-to-probability.html"><a href="1-introduction-to-probability.html#multinomial-coefficients-ds-1.9"><i class="fa fa-check"></i><b>1.9</b> Multinomial Coefficients (D&amp;S 1.9)</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-conditional-probability.html"><a href="2-conditional-probability.html"><i class="fa fa-check"></i><b>2</b> Conditional Probability</a><ul>
<li class="chapter" data-level="2.1" data-path="2-conditional-probability.html"><a href="2-conditional-probability.html#defining-conditional-probability-ds-2.1"><i class="fa fa-check"></i><b>2.1</b> Defining Conditional Probability (D&amp;S 2.1)</a></li>
<li class="chapter" data-level="2.2" data-path="2-conditional-probability.html"><a href="2-conditional-probability.html#independence"><i class="fa fa-check"></i><b>2.2</b> Independence</a></li>
<li class="chapter" data-level="2.3" data-path="2-conditional-probability.html"><a href="2-conditional-probability.html#bayes-theorem"><i class="fa fa-check"></i><b>2.3</b> Bayes’ Theorem</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html"><i class="fa fa-check"></i><b>3</b> Random Variables and Distributions</a><ul>
<li class="chapter" data-level="3.1" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#defining-random-variables-and-discrete-distributions"><i class="fa fa-check"></i><b>3.1</b> Defining Random Variables and Discrete Distributions</a></li>
<li class="chapter" data-level="3.2" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#continuous-distributions"><i class="fa fa-check"></i><b>3.2</b> Continuous Distributions</a></li>
<li class="chapter" data-level="3.3" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#cumulative-distribution-function"><i class="fa fa-check"></i><b>3.3</b> Cumulative Distribution Function</a></li>
<li class="chapter" data-level="3.4" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#bivariate-distributions"><i class="fa fa-check"></i><b>3.4</b> Bivariate Distributions</a><ul>
<li class="chapter" data-level="3.4.1" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#bivariate-discrete"><i class="fa fa-check"></i><b>3.4.1</b> Bivariate Discrete</a></li>
<li class="chapter" data-level="3.4.2" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#bivariate-continuous"><i class="fa fa-check"></i><b>3.4.2</b> Bivariate Continuous</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#marginal-distributions"><i class="fa fa-check"></i><b>3.5</b> Marginal Distributions</a><ul>
<li class="chapter" data-level="3.5.1" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#discrete-case"><i class="fa fa-check"></i><b>3.5.1</b> Discrete Case</a></li>
<li class="chapter" data-level="3.5.2" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#continuous-case"><i class="fa fa-check"></i><b>3.5.2</b> Continuous Case</a></li>
<li class="chapter" data-level="3.5.3" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#independence-1"><i class="fa fa-check"></i><b>3.5.3</b> Independence</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#conditional-distributions"><i class="fa fa-check"></i><b>3.6</b> Conditional Distributions</a></li>
<li class="chapter" data-level="3.7" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#functions-of-random-variables"><i class="fa fa-check"></i><b>3.7</b> Functions of Random Variables</a><ul>
<li class="chapter" data-level="3.7.1" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#cdf-method"><i class="fa fa-check"></i><b>3.7.1</b> CDF Method</a></li>
<li class="chapter" data-level="3.7.2" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#pdf-method"><i class="fa fa-check"></i><b>3.7.2</b> pdf Method</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#multivariate-transformations"><i class="fa fa-check"></i><b>3.8</b> Multivariate Transformations</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-expectations.html"><a href="4-expectations.html"><i class="fa fa-check"></i><b>4</b> Expectations</a><ul>
<li class="chapter" data-level="4.1" data-path="4-expectations.html"><a href="4-expectations.html#expectation-of-a-rv"><i class="fa fa-check"></i><b>4.1</b> Expectation of a RV</a></li>
<li class="chapter" data-level="4.2" data-path="4-expectations.html"><a href="4-expectations.html#properties-of-expectations"><i class="fa fa-check"></i><b>4.2</b> Properties of Expectations</a></li>
<li class="chapter" data-level="4.3" data-path="4-expectations.html"><a href="4-expectations.html#variance"><i class="fa fa-check"></i><b>4.3</b> Variance</a></li>
<li class="chapter" data-level="4.4" data-path="4-expectations.html"><a href="4-expectations.html#moments-and-moment-generating-functions"><i class="fa fa-check"></i><b>4.4</b> Moments and Moment Generating Functions</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="A-useful-functions.html"><a href="A-useful-functions.html"><i class="fa fa-check"></i><b>A</b> Useful functions</a><ul>
<li class="chapter" data-level="A.1" data-path="A-useful-functions.html"><a href="A-useful-functions.html#gamma-function"><i class="fa fa-check"></i><b>A.1</b> Gamma Function</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">STA 473 - Probability</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="expectations" class="section level1">
<h1><span class="header-section-number">Chapter 4</span> Expectations</h1>
<div id="expectation-of-a-rv" class="section level2">
<h2><span class="header-section-number">4.1</span> Expectation of a RV</h2>
<p>The sample mean is a useful measure of centrality of a set of data and we would like a similar quantity for a distribution.</p>
<p>Suppose we have a sample from a distribution that can take on integer values in the range of <span class="math inline">\(1\)</span> to <span class="math inline">\(5\)</span>. For example suppose we have the data <span class="math inline">\(\{ 1,1,2,3,3,3,4,5 \}\)</span>. Then the sample mean is <span class="math display">\[\begin{aligned} \bar{x} 
  &amp;= \frac{1}{n} \sum_{j=1}^n x_j = \frac{1}{8} (1+1+2+3+3+3+4+5)\\
  &amp;= \sum_{i=1}^5 \hat{p}_i \;i = \left(\frac{2}{8} \cdot 1 \right) + 
                            \left(\frac{1}{8} \cdot 2 \right) +
                            \left(\frac{3}{8} \cdot 3 \right) +
                            \left(\frac{1}{8} \cdot 4 \right) +
                            \left(\frac{1}{8} \cdot 5 \right) 
  \end{aligned}\]</span> where <span class="math inline">\(\hat{p}_i\)</span> values are the observed proportions for each possible value. If we have a really large sample then <span class="math inline">\(\hat{p}_i \approx Pr(X=i)\)</span> and it is natural to define the Expected Value as <span class="math display">\[E(X) = \begin{cases}
    \sum x \, f(x) \;\;\;\;\;\;\;\;\;\;\;\; \textrm{ if } X \textrm{ is discrete} \\
    \int_{-\infty}^\infty x \, f(x) \,dx \;\;\;\;\; \textrm{ if } X \textrm{ is continuous} 
    \end{cases}\]</span></p>
<p>We need to be careful to avoid the <span class="math inline">\(\infty - \infty\)</span> case and note that if <span class="math display">\[\sum_{\textrm{Negative }x} x\,f(x) = -\infty \;\;\;\;\; \textrm{ and } \;\;\;\;\;\sum_{\textrm{Positive }x} x\,f(x) = \infty\]</span> or <span class="math display">\[\int_{-\infty}^0 x\,f(x)\, dx = -\infty \;\;\;\;\;\; \textrm{ and } \;\;\;\;\;\int_{0}^\infty x\,f(x)\,dx = \infty\]</span> then the resulting expectation could be written as <span class="math inline">\(-\infty + \infty\)</span> and that quantity <em>does not exist</em>.</p>
<p><strong>Example</strong> Suppose that the lifetime, <span class="math inline">\(X\)</span>, of an applience has a pdf <span class="math display">\[f(x) = 2 e^{-2x}\;\cdot I(x&gt;0)\]</span> Then the expectation of <span class="math inline">\(X\)</span> is <span class="math display">\[E(X) = \int_{-\infty}^{\infty} x \,f(x) \, dx = \int_0^\infty x \, 2e^{-2x} \, dx = 2\int_0^\infty x \, e^{-2x} \, dx\]</span> To finish solving this integral, we need to do integration by parts letting <span class="math display">\[\begin{aligned} 
  u =2x    &amp; \;\;\;\;&amp; dv =   e^{-2x}\,dx \\
  du= 2\,dx  &amp;         &amp; v = -\frac{1}{2} e^{-2x} 
  \end{aligned}\]</span> and therefore <span class="math display">\[\begin{aligned} E(X) 
  &amp;= -xe^{-2x} \Big\vert_0^\infty+ \int_0^\infty e^{-2x}\,dx \\
  &amp;= 0 + -\frac{1}{2} e^{-2x} \Big\vert_0^\infty \\
  &amp;= \frac{1}{2}
  \end{aligned}\]</span></p>
<p><strong>Expectations of functions of a RV</strong> Suppose we have a random variable <span class="math inline">\(X\)</span> and some function <span class="math inline">\(Y=r(X)\)</span>, then I might want to know the expectation of the random variable <span class="math inline">\(Y\)</span>. We could just derive the pdf of <span class="math inline">\(Y\)</span> and calculate its expectation, but that is just a bunch of integration and differientation that cancels out because (<em>in the continuous case and assuming <span class="math inline">\(r(x)\)</span> is invertable</em>) <span class="math display">\[\begin{aligned} E(Y) 
  &amp;= \int y \, g(y) \, dy \\
  &amp;= \int r(x)\, f\big( r(x) \big) \Bigg\vert \frac{dx}{dy} \Bigg\vert \,dy \\
  &amp;= \int r(x)\, f(x)  \,dx 
  \end{aligned}\]</span></p>
<p>We could prove that if the expectation of <span class="math inline">\(Y=r(X)\)</span> exists, then for any function <span class="math inline">\(r(x)\)</span> is equal to <span class="math display">\[E(Y) = E[r(X)] = \begin{cases}
  \sum r(x) \,f(x) \\
  \int r(x) \,f(x) \, dx
  \end{cases}\]</span></p>
<p><strong>Example</strong> Suppose that we have a random variable with pdf <span class="math display">\[f(x) = 3x^2 \, I(0&lt;x&lt;1)\]</span> then <span class="math display">\[E(X) = \int_0^1 x\, f(x) \,dx = \int_0^1 x \, 3x^2 \,dx = \int_0^1 3 x^3 \, dx = \frac{3}{4}x^4 \Bigg\vert_0^1 = \frac{3}{4}\]</span> and if we consider <span class="math inline">\(Y=X^2\)</span> then <span class="math display">\[E(Y) = E(X^2) = \int_0^1 x^2\, f(x)\, dx = \int_0^1 3x^4 = \frac{3}{5}x^5\Bigg\vert_0^1 = \frac{3}{5}\]</span></p>
<p>Notice that <span class="math inline">\(E(X^2) \ne \Bigg( E(X) \Bigg)^2\)</span>. For general <span class="math inline">\(r(X)\)</span>, we typically see that <span class="math inline">\(E(r(X)) \ne r\Bigg( E(X) \Bigg)\)</span>. However for linear functions, it is true.</p>
<ol style="list-style-type: decimal">
<li><p>Suppose that <span class="math inline">\(X\)</span> has a Uniform(<span class="math inline">\(a\)</span>, <span class="math inline">\(b\)</span>) distribution. Find the expected value of <span class="math inline">\(X\)</span>.</p></li>
<li><p>Suppose that <span class="math inline">\(X\)</span> has a Uniform(<span class="math inline">\(a\)</span>, <span class="math inline">\(b\)</span>) distribution. Find the expected value of <span class="math inline">\(Y=\sqrt X\)</span>.</p></li>
<li><p>Suppose that <span class="math inline">\(X\)</span> has a Uniform(<span class="math inline">\(a\)</span>, <span class="math inline">\(b\)</span>) distribution. Find the expected value of <span class="math inline">\(Y=1/X\)</span>.</p></li>
<li><p>A 1-meter stick is broken at a random spot along the stick. Find the expected value of the length of the longer piece.</p></li>
</ol>
<p><strong>Expectations of functions of several variables</strong> Suppose that a multivariate distribution with joint pdf <span class="math inline">\(f(x_1, x_2, \dots, x_n)\)</span> and we define <span class="math inline">\(Y=r(X_1,X_2,\dots,X_n)\)</span> then <span class="math display">\[E(Y) = \iint\dots\int r(x_1, x_2, \dots, x_n)\, f(x_1, x_2, \dots, x_n)\, dx_1 dx_2 \dots dx_n\]</span> <strong>Example</strong> Suppose that we have a bivariate distribution <span class="math display">\[f(x_1, x_2) = 8x_1 x_2 \, \cdot I(0 &lt; x_1 &lt; x_2 &lt; 1)\]</span> and we wish to know the expectation of <span class="math inline">\(Y=X_1+X_2\)</span>. Then <span class="math display">\[\begin{aligned} E( X_1+X_2 ) 
  &amp;= \int_0^1 \int_0^{x_2} (x_1+x_2) \, 8x_1x_x \; dx_1 dx_2 \\
  &amp;= \vdots \\
  &amp;= \frac{4}{3}
  \end{aligned}\]</span></p>
</div>
<div id="properties-of-expectations" class="section level2">
<h2><span class="header-section-number">4.2</span> Properties of Expectations</h2>
<ol style="list-style-type: decimal">
<li><p>Prove that for finite constants <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> and continuous random variable <span class="math inline">\(X\)</span>, we have <span class="math display">\[E(aX + b) = aE(X) + b\]</span></p></li>
<li><p>Show that if continuous random variable <span class="math inline">\(X\)</span> has support on the interval <span class="math inline">\((a,b)\)</span> where <span class="math inline">\(a&lt;b\)</span>, then <span class="math inline">\(a &lt; E(X) &lt; b\)</span>. This is true in the discrete case as well.</p></li>
<li><p>Show that if <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are (possibly not independent!) continuous random variables with joint pdf <span class="math inline">\(f(x_1, x_2)\)</span> that <span class="math inline">\(E( X_1 + X_2 ) = E(X_1) + E(X_2)\)</span>. By induction this result will hold for the sum of a finite number number random variables. Notice the proof for the discrete case is similar with simply replacing integrals with summations.</p></li>
<li><p>Suppose that three random variables <span class="math inline">\(X_1, X_2, X_3\)</span> are sampled from a distribution that has mean <span class="math inline">\(E(X_i)=5\)</span>. Find the expectation of <span class="math inline">\(2X_1 - 3X_2 -X_3 - 5\)</span>. <em>When we say <span class="math inline">\(X_1, X_2, \dots\)</span> are sampled from a distribution, we actually mean that <span class="math inline">\(X_1,X_2,\dots\)</span> are independent and each have marginal distribution as given. So when you heard “We sampled from population …” in your Introduction to Statistics course, they actually were telling you that the observations are independent.</em></p></li>
<li><p>Suppose that three random variables <span class="math inline">\(X_1, X_2, X_3\)</span> are sampled from a uniform distribution on <span class="math inline">\((0,1)\)</span>. What is the expectation of <span class="math inline">\((X_1 - 2X_2 + X_3)^2\)</span>.</p></li>
<li><p><em>Bernoulli Expectation</em> Suppose that the random variable <span class="math inline">\(X_i\)</span> can take on values <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span> and the probability it takes on <span class="math inline">\(1\)</span> is <span class="math inline">\(f(1)=p\)</span>. What is the expected value of <span class="math inline">\(X_i\)</span>?</p></li>
<li><p><em>Binomial Expectation</em> Suppose that we have <span class="math inline">\(n\)</span> indentically distributed Bernoulli random variables, each of which having probability of success <span class="math inline">\(f_i(1)=p\)</span>. Letting <span class="math inline">\(Y=\sum_1^n X_i\)</span>, what is the expected value of <span class="math inline">\(Y\)</span>?</p></li>
<li><em>Hypergeometric Expectation</em> Suppose that we have a bag with <span class="math inline">\(N\)</span> balls, of which <span class="math inline">\(M\)</span> are red and <span class="math inline">\(N-M\)</span> are blue. We will draw <span class="math inline">\(n\)</span> balls out (without replacement) and we are interested in the total number of red balls drawn. Let <span class="math inline">\(X_i\)</span> be <span class="math inline">\(1\)</span> if the <span class="math inline">\(i\)</span>th draw was a red ball.
<ol style="list-style-type: lower-alpha">
<li>First consider the case where we draw <span class="math inline">\(n=1\)</span> ball. What is the probability that we draw a red ball on the first draw and therefore what is <span class="math inline">\(E(X_1)\)</span>?</li>
<li>Next consider the case where we draw <span class="math inline">\(n=2\)</span> balls. What is the probability that we draw a red ball on the second draw and therefore what is <span class="math inline">\(E(X_2)\)</span>?</li>
<li>The same pattern holds for the rest of <span class="math inline">\(X_3, X_4, \dots X_n\)</span>. Given that, what is the expected value of <span class="math inline">\(Y=\sum_1^n X_i\)</span>?</li>
</ol></li>
<li><p>Suppose that continuous random variables <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> are independent, each having a marginal pdf <span class="math inline">\(f_i(x_i)\)</span>. Show that <span class="math display">\[E\left( \prod_{i=1}^n X_i \right) = \prod_{i=1}^n E(X_i)\]</span> Notice that this result requires that the variables are independent, whereas the result in 4.2.3 did not require independence.</p></li>
<li><p>A gambler will play a game where he is equally likey to win or lose on a given play. When the gambler wins, her fortune is doubled, but when she loses, it is cut in half. Given that the gambler started the game with a fortune of <span class="math inline">\(c\)</span>, what is the expected fortune after <span class="math inline">\(n\)</span> plays?</p></li>
</ol>
<p>It can be shown that for non-negative, continuous random variables <span class="math display">\[E(X) = \int_0^\infty (1-F(x))\,dx\]</span> and for non-negative discrete random variables <span class="math display">\[E(X) = \sum_{x=1}^\infty Pr( X \ge x )\]</span></p>
<p>The proof in the discrete case is a reordering of <span class="math display">\[E(X) = \sum_{x=0}^\infty x\,f(x) = \sum_x^\infty x\, Pr(X=x)\]</span> to summing one copy of <span class="math inline">\(Pr(X=1)\)</span> and two copies of <span class="math inline">\(Pr(X=2)\)</span> and three copies of <span class="math inline">\(Pr(X=3)\)</span> and so on.</p>
<table>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(E(X)\)</span></td>
<td align="center">=</td>
<td align="center">0</td>
<td align="center"><span class="math inline">\(+ Pr(X=1)\)</span></td>
<td align="center"><span class="math inline">\(+Pr(X=2)\)</span></td>
<td align="center"><span class="math inline">\(+Pr(X=3)\)</span></td>
<td align="center"><span class="math inline">\(\dots\)</span></td>
</tr>
<tr class="even">
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"><span class="math inline">\(+Pr(X=2)\)</span></td>
<td align="center"><span class="math inline">\(+Pr(X=3)\)</span></td>
<td align="center"><span class="math inline">\(\dots\)</span></td>
</tr>
<tr class="odd">
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"><span class="math inline">\(+Pr(X=3)\)</span></td>
<td align="center"><span class="math inline">\(\dots\)</span></td>
</tr>
<tr class="even">
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"><span class="math inline">\(\ddots\)</span></td>
</tr>
</tbody>
</table>
<p>Notice that the first row of this sum is <span class="math inline">\(Pr(X\ge1)\)</span> and the second is <span class="math inline">\(Pr(X\ge 2)\)</span> and that establishes the result.</p>
<ol start="11" style="list-style-type: decimal">
<li><p><em>Geometric Expectation</em> Suppose that each time a person plays a game, they have a probability <span class="math inline">\(p\)</span> of winning. Let the random variable <span class="math inline">\(X\)</span> be the number of games played until the person wins. We have previously shown that <span class="math display">\[f(x) = (1-p)^{x-1}p \;\; I(x\in \{1,2,\dots\})\]</span> <span class="math display">\[Pr(X \le x) = 1 - (1-p)^x\]</span> for <span class="math inline">\(x \in \{1,2,\dots\}\)</span> What is expected number of times a player must play until they win?</p></li>
<li><p><em>Gamma Expectation</em> Suppose that we have a random variable <span class="math inline">\(X\)</span> with a <span class="math inline">\(Gamma(\alpha, \beta)\)</span> distribution and therefore <span class="math display">\[f(x) = \frac{\beta^\alpha}{\Gamma(\alpha)} x^{\alpha-1} e^{-x\beta} \;\cdot I(x&gt;0)\]</span> Show that <span class="math display">\[E(X) = \frac{\alpha}{\beta}\]</span></p></li>
</ol>
</div>
<div id="variance" class="section level2">
<h2><span class="header-section-number">4.3</span> Variance</h2>
<p>Although the mean of a distribution is quite useful, it is not the only measure of interest. A secondary measure of interest is a measure of <em>spread</em> of the distribution. Just as the sample variance is interpreted as the “typical squared distance to the mean” we will define the distribution variance as the “expected squared distance to the mean”.</p>
<p>For notational convenience, let <span class="math inline">\(\mu=E(X)\)</span> and define <span class="math display">\[Var(X) = E\big[ (X-\mu)^2 \big]\]</span></p>
<p>Because expectations don’t necessarily exist, we’ll say that <span class="math inline">\(Var(X)\)</span> does not exist if <span class="math inline">\(E(X)\)</span> does not exist or if <span class="math inline">\(E[(X-\mu)^2]\)</span> does not exist. Notice that the Variance is non-negative because of the square.</p>
<p>Finally, we will define the standard deviation of <span class="math inline">\(X\)</span> as the positive square-root of the variance. That is <span class="math inline">\(StdDev(X) = \bigg\vert \sqrt{Var(X)} \bigg\vert\)</span>.</p>
<p>Notationally all of this is a bit cumbersome and we’ll use <span class="math display">\[E(X) = \mu_X\;\;\;\;\;\;\;\;\;\; Var(X) = \sigma^2_X \;\;\;\;\;\;\;\;\;\; StdDev(X) = \sigma_X\]</span> If we have only a single random variable in a situation, we will suppress the subscript.</p>
<ol style="list-style-type: decimal">
<li>Suppose that RV <span class="math inline">\(X\)</span> has <span class="math inline">\(Var(X)\)</span> that exists, then for constants <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>, show that the RV <span class="math display">\[Y=aX+b\]</span> has variance <span class="math display">\[Var(Y) = a^2 Var(X)\]</span> Notice that shifting the entire distribution of <span class="math inline">\(X\)</span> by some constant <span class="math inline">\(b\)</span> does not affect the <em>spread</em> of the shifted distribution.</li>
</ol>
<p>Next we condider the sum of independent random variables <span class="math inline">\(X_1+X_2\)</span>. <span class="math display">\[\begin{aligned} Var( X_1 + X_2 ) 
  &amp;= E\Bigg[ \bigg[ (X_1 + X_2) - E(X_1+X_2) \bigg]^2 \Bigg] \\
  &amp;= E\Bigg[ \bigg[ X_1 + X_2 - \mu_1 - \mu_2 \bigg]^2 \Bigg] \\
  &amp;= E\Bigg[ \bigg[ (X_1 -\mu_1) + (X_2 - \mu_2)\bigg]^2 \Bigg] \\
  &amp;= E\Bigg[  (X_1 -\mu_1)^2 + 2(X_1-\mu_1)(X_2-\mu_2) + (X_2 - \mu_2)^2  \Bigg] \\
  &amp;= E\big[  (X_1 -\mu_1)^2 \big] + E\big[ 2(X_1-\mu_1)(X_2-\mu_2)\big]  + E\big[(X_2 - \mu_2)^2  \big] \\
  &amp;= Var(X_1) \;\;\;\;\;\;\;\;\;+ 2E\big[ (X_1-\mu_1)(X_2-\mu_2)\big]  + Var(X_2) 
  \end{aligned}\]</span> Because <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are independent then <span class="math display">\[E\big[ (X_1-\mu_1)(X_2-\mu_2)\big] = E[(X_1-\mu_1)] E[(X_2-\mu_2)] = (\mu_1 -\mu_1) (\mu_2-\mu_2) = 0\]</span> Repeating this argument for <span class="math inline">\(n\)</span> independent random variables, we therefore have <span class="math display">\[Var( X_1+X_2+\dots+X_n ) = Var(X_1) + Var(X_2) + \dots + Var(X_n)\]</span> <span class="math display">\[Var\Bigg( \sum_{i=1}^n X_i \Bigg) = \sum_{i=1}^n Var(X_i)\]</span> Notice that this result <em>requires</em> independence so that the cross-product terms are zero!</p>
<ol start="2" style="list-style-type: decimal">
<li><p>Show that <span class="math inline">\(Var(X) = E(X^2) - \mu^2\)</span>. This formula is far more convenient to use, generally.</p></li>
<li><p>Suppose that <span class="math inline">\(X\)</span> has a uniform distribution on the interval <span class="math inline">\([0,1]\)</span>. Compute the variance of <span class="math inline">\(X\)</span>.</p></li>
<li><p>Suppose that <span class="math inline">\(Y\)</span> has a uniform distribution on the interval <span class="math inline">\([a,b]\)</span> where <span class="math inline">\(a&lt;b\)</span>. Compute the variance of <span class="math inline">\(Y\)</span>.</p></li>
<li><p>Suppose that <span class="math inline">\(X\)</span> has expectation <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>. Show that <span class="math display">\[E\bigg[ X(X-1) \bigg]=\mu(\mu-1) + \sigma^2\]</span></p></li>
<li><p>Suppose that <span class="math inline">\(X\)</span> has a <span class="math inline">\(Gamma(\alpha,\beta)\)</span> distribution. Find that the variance of <span class="math inline">\(X\)</span> is <span class="math inline">\(\frac{\alpha}{\beta^2}\)</span>.</p></li>
<li><p>Suppose that <span class="math inline">\(X\)</span> has a <span class="math inline">\(Bernoulli(p)\)</span> distribution, that is <span class="math inline">\(X\)</span> takes on values <span class="math inline">\(1\)</span> or <span class="math inline">\(0\)</span> with probabilities <span class="math inline">\(Pr(X=1)=p\)</span> and <span class="math inline">\(Pr(X=0) = 1-p\)</span>. Find the variance of <span class="math inline">\(X\)</span>.</p></li>
<li><p>Suppose that <span class="math inline">\(Y\)</span> has a <span class="math inline">\(Binomial(n,p)\)</span> distribution. That is that <span class="math display">\[Y=\sum_{i=1}^n X_i\]</span> where <span class="math inline">\(X_i\)</span> are independent <span class="math inline">\(Bernoulli(p)\)</span> random variables. Show that <span class="math display">\[Var(Y) = np(1-p)\]</span></p></li>
</ol>
</div>
<div id="moments-and-moment-generating-functions" class="section level2">
<h2><span class="header-section-number">4.4</span> Moments and Moment Generating Functions</h2>

<div class="definition">
<p><span id="def:unnamed-chunk-8" class="definition"><strong>Definition 4.1 </strong></span>Just as the <span class="math inline">\(E(X)\)</span> defines the center of a distribution and <span class="math inline">\(E[ (X-\mu)^2 ] = E(X^2)-\mu^2\)</span> defines the variance, the quantities <span class="math display">\[M_k = E\big( X^k )\;\;\;\;\;\;\textrm{ where } k\in \{1,2,3,\dots\}\]</span></p>
are what we call the <span class="math inline">\(k\)</span>th moment of the distribution. These moments define other attributes of the distribution, but sometimes it is useful to define a similar quantity called the <span class="math inline">\(k\)</span>th <em>central moment</em> <span class="math display">\[m_k = E\big( ( X-\mu ) ^k )\;\;\;\;\;\;\textrm{ where } k\in \{1,2,3,\dots\}\]</span>
</div>
<p></p>
<p>These two quantities can define several aspects of the distribution. For example, <span class="math inline">\(M_1=E(X)\)</span> is the distribution mean, while <span class="math inline">\(M_2=E(X^2)\)</span> is related to the variance. Other attributes are related to higher moments (e.g. the distribution skew is related to <span class="math inline">\(m_3\)</span>).</p>
<p>Somewhat obnoxiously, the book defines <span class="math inline">\(M_k\)</span> to exist if and only if <span class="math inline">\(E\Big(|X|^k\Big) &lt; \infty\)</span>. <em>(This is obnoxious because we used a different criteria to say if <span class="math inline">\(E(X)\)</span> existed in section 4.1 and the definition for the <span class="math inline">\(k\)</span>th moment is a more strict requirement.)</em></p>

<div class="theorem">
<span id="thm:unnamed-chunk-9" class="theorem"><strong>Theorem 4.1 </strong></span>For positive integers <span class="math inline">\(j&lt;k\)</span>, if <span class="math inline">\(M_k\)</span> exists, then <span class="math inline">\(M_j\)</span> must also exist.
</div>
 
<div class="proof">
<span class="proof"><em>Proof. </em></span> <span class="math display">\[\begin{aligned} E\Big( |X|^j \Big) 
  &amp;=   \int_{-\infty}^{\infty} |x|^j \,f(x) \,dx \\
  &amp;=   \int_{|x|\le 1} |x|^j \, f(x) \,dx + \int_{|x|&gt;1} |x|^j \, f(x) \, dx \\
  &amp;\le \int_{|x|\le 1} 1 \, f(x)\,dx + \int_{|x|&gt;1} |x|^k \, f(x) \, dx \\
  &amp;\le 1 + M_k\\
  &amp;&lt; \infty \textrm{ by assumption }
  \end{aligned}\]</span>
</div>
<p></p>

<div class="definition">
<span id="def:unnamed-chunk-11" class="definition"><strong>Definition 4.2 </strong></span>Let <span class="math inline">\(X\)</span> be a random variable. For each real number <span class="math inline">\(t\)</span>, define <span class="math display">\[\psi(t) = E( e^{tX})\]</span> as the <em>Moment Generating Function of <span class="math inline">\(X\)</span>, which we denote mgf of <span class="math inline">\(X\)</span>.</em>
</div>
<p></p>

<div class="theorem">
<span id="thm:unnamed-chunk-12" class="theorem"><strong>Theorem 4.2 </strong></span>Let <span class="math inline">\(X\)</span> be a random variable whose mgf <span class="math inline">\(\psi(t)\)</span> is finite for some neighborhood about <span class="math inline">\(t=0\)</span>. Then for positive integer <span class="math inline">\(k\)</span>, the <span class="math inline">\(k\)</span>th momement is the <span class="math inline">\(k\)</span>th derivative of <span class="math inline">\(\psi(t)\)</span> evaluated at <span class="math inline">\(t=0\)</span>. That is <span class="math display">\[M_k = E(X^k) = \psi^{(k)}(0)\]</span>
</div>
 
<div class="proof">
<span class="proof"><em>Proof. </em></span> A full proof is quite technical, but it revolves around showing that it is permissible to interchange the order of integration/summation and differentiation in this case and that therefore: <span class="math display">\[\begin{aligned} \psi^{(n)}(0) 
  &amp;= \frac{d^n}{dt^n} E(e^tX) \Bigg\vert_{t=0} \\
  &amp;= E\Bigg[ \Big( \frac{d^n}{dt^n} e^{tX}  \Big) _{t=0} \Bigg] \\
  &amp;= E\Big[ X^n e^{tX}\vert_{t=0}\Big] \\
  &amp;= E\big[X^n\big]
  \end{aligned}\]</span>
</div>
<p></p>

<div class="theorem">
<span id="thm:unnamed-chunk-14" class="theorem"><strong>Theorem 4.3 </strong></span>If the mgfs of two random variables are finite and identical for all values of <span class="math inline">\(t\)</span> in a neighborhood of <span class="math inline">\(t=0\)</span>, then the probability distributions of the two variables are the same.
</div>
 
<div class="remark">
<span class="remark"><em>Remark. </em></span> This theorem allows us to compare the mgf of some variable of interest to the set of known mgfs and claim that because the mgfs match, then the variable of interest must follow the matching distribution. This is often a very easy way to show that a variable has a particular distribution and is the reason that we have introduced moment generating functions.
</div>
<p></p>
<ol style="list-style-type: decimal">
<li><p>Suppose that the random variable <span class="math inline">\(X\)</span> has an <span class="math inline">\(Exponential(\beta)\)</span> distribution which is a special case of the Gamma distribution. <span class="math inline">\(Exponential(\beta) = Gamma(1,\beta)\)</span> distribution. The table of distributions in your book shows that the <span class="math inline">\(Var(X)=\beta^2\)</span>. Derive the mgf of the Exponential distribution and use it to derive both the expectation and variance.</p></li>
<li><p><strong>Mgf of a linear transformation</strong> Suppose that we have a random variable <span class="math inline">\(X\)</span> with mgf <span class="math inline">\(\psi_1(t)\)</span>. Show that <span class="math inline">\(Y=aX+b\)</span> for real constants <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> has the mgf <span class="math display">\[\psi_2(t) = e^{bt}\psi_1(at)\]</span></p></li>
<li><p>Suppose that <span class="math inline">\(X \sim Exponential(\beta)\)</span>. Show that <span class="math inline">\(Y=2X\)</span> has the mgf of an <span class="math inline">\(Exponential(\beta/2)\)</span> distribution and therefore <span class="math inline">\(Y \sim Exponential(\beta/2)\)</span>.</p></li>
<li><p>Derive the moment generating function of the <span class="math inline">\(Gamma(\alpha, \beta)\)</span> distribution.</p></li>
<li><p>Derive the moment generating function of <span class="math inline">\(Bernoulli(p)\)</span> distribution.</p></li>
<li><p>Suppose that independent random variables <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> each have mgfs <span class="math inline">\(\psi_1(t), \psi_2(t), \dots, \psi_n(t)\)</span>. Show that the mgf of <span class="math inline">\(Y=\sum X_i\)</span> is <span class="math display">\[\psi_Y(t) = \prod_{i=1}^n \psi_i(t)\]</span></p></li>
<li><p>Suppose that <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> are independent <span class="math inline">\(Bernoulli(p)\)</span> random variables. Derive the mgf of <span class="math inline">\(Y=\sum X_i\)</span>. That is, derive the mgf of the Binomial distribution.</p></li>
<li><p>Suppose that random variable <span class="math inline">\(X\)</span> has support on the non-negative integers <span class="math inline">\(0, 1, 2, \dots\)</span>. The probability function of <span class="math inline">\(X\)</span> is <span class="math display">\[f(x)=\frac{\lambda^x}{x!}e^{-\lambda}\,\cdot I(x\in \{0,1,2,\dots\})\]</span> The random variable <span class="math inline">\(X\)</span> has a <span class="math inline">\(Poisson(\lambda)\)</span> distribution. Derive the mgf of this distribution. <em>Hint: what is the summation constant in the pf?</em></p></li>
<li><p>Suppose that <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> are indepenent random variables each with distribution <span class="math inline">\(Poisson(\lambda)\)</span>. Derive the moment generating function of the distribution of <span class="math inline">\(Y=\sum X_i\)</span> and state its distribution.</p></li>
</ol>

</div>
</div>



            </section>

          </div>
        </div>
      </div>
<a href="3-random-variables-and-distributions.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="A-useful-functions.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/dereksonderegger/edit/master/473/04_Expectation.Rmd",
"text": "Edit"
},
"download": [["Probability.pdf", "PDF"], ["Probability.epub", "EPUB"]],
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
