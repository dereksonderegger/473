[
["index.html", "STA 473 - Probability Preface Acknowledgements", " STA 473 - Probability Derek L. Sonderegger 2017-02-27 Preface This is a set of questions to be used in an Inquiry Based Learning class for an undergraduate level course in probability. Acknowledgements Many people have helped. I should thank them. "],
["1-introduction-to-probability.html", "Chapter 1 Introduction to Probability 1.1 History of Probability 1.2 Interpretations of Probability 1.3 Experiments and Events 1.4 Review of Set Theory (D&amp;S 1.4) 1.5 Definition of Probability (D&amp;S 1.5) 1.6 Finite Sample Spaces (D&amp;S 1.6) 1.7 Ordered Counting (D&amp;S 1.7) 1.8 Combinations (D&amp;S 1.8) 1.9 Multinomial Coefficients (D&amp;S 1.9)", " Chapter 1 Introduction to Probability 1.1 History of Probability 1.2 Interpretations of Probability 1.3 Experiments and Events 1.4 Review of Set Theory (D&amp;S 1.4) Create a sample space \\(\\mathcal{S}\\) where Where the the number of outcomes is finite. Define events (subsets of \\(\\mathcal{S}\\)) that do not have a 1-to-1 correspondence with the outcomes. Create a sample space \\(\\mathcal{S}\\) where Where the the number of outcomes is countably infinite. Define a finite number of events (subsets of \\(\\mathcal{S}\\)) that do not have a 1-to-1 correspondence with the outcomes and that the union of all your events is \\(\\mathcal{S}\\). Define an infinite number of events (subsets of \\(\\mathcal{S}\\)) that do not have a 1-to-1 correspondence with the outcomes and that the union of all your events is \\(\\mathcal{S}\\). Create a sample space \\(\\mathcal{S}\\) where Where the the number of outcomes is uncountably infinite. Define a finite number of events (subsets of \\(\\mathcal{S}\\)) that do not have a 1-to-1 correspondence with the outcomes and that the union of all your events is \\(\\mathcal{S}\\). Define an countably infinite number of events (subsets of \\(\\mathcal{S}\\)) that do not have a 1-to-1 correspondence with the outcomes and that the union of all your events is \\(\\mathcal{S}\\). It is time to define the set of events more carefully. The take-home idea is that if you add an event, say \\(A\\), you also add some other events related to \\(A\\). The rules are summarized below. \\(\\mathcal{S}\\) is an event. This is to say something will happen. If \\(A\\) is an event, then \\(A^c\\) is also an event. If \\(A_i\\) is a countable sequence of events, then \\(\\cup_{i=1}^\\infty A_i\\) is also an event. Prove that \\(\\emptyset\\) is is an event. Prove two of the conclusions of theorem 1.4.4. I would expect a proof of \\(A \\cup A^c=\\mathcal{S}\\) to look something like, “Let \\(e\\) be an arbitrary event in \\(\\mathcal{S}\\). Due to the nature of complements, either \\(e \\in A\\) or \\(e \\in A^c\\). Therefore \\(e in A \\cup A^c\\) but because \\(e\\) was an arbitrary element of \\(\\mathcal{S}\\) then \\(\\mathcal{S} \\subset A \\cup A^c\\). However because \\(\\mathcal{S}\\) is the set of all possible events, then \\(A \\cup A^c \\subset \\mathcal{S}\\) and thus \\(A \\cup A^c = \\mathcal{S}\\).” Chapter problem 1.4.1. Suppose \\(A \\subset B\\). Show that \\(B^c \\subset A^c\\). Do this in a similar fashion as problem 5. Chapter problem 1.4.2. Show this by Venn diagrams. Chpater problem 1.4.3. Prove DeMorgan’s Laws. Prove this via Venn diagrams. Chapter problem 1.4.6. Chapter problem 1.4.7 Chapter problem 1.4.13 Chapter problem 1.4.14 1.5 Definition of Probability (D&amp;S 1.5) Axiom 1 For every event \\(A\\), the probabilitity of the event, denoted \\(Pr(A)\\) has the property \\(Pr(A) \\ge 0\\) Axiom 2 If an event is sure to occur, then the event has probability 1. That is, \\(Pr(\\mathcal{S})=1\\). Axiom 3 For every finite or countably infinite sequence of events \\(A_1, \\, A_2, \\, \\dots\\) where \\(A_i \\cap A_j = \\emptyset \\;\\;\\forall\\; i,j\\) (that is the sequence \\(A_i\\) is pairwise disjoint), then \\[Pr\\left( \\bigcup_{i=1}^\\infty A_i \\right) = \\sum_{i=1}^\\infty Pr\\left(A_i\\right)\\] Consider drawing a single card from a well shuffled deck of playing cards (4 suits, each with 13 cards Ace, two, …, Queen, King). Consider the events \\(H,S,C,D\\), which are drawing a \\(H\\)eart, \\(S\\)pade, \\(C\\)lub, and \\(D\\)iamond. Explain why \\(H\\) and \\(S\\) are disjoint but \\(H^c\\) and \\(S^c\\) are not. Prove \\(Pr(\\emptyset) = 0\\) Argue that Axiom 3 should have been “For every countably infinite sequence of events \\(A_i\\)” because you can pad any finite sequence with an infinite sequence of empty sets. Prove \\(Pr(A^c) = 1 - Pr(A)\\) Often we will draw Venn diagrams where the area of the event is its probability. Many of the probability calculations can be most easily understood using a Venn diagram along with the algebraic proof. Prove if \\(A \\subset B\\) then \\(Pr(A) \\le Pr(B)\\). Show this formally and via Venn diagrams Prove that \\(Pr(A) = Pr(A \\cap B) + Pr( A \\cap B^c)\\) Show this formally and via Venn diagrams Prove that \\(Pr( A \\cup B) = Pr(A) + Pr(B) - Pr( A \\cap B)\\) Show this formally and via Venn diagrams. Notice our Axiom 3 addresses the case where \\(A\\) and \\(B\\) are disjoint. Consider events \\(A\\) and \\(B\\) where \\(Pr(A)=1/3\\) and \\(Pr(B) = 1/2\\). Determine the value of \\(Pr(A \\cap B^c)\\) when \\(A\\) and \\(B\\) are disjoint \\(A \\subset B\\) \\(Pr(A \\cap B) = 1/8\\) Suppose that Adam has a probability of failing an exam of \\(Pr(A) = 0.5\\) while Bob only has a probability of failing the exam of \\(Pr(B) = 0.2\\). Suppose the probability of both students failing is \\(0.1\\) What is the probability that at least one of these two students will fail? What is the probability that neither student will fail? What is the probability that exactly one student will fail? A point \\((x,y)\\) is to be selected from the unit square \\(\\mathcal{S}\\) (\\(0\\le x\\le 1,\\; 0\\le y \\le 1\\)). Suppose that the probability that the point is selected from a specific region is equal to the area of the region. Find the probabiliy the point selected is from each of the following regions: \\((x,y)\\) such that \\((x-1/2)^2 + (y-1/2)^2 \\ge 1/4\\) \\((x,y)\\) such that \\(1/2 \\le x+y \\le 3/2\\) \\((x,y)\\) such that \\(y \\le 1-x^2\\) \\((x,y)\\) such that \\(x=y\\) Bonferroni’s Inequality. Let \\(A_1, A_2, \\dots\\) be an arbitrary infinite squence of events. Define the seqence of events \\(B_1, B_2, \\dots\\) as \\[B_1=A_1\\] \\[B_2 = A_1^c \\cap A_2\\] \\[B_3 = A_1^c \\cap A_2^c \\cap A_3\\] \\[B_4 = A_1^c \\cap A_2^c \\cap A_3^c \\cap A_4\\] Prove that \\(B_i \\subset A_i\\), \\(B_i \\cap B_j = \\emptyset\\) for \\(i\\ne j\\), and that \\(\\bigcup_{i=1}^n A_i \\;=\\; \\bigcup_{i=1}^n B_i\\). Prove that \\[Pr\\left( \\bigcup_{i=1}^n A_i \\right) = \\sum_{i=1}^n Pr(B_i)\\] Prove that \\[Pr\\left( \\bigcup_{i=1}^n A_i \\right) \\le \\sum_{i=1}^n Pr(A_i)\\] Using the previous result (b), prove that for sets \\(D_1,D_2,\\dots,D_n\\) that \\[Pr\\left( \\bigcap_{i=1}^n D_i \\right) \\ge 1 - \\sum_{i=1}^n Pr(D_i^c)\\] 1.6 Finite Sample Spaces (D&amp;S 1.6) When dealing with sample spaces with only a finite number of outcomes (say \\(n\\) outputcomes \\(s_i\\)), it is often convenient to define each outcome as an event. Let \\(s_i\\) be outcomes in the sample space \\(\\mathcal{S}\\). Let each of these outcomes have probability \\(p_i\\). For the axioms of probability to hold then: \\[p_i \\ge 0\\] \\[\\sum_{i=1}^n p_i = 1\\] When fair dice, we assume that each side has equal probability of being rolled. For rolling a 6-sided die, what is the probability of rolling an even number? When rolling two (or more) differently colored dice, we assume that the die do not affect the outcome of the other and that every pair of is equally likely. Alternatively you can think of rolling 1 die and then the other. So for rolling two six sided dice, there are 36 possible rolls, and notice, for example, \\((2,3)\\) is a different roll that \\((3,2)\\). What is the probability that the sum of the two rolls is even? If a fair coin is flipped three times… What are the possible outcomes (enumerate these)? Explain why it is reasonable that each outcome is equally probable? What is the probability that all three faces will be the same? 1.7 Ordered Counting (D&amp;S 1.7) Often we situations where it is reasonable to beleve that each outcome of an event is equally likely and therefore we can figure out the probability if we knew how many events there were. E.g. there are 36 different outcomes for rolling two fair 6-sided dice, so each outcome has a 1/36 probability. Prove/argue/justify that if the outcome of experiment is composed of 2 parts, where the first part has \\(m\\) outcomes \\(x_1, \\dots, x_m\\) and the second part has \\(n\\) outcomes, \\(y_1,\\dots,y_n\\) then there is a total of \\(mn\\) outcomes \\((x_i,y_j)\\). This is often called the Multiplication Rule for Counting. I own 3 pair of pants that are “work appropriate.” I also own 6 different shirts and 5 pairs of shoes that are “work appropriate.” How many different outfits are possible? How many way can the numbers \\(1,2,3,4,\\) and \\(5\\) be arranged? Ordered Sampling without Replacement For distinct objects \\(1,2,\\dots,n\\) prove that there are are \\(P_{n,k}=\\frac{n!}{(n-k)!}\\) arrangements of \\(k\\) elements (where the order is important, i.e. \\(1,2,3\\) is distinct from \\(2,1,3\\)) and \\(n! = n\\cdot(n-1)\\cdot(n-2)\\dots\\cdot(2)\\cdot(1)\\) and by definition \\(0!=1\\). We call \\(P_{n,k}\\) the number of permutations of \\(k\\) elements taken from a set of \\(n\\) distinct objects. Hint: First consider base cases of \\(k=1\\) and then \\(k=2\\) and that the formula is appropriate. Then, to complete the induction arguement, show that if we have a \\(P_{n,k}\\) permutations of \\(k\\) objects, then increasing to \\(k+1\\) elements simply results in \\((n-k) \\cdot P_{n,k}\\) arrangements due to the Multiplation Rule of counting. From \\(n=17\\) students, one student will get a candy bar, another will get a soda, and a third will receive some gummi bears. How many different ways could the treats be distributed to the students? Suppose we are going to randomly select \\(3\\) elements from the digits \\(0,1,2,\\dots,9\\) but we will select these with replacement (so we could get the \\(022\\)). How many outcomes are there? Ordered Sampling with Replacement Suppose that we have \\(n\\) distinct objects labeled \\(1,2,\\dots,n\\) and we are going to sample \\(k\\) of these objects with replacement. Justify/derive a formula for the number of outcomes. Consider the sequence of numbers \\(0000, 0001, 0002, \\dots, 9998, 9999\\). How many of these numbers are composed of 4 different digits? Often times I am interested in calculating the probability of a particular event and we can often do it in the following manner: First count the number of equally likely outcomes there are. Count the number of outcomes where the event of interest occures. Then calculate \\[Pr\\left( \\textrm{Event} \\right ) = \\frac{\\textrm{Number of outcomes where event happens}}{\\textrm{Total number of equally likely outcomes}}\\] As I work in the evenings, I often listen to music. Suppose that I have a playlist of \\(n=300\\) songs and I listen to them on shuffle where the software always selects from the list with equal probability when selecting which song to play next. If I listen to \\(k=10\\) songs, what is the probability that at least one of the songs will be duplicated? What about if I listen to \\(k=30\\) songs? If 14 balls are randomly thrown into 25 boxes such that there is equal chance for a ball to land in any box, what is the probability that no box recieves more than one ball? 1.8 Combinations (D&amp;S 1.8) Often we want to count the number of arrangements of \\(k\\) elements selected without replacement from \\(n\\) distinct objects but where the order doesn’t matter. Another way of saying this is that we want to count the number of sets of size \\(k\\) taken from \\(n\\) distinct objects. Unordered Sampling without ReplacementFor a set of \\(k\\) elements, prove that there are \\(k!\\) permutations of those elements. Using this information, argue that the number of distinct sets of \\(k\\) objects taken from \\(n\\) elements is (which the book denotes as \\(C_{n,k}\\) and many others denote \\(\\binom{n}{k}\\)) is \\[C_{n,k} = \\binom{n}{k} = \\frac{P_{n,k}}{k!} = \\frac{n!}{k!(n-k)!}\\] I have 3 identical cans of soda that I will distribute randomly to 17 students. I will select (with equal probabilities per student) 3 students. How many ways could I choose 3 students? Suppose I have a character string composed of only 0s and 1s. The character string is \\(20\\) characters long and \\(8\\) of them are 0s. How many different strings are there? Unordered Sampling with Replacement Suppose that I have \\(n=7\\) boxes into which I will randomly throw \\(k=3\\) balls. n=7 boxes +----+-----+-----+-----+-----+-----+-----+ | 1 | 2 | 3 | 4 | 5 | 6 | 7 | +----+-----+-----+-----+-----+-----+-----+ Now suppose that we throw, at random, \\(k=3\\) balls into the boxes. We might end up with one ball in box 3 and two in box 6. n=7 boxes +----+-----+-----+-----+-----+-----+----+ | | | 0 | | | 00 | | +----+-----+-----+-----+-----+-----+----+ Argue that throwing \\(k\\) balls randomly into \\(n\\) boxes is equivalent to selecting a set of \\(k\\) elements from \\(n\\) distinct objects with replacement. Hint show that every set chosen with resampling can be represented via boxes/balls and that every boxes/balls combination represents a possible set of \\(k\\) elements from \\(n\\) distinct objects with replacement. The guts of the boxes/balls diagram is the arrangement of box partitions and balls because the outer box walls don’t matter because the balls get into a box. | | | 0 | | | 00 | | which we can clean up a bit by remembering that the other walls have to be there and we’ll represent the balls with 0 and the box partitions with a 1. |110111001|. This reduces the problem into how many binary strings can I produce with \\(n-1\\) 1s and \\(k\\) 0s. How many are there? Suppose that I will distribute my three soda cans to 17 students by drawing names out of a hat, but replacing the student’s name after it is draw. How many different outcomes could occur? Suppose I draw 2 cards from a standard deck of 52 cards, what is the probability that I draw two cards of the same suit? Ten teams are playing in a tournament. In the first round there, there will be five games played. How many possible arrangements are there? What is the probability that the Ashville Avalanch plays the Boston Behemoths (these are two of the ten teams playing)? Suppose that I flip a fair coin 10 times. What is the probability I observe 3 heads? 1.9 Multinomial Coefficients (D&amp;S 1.9) From the Math/Stat department faculty, a committee of 5 members is to be selected. There are 8 Math, 4 Statistics, and 4 Math Ed Professors. What is the probability that committee is composed of 3 Math, 1 Stats, and 1 Math Ed professor. Suppose that we are creating a string of beads from 9 red, 7 blue, and 10 yellow beads. How many different arrangements can be made? "],
["2-conditional-probability.html", "Chapter 2 Conditional Probability 2.1 Defining Conditional Probability (D&amp;S 2.1) 2.2 Independence 2.3 Bayes’ Theorem", " Chapter 2 Conditional Probability 2.1 Defining Conditional Probability (D&amp;S 2.1) Out of \\(n=17\\) students in a class, I will chose one at random student to give gummi bears to. In this class there are \\(12\\) men and \\(5\\) women. Student \\(A\\) is very interested in her probability of being selected. Denote the event \\(A\\) as being that the student gets the gummi bears. Denote event \\(W\\) as a woman is selected. What is the probability that student \\(A\\) is selected? What is the probability that a woman is selected? Suppose I restricted my selection to only the women? What is the probability that student \\(A\\) is selected. Can you write this probability as a function of your answers in parts (a) and (b)? Consider the case where we take the sum of 2 six-sided dice. Define \\(A\\) as the event that the sum is greater than or equal to 9, and \\(B\\) being the event that the sum is greater than or equal to 6. Create a table of the possible outcomes. Notice that each outcome is equally likely. What is the probability that \\(A\\) occurs? Use correct notation and leave it as a fraction \\(\\frac{???}{36}\\) Suppose that you are told that event \\(B\\) has occured. How many equally likely outcomes are there and what is the probability that \\(A\\) occurs? This will be denoted as \\(Pr(A|B)\\). (Leave this as a fraction). Notice that you the numerator in answer in part (a) is the same as the numerator as you had in part (b), but the denominators are different. What number do you need to multiple your part (a) answer by to get your part (b) answer. How does this relate to \\(P(B)\\)? It seems that my children (Casey and Elise) get sick with annoying frequency. Suppose the probability that my son Casey gets sick is \\(Pr(C) = 0.05\\) and furthermore the probability that both children get sick is \\(Pr(E \\cap C) = 0.03\\) If Casey is sick right now, what is the probability that Elise is also sick? When we talk about events \\(A\\) and \\(B\\), we defined them as possible outcomes, but it isn’t until we define probability of the events \\(Pr(A)\\) \\(Pr(B)\\) that we care about the sample space \\(\\mathcal{S}\\) of all possible events. What we are now trying to do is to claim that some addition knowledge allows us to refine the sample space to some smaller subset of \\(\\mathcal{S}\\), perhaps \\(B\\subset \\mathcal{S}\\). So for events in \\(B\\), we now need to re-scale all the probabilities to reflect that we now know that event \\(B\\) did happen. If we previously know that \\(Pr(A\\cap B)=0.3\\) and \\(Pr(B)=0.6\\), then half of the probability associated with \\(B\\) is overlapping with \\(A\\). So if we just restrict ourselves to cases where \\(B\\) occurs, then the probability that \\(A\\) will occur is \\(1/2\\). Notice that our notation \\(Pr( A | B)\\) is addressing the refinement of the sample space, but we’ve just defined our notation this way. We have not, and will not ever define \\(A|B\\) because event \\(A\\) is event \\(A\\), regardless of the sample space we use to figure out its probability. We formally define the conditional probability of \\(A\\) given \\(B\\) as \\[Pr(A|B) = \\frac{ Pr(A \\cap B)}{Pr(B)} \\;\\;\\; \\textrm{Assuming } Pr(B) \\ne 0\\] If \\(Pr(B)=0\\) then the conditional probablity is undefined. Notice that this can be happily re-arranged to \\[ Pr( A \\cap B) = Pr(A | B) Pr(B) \\] I can also notice that I could condition on either event \\(A\\) or event \\(B\\), so we could also have \\[Pr( A \\cap B) = Pr(B|A)Pr(A)\\] If \\(B \\subset A\\) and \\(Pr(B)&gt;0\\), what is \\(Pr(A|B)\\) If \\(A\\) and \\(B\\) are disjoint and \\(Pr(B)&gt;0\\), what is \\(Pr(A|B)\\)? Suppose that events \\(A_1,A_2,\\dots,A_n\\) are events such that \\(Pr\\left( \\bigcap_{i=1}^n A_i \\right) &gt; 0\\). Show that \\[Pr\\left( \\bigcap_{i=1}^n A_i \\right) = Pr\\left( A_n \\Big\\rvert \\bigcap_{i=1}^{n-1}A_i \\right) Pr\\left( A_{n-1} \\Big\\rvert \\bigcap_{i=1}^{n-2}A_i \\right)\\dots Pr\\left( A_3 \\Big\\rvert A_2 \\cap A_1 \\right) Pr\\left( A_2 \\Big\\rvert A_1 \\right) Pr\\left( A_1 \\right)\\] For events \\(A\\), \\(B\\), and \\(D\\) such that \\(Pr(D)&gt;0\\) show that: \\(Pr(A^C | D) = 1 - Pr(A | D)\\). \\(Pr( A \\cup B \\Big\\rvert D) = Pr(A | D) + Pr(B|D) - Pr( A \\cap B | D)\\) Suppose that we have \\(K\\) events \\(B_k\\) such that the \\(B_1, B_2, \\dots, B_K\\) are disjoint and \\(\\bigcup B_k=\\mathcal{S}\\). Then we call the events \\(B_1,\\dots,B_K\\) a partition of the sample space \\(\\mathcal{S}\\). A partition of \\(\\mathcal{S}\\) is often useful for caculating probabilities due to the disjoint nature of the \\(B_k\\) elements. Law of Total Probability Prove that for for a partition \\(B_1, \\dots, B_K\\) of \\(\\mathcal{S}\\), that \\[Pr(A) = \\sum_{k=1}^K Pr(A \\cap B_k) = \\sum_{k=1}^K Pr(A \\Big\\rvert B_k) Pr( B_k )\\] Note: There is a conditional version of the Law of Total probability, which is proved in an analogous fashion: \\[Pr(A|C) = \\sum_{k=1}^K Pr(A \\cap B_k \\big\\rvert C) = \\sum_{k=1}^K Pr(A \\big\\rvert B_k \\cap C) Pr( B_k \\big\\rvert C)\\] A child’s bookshelf contains three shelves. On the shelves are \\(n_1=10\\),\\(n_2=20\\) and \\(n_3=30\\) books. Within each set of books, there are some number of Dr Suess books \\(m_1=5\\), \\(m_2=4\\), \\(m_3=2\\). The child will select a shelf at random (equal probability) and then from the shelf will select a book at random. What is the probability the child selects a Dr Suess book? A camera with a motion detector was mounted facing a forest trail. \\(50\\%\\) of the pictures were taken during the daytime, \\(15\\%\\) were taken during twilight hours (dawn and dusk), and \\(35\\%\\) were taken during the night. Of the pictures taken during the daytime, \\(80\\%\\) were of hikers and \\(20\\%\\) were of wild animals. Of the pictures taken at twilight \\(30\\%\\) were of hikers and \\(70\\%\\) were of wild animals. Finally, of the pictures taken during the night, \\(100\\%\\) were wild animals. What is the probability that a randomly selected photo is of a hiker and was taken at twilight? What is the probability a photo was taken at night given that is of a wild animal? I have kept track of the probabilities of how many cats will sit with me and/or my wife on the couch. Below is a table of probabilities. 0 Cats 1 Cat 2 Cats 3 Cats 0 People 0.08 0.08 0.03 0.01 1 Person 0.1 0.25 0.125 0.025 2 People 0.03 0.09 0.12 0.06 What is the probability that one human is sitting on the couch? What is the probability that at least two cats are sitting on the couch? Given that there are two cats sitting on the couch, what is the probability that there are two humans also on the couch? My cat Kaylee occasionally likes to sit on people’s laps while they are seated at the table. My wife is strongly opposed to this and will scold the cat when she catches her in the act. Suppose that that Kaylee will select my lap \\(60\\%\\) of the time and the remaining \\(40\\%\\) of the time she jumps into my wife’s lap. If Kaylee jumps into my wife’s lap, there is a \\(100\\%\\) chance of being scolded, while if she jumps into mine, there is only a \\(20\\%\\) chance of being scolded. Given that Kaylee was just scolded for being in a lap, what is the probability she was in my wife’s lap? There are two brands of Mac &amp; Cheese that my daughter will eat. When I go shopping I will pick from the two brands with a \\(70\\%\\) probability of choosing the brand that I bought the previous time. The first time I went shopping, I chose from the two brands with equal probability. What is the probability that I chose brand \\(A\\) on the first and second trips, and brand \\(B\\) on the third and fourth trips? 2.2 Independence Definition: Two events, \\(A\\) and \\(B\\) are independent if \\(Pr( A \\cap B) = Pr(A)Pr(B)\\). Definition: Events \\(A_1, A_2,\\dots,A_K\\) are pairwise independent if \\(A_i\\) and \\(A_j\\) are independent for any \\(i,j\\). Definition: Events \\(A_1, A_2,\\dots,A_K\\) are mutually independent if for all subsets \\(I\\) of \\(1,2,\\dots,K\\), \\(Pr( \\bigcap_{i\\in I} A_i) = \\prod_{i \\in I} Pr(A_i)\\) Show that if \\(Pr(A)&gt;0\\) and \\(Pr(B)&gt;0\\), then \\(A\\) and \\(B\\) are independent if and only if \\(Pr(A|B)=Pr(A)\\) and \\(Pr(B|A)=Pr(B)\\) Give an example of three events that are pairwise independent but not mutually independent. Convention If I say that a set of events are “independent”“, then we intend to say”mutually independent“” but are being lazy. Show that if \\(A\\) and \\(B\\) are indendent, then \\(A\\) and \\(B^c\\) are also independent. Suppose that we flip a fair coin three times. Denote \\(H_i\\) as the event that I flip a head on the \\(i\\)th flip. Find \\(Pr( H_1 \\cap H_2 \\cap H_3 )\\) Find \\(Pr( H_1 \\cap H_2^c \\cap H_3)\\) Find \\(Pr( H_1^c \\cap H_2 \\cap H_3)\\) How many ways can we have 2 heads? What is the probability of 2 heads? I will roll a 20-sided die three times. Define the event \\(H_i\\) as the event that I roll a 17 or greater on the \\(i\\)th roll. Find \\(Pr( H_1 \\cap H_2 \\cap H_3 )\\) Find \\(Pr( H_1 \\cap H_2^c \\cap H_3)\\) Find \\(Pr( H_1^c \\cap H_2 \\cap H_3)\\) How many ways can we have exactly 2 \\(H\\) events happen? What is the probability of exactly 2 \\(H\\) events happening? I will roll my 20-sided die until I roll a 20. What is the probability that I roll a 20 on my first roll? What is the probability that the first 20 I roll is on the \\(5\\)th roll? A family has two children. It is known that at least one is a boy. What is the probability that the family has two boys, given that at least is one a boy? Assume that genders are equally likely and that genders of siblings are independent. 2.3 Bayes’ Theorem The goal of Bayes’ Theorem is to reverse the order of conditioning. Suppose we are interested in two events \\(A\\) and \\(B\\). We might be given some information about \\(P(A|B)\\) but we want to know about \\(P(B|A)\\). Bayes’ Theorem Prove that for two events \\(A\\) and \\(B\\) such that \\(Pr(A)&gt;0\\) then \\[\\begin{aligned} Pr(B|A) &amp;= \\frac{Pr(A|B)Pr(B)}{Pr(A|B)Pr(B)+Pr(A| B^c)Pr(B^c)} \\end{aligned}\\] Bayes’ Theorem (general case) Again consider an event \\(A\\) such that \\(Pr(A)&gt;0\\). For an arbitrary partition of \\(\\mathcal{S}\\), say \\(B_k\\) \\(k=1,\\dots,K\\), prove that \\[\\begin{aligned} Pr(B_k|A) &amp;= \\frac{Pr(A|B_k)Pr(B_k)}{\\sum_{i=1}^K Pr(A| B_k)Pr(B_k)} \\end{aligned}\\] A softball team has two pitchers, Jeff and Bob. Of the two Jeff is the better pitcher and wins 80% of the games he pitches for but can only play in 30% of the games. Bob pitches in the rest of the games, but only wins 40% of his games. What is the probability that the softball team wins a game? Given that the team won, what is the probability that Jeff pitched? One card is selected at random from a standard deck of 52 playing cards. It is inserted into a second standard deck and the second deck is then well shuffled. A card is drawn at random from the second deck. What is the probability it is an ace? Given that an ace was drawn from the second deck, what is the probability that an ace was transfered from the first deck? My three cats love licking up the milk out of my cereal bowl if I leave it unattended. If unattended, there is a 30% chance that Beau will clean the bowl, a 50% chance that Tess will, and 20% chance that Kaylee will. Unfortunately the milk makes the cats nauseous and if a cat gets milk there is a good chance the cat will puke. In particular the probability that Beau will puke given he has had milk is 30%, for Tess it is 60%, and for Kaylee it is 40%. My daughter recently left a cereal bowl out and a cat finished the milk. What is the probability that a cat has puked as a result. Given that a cat has puked in response, what is the probability it was Kaylee? I have two decks of cards. The first deck has 40 red cards and 10 black. The second deck has 25 red and 25 black. I select a deck at random, and then draw two cards. Given that I’ve selected two red cards, what is the probability that I initially chose the first deck? An inexpensive and convenient enzyme immunoassay screening tests for HIV in a human. If the person is actually HIV negative then the test returns negative with a probability of \\(0.985\\). If the person is HIV positive, the test returns a positive result with probability \\(0.9997\\). HIV is a major epidemic in Sub-Saharan Africa with approximately 5% of the adult population having HIV. Major aid organizations want to help identify people with HIV for treatment and will use this cheap and convenient test in their efforts. Suppose that an adult in Sub-Sahran Africa is selected and tested and the test result is that the person has HIV. What is the probability that the person actual has HIV? "],
["3-random-variables-and-distributions.html", "Chapter 3 Random Variables and Distributions 3.1 Defining Random Variables and Discrete Distributions 3.2 Continuous Distributions 3.3 Cumulative Distribution Function 3.4 Bivariate Distributions", " Chapter 3 Random Variables and Distributions 3.1 Defining Random Variables and Discrete Distributions A random variable is a function that takes outcomes in the sample space \\(\\mathcal{S}\\) and maps them to numeric values in \\(\\mathbb{R}\\). Often times we abbreviate random variable as RV. The idea is that random events such as flipping Heads, a medical test showing the patient has a disease, Chris Froome winning the Tour de France, or rolling a Leaning Jowler in Pass the Pigs are all random events but to do math on them, we need to turn them into numbers. In cases where the sample space \\(\\mathcal{S}\\) is already numeric, the random variable can just be the identity, but in other cases, we might have to be more careful. For example, if my experiment is flipping a fair coin \\(n=4\\) times, I could define the random variable \\(X=\\) number of heads and \\(X\\) could take on any of the values \\(x \\in \\{0,1,2,3,4\\}\\). I could similarly define \\[Y= \\begin{cases} 0 \\;\\;\\; \\textrm{ if number of heads } &lt; 2 \\\\ 1 \\;\\;\\; \\textrm{ if number of heads } &gt; 2 \\end{cases}\\] A RV function doesn’t have to be one-to-one and it doesn’t have to map to the entire set of real values. Because events in the sample space \\(\\mathcal{S}\\) have an associated probability, then it is natural to define an event, say \\(B\\) to be all the outcomes \\(s \\in \\mathcal{S}\\) such that \\(X(s) = x\\) and then define \\(Pr(X=x) = Pr(B)\\). Notation: We will refer to the random variable using the capital letters, (e.g. \\(X\\), \\(Y\\), \\(Z\\), \\(W\\)) and the possible values they take on using lower case letters. With this notation, the RV \\(X\\) could take on values \\(x \\in \\{0,1,2,3,4\\}\\) We consider flipping a fair coin \\(n=4\\) times. What is the set of outcomes? As usual, we will define an event for each outcome. What is the probability of each outcome? For the RV \\(X\\) defined above, what outcomes define the event \\(B\\) such that \\(s \\in B \\implies X(s)=2\\)? What is \\(Pr(B)\\)? Therefore what is \\(Pr(X=2)\\)? For the RV \\(Y\\) defined above, what outcomes define the event \\(A\\) such that \\(s \\in A \\implies Y(s)=1\\)? What is \\(Pr(A)\\)? Therefore what is \\(Pr(Y=1)\\)? For each value that \\(X\\) or \\(Y\\) could take on, we could figure out the probability of the associated event. We define the random variables distribution as a description of what values the RV can take on and \\(Pr(X \\in C)\\), for any interval \\(C = \\{c: a\\le c \\le b\\}\\) for any \\(a&lt;b\\). This is actually a very awkward definition and we will examine more convenient ways to specify these same probabilities. Discrete random variables are RVs that can only take on a finite or countably infinite set of values. Continuous random variables are RVs that can take on an unaccountably infinite set of values. We now define the probability function of a discrete RV \\(X\\) as \\[f(x) = Pr(X = x)\\] and the closure of the set \\(\\{x: \\textrm{ such that } f(x) &gt; 0\\}\\) is referred to as the support of \\(X\\). Notice that this function is defined for all \\(x\\in \\mathbb{R}\\), but for only a countable number of cases is \\(f(x)&gt;0\\). Suppose that RV \\(X\\) can take on the values \\(\\{x_1,x_2,\\dots,x_K\\}\\). Prove that \\[\\sum_{k=1}^K f(x_k) = 1\\] Bernoulli Distribution. Suppose that the random variable \\(W\\) takes on the values \\(0\\) and \\(1\\) with the probabilities \\(Pr(W=1) = p\\) and \\(Pr(W=0) = 1-p\\). Then we say that \\(W\\) has a Bernoulli distribution with probability of success \\(p\\), which I might write as \\(W \\sim Bernoulli(p)\\). Show that for any interval \\(C =\\{c: a\\le c \\le b\\}\\) in \\(\\mathbb{R}\\), you can find \\(Pr(W \\in C)\\). Uniform Distribution on Integers. Suppose that we have integers \\(a\\) and \\(b\\) such that \\(a &lt; b\\). Suppose that the RV \\(X\\) is equally likely to be any of the consecutive integers \\(a,\\dots,b\\). What is \\(f(x)\\)? (Make sure your definition applies to any \\(x\\in\\mathbb{R}\\)) Binomial Distribution Suppose that we have an experiment that consists of \\(n\\) independent Bernoulli(\\(p\\)) trials. We are interested in the distribution of \\(X=\\) # of successful trials. That is \\(X\\sim Binomial(n,p)\\). For any integer \\(x \\in \\{0,1,\\dots,n\\}\\), what is \\(Pr(X=x)\\)? Define \\(f(x)\\) for all values of \\(x \\in \\mathbb{R}\\). Give two examples of random variables which have Bernoulli(\\(p=1/2\\)) distributions. These two RVs should not the same RVs, but they have the same distribution. That is to say, RVs have distributions, but distributions are not RVs. Suppose that two fair six-sided dice are rolled and the RV of interest is the absolute value of the difference between the dice. Give the probability distribution along with an illuminating graph. Suppose that a box contains 7 red balls and 3 green balls. If five balls are selected at random, without replacement, determine the probability function of \\(X\\) where \\(X\\) is the number of red balls selected. Suppose that a random variable \\(X\\) has a discrete distribution with the following probability function: \\[f(x) = \\begin{cases} \\frac{c}{2^x} \\;\\textrm{ for } x = 0, 1, 2, \\dots \\\\ 0 \\;\\;\\;\\; \\textrm{otherwise} \\end{cases}\\] Find the value for \\(c\\) that forces the requirement that \\[\\sum_{x=0}^\\infty f(x) = 1\\] Hint: This is a particular power series. Go to any Calculus book (or internet) for an appropriate result. Make sure you introduce the result and show why the result applies to \\(f(x)\\) in your solution. For the binomial distribution (and many distributions we will consider this semester), it would be nice to not have to calculate various probabilities by hand. Most mathematical software packages include some way to calculate these probabilities. System Documentation Link or site link Matlab https://www.mathworks.com/help/stats/working-with-probability-distributions.html Mathematica http://reference.wolfram.com/language/howto/WorkWithStatisticalDistributions.html R https://dereksonderegger.github.io/570L/3-statistical-tables.html Web App https://ismay.shinyapps.io/ProbApp/ Each time I ride home or to work, there is a \\(p=0.1\\) probability that I will get stopped by a train. Let \\(X\\) be the number of times I’m stopped by the train in the next 10 trips. Assume that the probability I’m stopped on trip \\(i\\) is independent of all other trips \\(j\\). What is the distribution of \\(X\\)? Remember, to specify the distribution by name, you must specify the name and the value of all parameters. What is \\(Pr(X =6)\\)? What is \\(Pr(X &lt; 6)\\)? What is \\(Pr(X \\ge 6)\\)? 3.2 Continuous Distributions We wish to define something similar to the probability function for continuous RVs. However, because there are an uncountable number of values that the RV could take on, we have to be careful and define probability on intervals of the form \\([a,b]\\). We define the probability density function (usually denoted pdf) as the function \\(f(x)\\) such that \\[Pr( a \\le X \\le b) = \\int_a^b f(x) dx\\] We further define the support of the distribution as the closure of the set \\({x: f(x)&gt;0}\\). This is the second time we’ve defined the support as the closure of the set. In the discrete case, it didn’t really matter, but here it does. If we define the pdf on the set \\([0,1]\\) versus \\((0,1)\\), we want the support to contain the end points of the interval, that is the support is the closure of \\((0,1)\\) which is \\([0,1]\\). Show that, for any \\(a\\in\\mathbb{R}\\), \\(Pr( X = a ) = 0\\) is consistent with this definition of \\(f(x)\\). Hint: What happens as \\(b-a\\) gets small? Take the limit. Prove that \\(\\int_{-\\infty}^\\infty f(x) dx = 1\\). Further notice that \\(f(x)\\ge0\\) for all \\(x\\) because otherwise that would imply that there are events with negative probabilities. Continuous Uniform. Suppose that the random variable \\(X\\) will be a randomly chosen real value from the interval \\([a,b]\\). Suppose that for any sub interval \\(d=[d_1,d_2]\\) where \\(a \\le d_1 \\le d_2 \\le b\\), that \\(Pr( X \\in d)\\) is proportional to the length of the interval \\(d\\). What is the pdf of the distribution of \\(X\\)? It is unfortunate that your book doesn’t introduce indicator functions at this point in time. We can define the following: \\[I( \\textrm{logical test} ) = \\begin{cases} 1 \\;\\;\\; \\textrm{ if logical test is true } \\\\ 0 \\;\\;\\; \\textrm{ if logical test is false } \\end{cases}\\] Suppose that the pdf of a random variable \\(X\\) is \\[f(x) = \\frac{4}{3}\\left(1-x^3\\right) \\;I(0&lt;x&lt;1)\\] Create a graph of the pdf. Find \\(Pr\\left( X\\le \\frac{1}{2} \\right)\\)? Find \\(Pr\\left( \\frac{1}{4} &lt; X &lt; \\frac{3}{4} \\right)\\) Find \\(Pr\\left( X &gt; \\frac{3}{4} \\right)\\) Suppose the random variable \\(X\\) has pdf \\[f(x) = c\\cdot e^{-5x}I(x\\ge 0) = \\begin{cases} c\\cdot e^{-5 x} \\;\\;\\; \\textrm{ if } x \\ge 0 \\\\ 0 \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; \\textrm{otherwise} \\end{cases}\\] What is the value of \\(c\\) that makes this a valid pdf? That is, what value of \\(c\\) makes this function integrate to 1? Graph this function. What is \\(f(0)\\)? Many students dislike that \\(f(0)&gt;1\\) is greater than one. Why isn’t this a problem? What is the probability \\(Pr(X \\le 0.5)\\)? Suppose that the pdf of the random variable \\(X\\) is \\[f(x)= c \\cdot x \\;\\;I(0\\le x \\le 3)\\] What value of \\(c\\) makes this a valid pdf? Find a value of \\(t\\) such that \\(Pr( X \\le t ) = 0.25)\\) Find a value of \\(t\\) such that \\(Pr( X &gt; t) = 0.5)\\) Given the same random variable \\(X\\) described in problem 3.2.5, consider the random variable \\(Y\\) which is the simply the nearest integer to \\(X\\). What is the pf of \\(Y\\)? We now have defined both the probability function (pf) for discrete random variables and probability density function (pdf) for continuous random variables. We now try to make a physics analogy to describe the difference between the two. In both cases we want to think about probability as physical mass. Discrete Case In this case, all the probability mass is concentrated at precise points. So we have little nuggets of probability mass, at discrete points. If I want to know, say \\(Pr(X \\le 3)\\) then we have to sum the probability across all the discrete locations such that \\(X \\le 3\\). Continuous Case In this case, the probability mass is spread out across \\(\\mathbb{R}\\) and the concentration of mass is not uniform, some spots have more concentrated mass. In this case, we don’t have a definite amount of mass at a particular point, but we do have a description of how dense the mass is at any point. In this case if we want to know \\(Pr(X \\le 3)\\) then we have to break up \\(\\mathbb{R}\\) into a bunch of intervals and then combine the length of the interval along with information about the average density of each interval. Each bar has some probability mass (mass is the length of the interval times the average density along the interval) and then we just sum up the bars. If we take the limit as we make the bars narrower, then we end up with \\[Pr(X \\le 3) = \\int_{-\\infty}^3 f(x) \\, dx\\] You might be a little freaked out because typically you would think about mass being the area or volume times the density, but we need to start in the 1-dimension case before we address the 2-dimension and 3-dimension cases. 3.3 Cumulative Distribution Function It is somewhat annoying to mathematically describe distributions in two different ways, depending on if the random variable is discrete or continuous. \\[\\begin{aligned} Pr \\left( X=x \\right) = f(x) \\;\\;\\; &amp; \\textrm{ if X is discrete RV } \\\\ Pr \\left(a \\le X \\le b\\right) = \\int_a^b f(x)\\,dx \\;\\;\\; &amp; \\textrm{ if X is continuous RV } \\end{aligned}\\] While the probability function and probability density function are both useful functions, it is mathematically convenient to have a mathematical description of the distribution that has the same interpretation regardless of if the distribution is discrete or continuous. Definition: For a random variable \\(X\\) (notice we don’t specify if it is continuous or discrete) the Cumulative Distribution Function (CDF) is defined as \\[F(x) = Pr( X \\le x )\\] Notice that this is defined for all \\(x\\) in \\(\\mathbb{R}\\). Suppose that random variable \\(X\\) has a Uniform distribution on the integers \\(1,2,3,4,5\\). These are the only values that \\(X\\) can take on, and \\[f(x) = \\frac{1}{5} \\, \\cdot I\\Big(x \\in \\{1,2,\\dots,5\\}\\Big)\\] Draw a graph of \\(F(x)\\) and make sure the graph demonstrates: That \\(F(x)\\) is defined for all \\(x \\in \\mathbb{R}\\). That \\(F(x)\\) is a step function. That \\(F(x)\\) is continuous from the right. That is, for \\(\\epsilon &gt; 0\\), \\(\\lim_{\\epsilon \\to 0} F(x+\\epsilon) = F(x)\\). That \\(\\lim_{x\\to -\\infty} F(x) = 0\\). That \\(\\lim_{x\\to \\infty} F(x) = 1\\). Define \\(B_x = \\left\\{s \\textrm{ such that } X(s) \\le x\\right\\}\\). Then we have for \\(x_1 &lt; x_2\\) that \\(B_{x_1} \\subset B_{x_2}\\) and therefore: \\[\\lim_{x\\to -\\infty} F(x) = \\lim_{x\\to -\\infty} Pr\\left( B_{x} \\right) = Pr( \\emptyset ) = 0\\] \\[\\lim_{x\\to \\infty} F(x) = \\lim_{x\\to \\infty} Pr\\left( B_{x} \\right) = Pr( \\mathcal{S} ) = 1\\] Show that \\(F(x)\\) must be non-decreasing. Notice this allows for \\(F(x)\\) to be a flat function, but cannot decrease. Suppose that the r.v. \\(X\\) has a Binomial(\\(n=5\\), \\(p=0.8\\)) distribution. Sketch the CDF. Geometric Distribution Suppose that we flip a biased coin that has probability of heads as \\(p \\in [0,1]\\). Let the r.v. \\(X\\) be the number of coin flips until the first head is observed. What values could \\(X\\) take? Mathematically, we say, what is the support of \\(X\\)? Is \\(X\\) a continuous or discrete random variable? Find the probability function, \\(f(x)\\). Show that cummulative distribution function is \\(F(x) = 1 - (1-p)^x\\). Hint: Geometric Series! For continuous random variables it is relatively easy to go back and forth from the cdf to the pdf (assuming the integration and differentiation isn’t too hard). \\[ F(x) = \\int_\\infty ^x f(u) \\, du\\] \\[ f(x) = \\frac{d}{dx}\\, F(x)\\] Exponential Distribution (Warning! There are two ways to parameterize the Exponential Distribution. Before you look anything up, make sure it is using the same parameterization you are.) Suppose that we have a continuous random variable \\(X\\) with pdf \\[f(x) = \\beta e ^{-\\beta x} \\;\\cdot I(x &gt; 0)\\] Find the cdf function \\(F(x)\\). For \\(\\beta=2\\) sketch the pdf and cdf. On the pdf and cdf graphs, represent \\(Pr(X &lt; 1)\\). In the pdf it will be some shaded area, in the cdf it is something else. Suppose that the cdf of a random variable \\(X\\) is as follows: \\[ F(x) = \\begin{cases} 0 \\;\\;\\; &amp; \\textrm{ for } x \\le 0 \\\\ \\frac{1}{9} x^2 &amp; \\textrm{ for } 0 \\le x \\le 3 \\\\ 1 &amp; \\textrm{ for } x &gt; 3 \\end{cases}\\] Find the pdf function \\(f(x)\\). Sketch the pdf and cdf. On the pdf and cdf graphs, represent \\(Pr( X \\le 2 )\\). Suppose that a point in the \\(xy\\)-plane is chosen at random from the interior of the unit circle, which is the circle centered at \\((0,0)\\) with radius 1. Notice the probability that the chosen point belongs to a given region is proportional to the area of the region. Let the random variable \\(Z\\) represent the distance of the point to the origin. Find and sketch the cdf of \\(Z\\). Find and sketch the pdf of \\(Z\\). On the pdf and cdf graphs, represent \\(Pr( Z \\le 0.5 )\\). We think of the cdf as a function that takes some value \\(x\\) and produces a probability. In the case where \\(F(x)\\) is monotonically increasing, we could define \\(F^{-1}(p)\\) which takes a probability and tells us what value of \\(x\\) produces \\(F(x)=p\\). The quantile function of a distribution generalizes the inverse function to work similarly for non-decreasing functions by defining \\[F^{-1}(p) = \\min(x) \\textrm{ such that } F(x) \\ge p\\] Suppose that a point in the \\(xy\\)-plane is chosen at random from the interior of the unit circle, which is the circle centered at \\(\\{0,0\\}\\) with radius 1. Notice the probability that the chosen point belongs to a given region is proportional to the area of the region. Let the random variable \\(Z\\) represent the distance of the point to the origin. What is the median of the distribution? That is, find the value \\(z\\) such that \\(Pr(Z \\le z)=0.5\\). Again sketch the pdf and cdf of this distribution and reperesent the median on both graphs along with pertinent information showing that indeed the value you found is the median. What is the \\(75\\)th percentile? Follow your previous steps in part (a) and (b). Binomial Distribution Again we consider the binomial distribution but now with parameters \\(n=6\\) and probability of success \\(p=0.4\\). Find and graph the pdf and cdf of this distribution. What is the median of this distribution? What is the \\(75\\)th percentile? What is the \\(80\\)th percentile? 3.4 Bivariate Distributions We now consider the case of having two random variables. We can think of selecting an individual from a population and measuring two (or more!) variables. For example, we might select a NAU student and measure both their height and weight and we want to understand how those two measurements vary. It should be clear that there is a positive correlation (taller people also tend to weigh more) and we want to estabilish the mathematical framework to address questions such as this. In general we will consider the distribution of 2 or more random variables and we will call this the joint distribution. In the bivariate case, we will consider the joint distribution of \\(X\\) and \\(Y\\). 3.4.1 Bivariate Discrete We first consider the case where both variables are discrete. In this case, the bivariate distribution can be defined by simply defining the probabilities \\(f(x,y)=Pr(X=x, Y=y)\\). Your book likes to emphasize the notation of an \\((X,Y)\\) pair and writes these as \\(Pr\\Big( (X,Y) = (x,y)\\Big)\\), but I dislike so many parentheses. Consider the experiment of rolling two six-sided fair dice. Define the discrete random variable \\(X\\) as the number of ones we roll and \\(Y\\) as the number of sixes. Fill in the table of \\(f(x,y)\\) values. \\(f(x,y)\\) \\(Y=0\\) \\(Y=1\\) \\(Y=2\\) \\(X=0\\) \\(X=1\\) \\(X=2\\) Find \\(Pr( X\\ge 1 \\textrm{ and } Y \\ge 1 )\\) Suppose that \\(X\\) and \\(Y\\) have a discrete joint distribution for which the joint p.f. is defined as follows: \\[f(x,y) = \\begin{cases} c|x+y| &amp; \\textrm{ for } x \\in \\{−2,−1,0,1,2\\} \\\\ &amp; \\textrm{ and } y \\in \\{−2,−1,0,1,2\\} \\\\ 0 &amp; \\textrm{ otherwise. } \\end{cases}\\] Find the value of the constant \\(c\\). Find \\(Pr(X = 0 \\textrm{ and } Y =−2)\\). Find \\(Pr(X=1)\\) Find \\(Pr(|X−Y|≤1)\\) 3.4.2 Bivariate Continuous Two random variables \\(X\\) and \\(Y\\) have a bivariate continuous distribution if there exists a non-negative function \\(f(x,y)\\) such that for every subset \\(C\\) of the \\(xy\\)-plane \\[Pr\\Big[ (X,Y)\\in C\\Big] = \\iint_C f(x,y)\\] Unsurprisingly, the requirements for a function \\(f(x,y)\\) to be a joint pdf are: \\[f(x,y) \\ge 0 \\textrm{ for all } (x,y) \\textrm{ in } \\mathbb{R}^2\\] \\[\\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} f(x,y) \\, dx\\,dy = 1\\] Consider the case where we have the joint pdf \\[f(x,y) = 3x \\, \\cdot I(0\\le y \\le x \\le 1)\\] We could visualize this in 3-D but I find it easier to visualize the \\(xy\\)-plane and then let the height of the density function be represented by color. We might now ask question such as “What is the probability that \\(X&lt;.5\\)?” or “What is the probability that \\(Y&gt;X^2\\)? In both cases, we just need to integrate the density across the full area of interest. \\[\\begin{aligned} Pr( X \\le 0.5 ) &amp;= \\int_0^{0.5} \\int_0^x f(x,y) \\,dy\\,dx \\\\ &amp;= \\int_0^{0.5} \\int_0^x 3x \\,dy\\,dx \\\\ &amp;= \\int_0^{0.5} 3xy \\rvert_{y=0}^x \\,dx \\\\ &amp;= \\int_0^{0.5} 3x^2 \\,dx \\\\ &amp;= x^3 \\rvert_{x=0}^{0.5} \\\\ &amp;= 0.5^3 \\\\ &amp;= \\frac{1}{8} \\end{aligned}\\] Notice that you could ask Wolfram Alpha for this by using the following: Integrate[ Integrate[ 3*x, {y,0,x}], {x,0,0.5} ] Similarly we could ask \\[\\begin{aligned} Pr( Y &gt; X^2 ) &amp;= \\int_0^{0.5} \\int_{x^2}^x f(x,y) \\,dy\\,dx \\\\ &amp;= \\int_0^{0.5} \\int_{x^2}^x 3x \\,dy\\,dx \\\\ &amp;= \\int_0^{0.5} 3xy \\rvert_{y=x^2}^x \\,dx \\\\ &amp;= \\int_0^{0.5} 3x^2 - 3x^3 \\,dx \\\\ &amp;= x^3 - \\frac{3}{4}x^4 \\rvert_{x=0}^{0.5} \\\\ &amp;= \\frac{1}{2^3} - \\frac{3}{4}\\frac{1}{2^4} \\\\ &amp;= \\frac{1}{8} - \\frac{3}{64} \\\\ &amp;= \\frac{5}{64} = 0.078125 \\end{aligned}\\] and verify this via Wolfram… Integrate[ Integrate[ 3*x, {y,x^2,x}], {x,0,0.5} ] Suppose the random variables \\(X\\) and \\(Y\\) have joint pdf \\[f(x,y) = c x^2 y \\,\\cdot I(x^2 \\le y \\le 1)\\] Draw the support of this distribution by drawing the parabola \\(y=x^2\\) on the \\(xy\\)-plane and shade in the area for which \\(f(x,y)&gt;0\\). Denote this region as \\(D\\) Integrate \\(f(x,y)\\) over its support. That is, find \\(\\iint_D f(x,y) \\, dx\\,dy\\). What is the value of \\(c\\) so that \\(\\iint f(x,y) \\, dx\\,dy = 1\\)? Find \\(Pr( X&gt;0 \\textrm{ and } Y&gt;X )\\) by shading in the area of the \\(xy\\)-plane corresponding to this event and then integrating \\(f(x,y)\\) over this region. Suppose that we have random variables \\(X\\) and \\(Y\\) that measure the lifespan of two components in a electronic system. Their joint pdf is \\[f(x,y) = \\frac{1}{8} x e^{-(x+y)/2} \\,\\cdot I(x&gt;0,\\,y&gt;0)\\] Graph the support of this function along with contour lines or shading that indicates where the density is high and where is is near zero. Find \\(Pr(X&gt;1 \\textrm{ and } Y&gt;1)\\) The joint CDF of the bivariate distribution is defined as we would expect it: \\[F(x,y) = Pr( X\\le x, \\textrm{ and } Y\\le y)\\] "]
]
