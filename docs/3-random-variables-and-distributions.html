<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>STA 473 - Probability</title>
  <meta name="description" content="STA 473 - Probability">
  <meta name="generator" content="bookdown 0.5 and GitBook 2.6.7">

  <meta property="og:title" content="STA 473 - Probability" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="dereksonderegger/474" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="STA 473 - Probability" />
  
  
  

<meta name="author" content="Derek L. Sonderegger">


<meta name="date" content="2018-01-17">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="2-conditional-probability.html">
<link rel="next" href="4-expectations.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="1-introduction-to-probability.html"><a href="1-introduction-to-probability.html"><i class="fa fa-check"></i><b>1</b> Introduction to Probability</a><ul>
<li class="chapter" data-level="1.1" data-path="1-introduction-to-probability.html"><a href="1-introduction-to-probability.html#history-of-probability"><i class="fa fa-check"></i><b>1.1</b> History of Probability</a></li>
<li class="chapter" data-level="1.2" data-path="1-introduction-to-probability.html"><a href="1-introduction-to-probability.html#interpretations-of-probability"><i class="fa fa-check"></i><b>1.2</b> Interpretations of Probability</a></li>
<li class="chapter" data-level="1.3" data-path="1-introduction-to-probability.html"><a href="1-introduction-to-probability.html#experiments-and-events"><i class="fa fa-check"></i><b>1.3</b> Experiments and Events</a></li>
<li class="chapter" data-level="1.4" data-path="1-introduction-to-probability.html"><a href="1-introduction-to-probability.html#review-of-set-theory-ds-1.4"><i class="fa fa-check"></i><b>1.4</b> Review of Set Theory (D&amp;S 1.4)</a></li>
<li class="chapter" data-level="1.5" data-path="1-introduction-to-probability.html"><a href="1-introduction-to-probability.html#definition-of-probability-ds-1.5"><i class="fa fa-check"></i><b>1.5</b> Definition of Probability (D&amp;S 1.5)</a></li>
<li class="chapter" data-level="1.6" data-path="1-introduction-to-probability.html"><a href="1-introduction-to-probability.html#finite-sample-spaces-ds-1.6"><i class="fa fa-check"></i><b>1.6</b> Finite Sample Spaces (D&amp;S 1.6)</a></li>
<li class="chapter" data-level="1.7" data-path="1-introduction-to-probability.html"><a href="1-introduction-to-probability.html#ordered-counting-ds-1.7"><i class="fa fa-check"></i><b>1.7</b> Ordered Counting (D&amp;S 1.7)</a></li>
<li class="chapter" data-level="1.8" data-path="1-introduction-to-probability.html"><a href="1-introduction-to-probability.html#combinations-ds-1.8"><i class="fa fa-check"></i><b>1.8</b> Combinations (D&amp;S 1.8)</a></li>
<li class="chapter" data-level="1.9" data-path="1-introduction-to-probability.html"><a href="1-introduction-to-probability.html#multinomial-coefficients-ds-1.9"><i class="fa fa-check"></i><b>1.9</b> Multinomial Coefficients (D&amp;S 1.9)</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-conditional-probability.html"><a href="2-conditional-probability.html"><i class="fa fa-check"></i><b>2</b> Conditional Probability</a><ul>
<li class="chapter" data-level="2.1" data-path="2-conditional-probability.html"><a href="2-conditional-probability.html#defining-conditional-probability-ds-2.1"><i class="fa fa-check"></i><b>2.1</b> Defining Conditional Probability (D&amp;S 2.1)</a></li>
<li class="chapter" data-level="2.2" data-path="2-conditional-probability.html"><a href="2-conditional-probability.html#independence"><i class="fa fa-check"></i><b>2.2</b> Independence</a></li>
<li class="chapter" data-level="2.3" data-path="2-conditional-probability.html"><a href="2-conditional-probability.html#bayes-theorem"><i class="fa fa-check"></i><b>2.3</b> Bayes’ Theorem</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html"><i class="fa fa-check"></i><b>3</b> Random Variables and Distributions</a><ul>
<li class="chapter" data-level="3.1" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#defining-random-variables-and-discrete-distributions"><i class="fa fa-check"></i><b>3.1</b> Defining Random Variables and Discrete Distributions</a></li>
<li class="chapter" data-level="3.2" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#continuous-distributions"><i class="fa fa-check"></i><b>3.2</b> Continuous Distributions</a></li>
<li class="chapter" data-level="3.3" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#cumulative-distribution-function"><i class="fa fa-check"></i><b>3.3</b> Cumulative Distribution Function</a></li>
<li class="chapter" data-level="3.4" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#bivariate-distributions"><i class="fa fa-check"></i><b>3.4</b> Bivariate Distributions</a><ul>
<li class="chapter" data-level="3.4.1" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#bivariate-discrete"><i class="fa fa-check"></i><b>3.4.1</b> Bivariate Discrete</a></li>
<li class="chapter" data-level="3.4.2" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#bivariate-continuous"><i class="fa fa-check"></i><b>3.4.2</b> Bivariate Continuous</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#marginal-distributions"><i class="fa fa-check"></i><b>3.5</b> Marginal Distributions</a><ul>
<li class="chapter" data-level="3.5.1" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#discrete-case"><i class="fa fa-check"></i><b>3.5.1</b> Discrete Case</a></li>
<li class="chapter" data-level="3.5.2" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#continuous-case"><i class="fa fa-check"></i><b>3.5.2</b> Continuous Case</a></li>
<li class="chapter" data-level="3.5.3" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#independence-1"><i class="fa fa-check"></i><b>3.5.3</b> Independence</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#conditional-distributions"><i class="fa fa-check"></i><b>3.6</b> Conditional Distributions</a></li>
<li class="chapter" data-level="3.7" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#functions-of-random-variables"><i class="fa fa-check"></i><b>3.7</b> Functions of Random Variables</a><ul>
<li class="chapter" data-level="3.7.1" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#cdf-method"><i class="fa fa-check"></i><b>3.7.1</b> CDF Method</a></li>
<li class="chapter" data-level="3.7.2" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#pdf-method"><i class="fa fa-check"></i><b>3.7.2</b> pdf Method</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#multivariate-transformations"><i class="fa fa-check"></i><b>3.8</b> Multivariate Transformations</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-expectations.html"><a href="4-expectations.html"><i class="fa fa-check"></i><b>4</b> Expectations</a><ul>
<li class="chapter" data-level="4.1" data-path="4-expectations.html"><a href="4-expectations.html#expectation-of-a-rv"><i class="fa fa-check"></i><b>4.1</b> Expectation of a RV</a></li>
<li class="chapter" data-level="4.2" data-path="4-expectations.html"><a href="4-expectations.html#properties-of-expectations"><i class="fa fa-check"></i><b>4.2</b> Properties of Expectations</a></li>
<li class="chapter" data-level="4.3" data-path="4-expectations.html"><a href="4-expectations.html#variance"><i class="fa fa-check"></i><b>4.3</b> Variance</a></li>
<li class="chapter" data-level="4.4" data-path="4-expectations.html"><a href="4-expectations.html#moments-and-moment-generating-functions"><i class="fa fa-check"></i><b>4.4</b> Moments and Moment Generating Functions</a></li>
<li class="chapter" data-level="4.5" data-path="4-expectations.html"><a href="4-expectations.html#mean-vs-median"><i class="fa fa-check"></i><b>4.5</b> Mean vs Median</a></li>
<li class="chapter" data-level="4.6" data-path="4-expectations.html"><a href="4-expectations.html#covariance-and-correlation"><i class="fa fa-check"></i><b>4.6</b> Covariance and Correlation</a></li>
<li class="chapter" data-level="4.7" data-path="4-expectations.html"><a href="4-expectations.html#conditional-expectation"><i class="fa fa-check"></i><b>4.7</b> Conditional Expectation</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-common-distributions.html"><a href="5-common-distributions.html"><i class="fa fa-check"></i><b>5</b> Common Distributions</a><ul>
<li class="chapter" data-level="5.1" data-path="5-common-distributions.html"><a href="5-common-distributions.html#introduction"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="5-common-distributions.html"><a href="5-common-distributions.html#bernoulli-and-binomial"><i class="fa fa-check"></i><b>5.2</b> Bernoulli and Binomial</a></li>
<li class="chapter" data-level="5.3" data-path="5-common-distributions.html"><a href="5-common-distributions.html#hypergeometric"><i class="fa fa-check"></i><b>5.3</b> Hypergeometric</a></li>
<li class="chapter" data-level="5.4" data-path="5-common-distributions.html"><a href="5-common-distributions.html#poisson"><i class="fa fa-check"></i><b>5.4</b> Poisson</a></li>
<li class="chapter" data-level="5.5" data-path="5-common-distributions.html"><a href="5-common-distributions.html#geometric-and-negative-binomial"><i class="fa fa-check"></i><b>5.5</b> Geometric and Negative Binomial</a></li>
<li class="chapter" data-level="5.6" data-path="5-common-distributions.html"><a href="5-common-distributions.html#normal"><i class="fa fa-check"></i><b>5.6</b> Normal</a></li>
<li class="chapter" data-level="5.7" data-path="5-common-distributions.html"><a href="5-common-distributions.html#uniform"><i class="fa fa-check"></i><b>5.7</b> Uniform</a></li>
<li class="chapter" data-level="5.8" data-path="5-common-distributions.html"><a href="5-common-distributions.html#exponential-and-gamma"><i class="fa fa-check"></i><b>5.8</b> Exponential and Gamma</a></li>
<li class="chapter" data-level="5.9" data-path="5-common-distributions.html"><a href="5-common-distributions.html#beta"><i class="fa fa-check"></i><b>5.9</b> Beta</a></li>
<li class="chapter" data-level="5.10" data-path="5-common-distributions.html"><a href="5-common-distributions.html#bivariate-normal"><i class="fa fa-check"></i><b>5.10</b> Bivariate Normal</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="A-gamma-function.html"><a href="A-gamma-function.html"><i class="fa fa-check"></i><b>A</b> Gamma Function</a></li>
<li class="chapter" data-level="B" data-path="B-useful-series-results.html"><a href="B-useful-series-results.html"><i class="fa fa-check"></i><b>B</b> Useful Series Results</a><ul>
<li class="chapter" data-level="B.1" data-path="B-useful-series-results.html"><a href="B-useful-series-results.html#ex"><i class="fa fa-check"></i><b>B.1</b> <span class="math inline">\(e^x\)</span></a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="C-tedious-results.html"><a href="C-tedious-results.html"><i class="fa fa-check"></i><b>C</b> Tedious results</a><ul>
<li class="chapter" data-level="C.1" data-path="C-tedious-results.html"><a href="C-tedious-results.html#normal-distribution"><i class="fa fa-check"></i><b>C.1</b> Normal distribution</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">STA 473 - Probability</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="random-variables-and-distributions" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> Random Variables and Distributions</h1>
<div id="defining-random-variables-and-discrete-distributions" class="section level2">
<h2><span class="header-section-number">3.1</span> Defining Random Variables and Discrete Distributions</h2>
<p>A random variable is a <em>function</em> that takes outcomes in the sample space <span class="math inline">\(\mathcal{S}\)</span> and maps them to numeric values in <span class="math inline">\(\mathbb{R}\)</span>. Often times we abbreviate random variable as RV.</p>
<p>The idea is that random events such as flipping Heads, a medical test showing the patient has a disease, Chris Froome winning the Tour de France, or rolling a Leaning Jowler in <em>Pass the Pigs</em> are all random events but to do math on them, we need to turn them into numbers.</p>
<p>In cases where the sample space <span class="math inline">\(\mathcal{S}\)</span> is already numeric, the random variable can just be the identity, but in other cases, we might have to be more careful. For example, if my experiment is flipping a fair coin <span class="math inline">\(n=4\)</span> times, I could define the random variable <span class="math inline">\(X=\)</span> number of heads and <span class="math inline">\(X\)</span> could take on any of the values <span class="math inline">\(x \in \{0,1,2,3,4\}\)</span>. I could similarly define</p>
<p><span class="math display">\[Y= \begin{cases} 
  0 \;\;\; \textrm{ if number of heads } &lt; 2 \\ 
  1 \;\;\; \textrm{ if number of heads } &gt; 2 
\end{cases}\]</span></p>
<p>A RV function doesn’t have to be one-to-one and it doesn’t have to map to the entire set of real values.</p>
<p>Because events in the sample space <span class="math inline">\(\mathcal{S}\)</span> have an associated probability, then it is natural to define an event, say <span class="math inline">\(B\)</span> to be all the outcomes <span class="math inline">\(s \in \mathcal{S}\)</span> such that <span class="math inline">\(X(s) = x\)</span> and then define <span class="math inline">\(Pr(X=x) = Pr(B)\)</span>.</p>
<p>Notation: We will refer to the random variable using the capital letters, (e.g. <span class="math inline">\(X\)</span>, <span class="math inline">\(Y\)</span>, <span class="math inline">\(Z\)</span>, <span class="math inline">\(W\)</span>) and the possible values they take on using lower case letters. With this notation, the RV <span class="math inline">\(X\)</span> could take on values <span class="math inline">\(x \in \{0,1,2,3,4\}\)</span></p>
<ol style="list-style-type: decimal">
<li>We consider flipping a fair coin <span class="math inline">\(n=4\)</span> times.
<ol style="list-style-type: lower-alpha">
<li>What is the set of outcomes? As usual, we will define an event for each outcome.</li>
<li>What is the probability of each outcome?</li>
<li>For the RV <span class="math inline">\(X\)</span> defined above, what outcomes define the event <span class="math inline">\(B\)</span> such that <span class="math inline">\(s \in B \implies X(s)=2\)</span>?</li>
<li>What is <span class="math inline">\(Pr(B)\)</span>? Therefore what is <span class="math inline">\(Pr(X=2)\)</span>?</li>
<li>For the RV <span class="math inline">\(Y\)</span> defined above, what outcomes define the event <span class="math inline">\(A\)</span> such that <span class="math inline">\(s \in A \implies Y(s)=1\)</span>?</li>
<li>What is <span class="math inline">\(Pr(A)\)</span>? Therefore what is <span class="math inline">\(Pr(Y=1)\)</span>?</li>
</ol></li>
</ol>
<p>For each value that <span class="math inline">\(X\)</span> or <span class="math inline">\(Y\)</span> could take on, we could figure out the probability of the associated event. We define the random variables <em>distribution</em> as a description of what values the RV can take on and <span class="math inline">\(Pr(X \in C)\)</span>, for any interval <span class="math inline">\(C = \{c: a\le c \le b\}\)</span> for any <span class="math inline">\(a&lt;b\)</span>. This is actually a very awkward definition and we will examine more convenient ways to specify these same probabilities.</p>
<p><strong>Discrete</strong> random variables are RVs that can only take on a finite or countably infinite set of values.</p>
<p><strong>Continuous</strong> random variables are RVs that can take on an unaccountably infinite set of values.</p>
<p>We now define the <em>probability function</em> of a discrete RV <span class="math inline">\(X\)</span> as <span class="math display">\[f(x) = Pr(X = x)\]</span> and the closure of the set <span class="math inline">\(\{x: \textrm{ such that } f(x) &gt; 0\}\)</span> is referred to as the <em>support of <span class="math inline">\(X\)</span></em>. Notice that this function is defined for all <span class="math inline">\(x\in \mathbb{R}\)</span>, but for only a countable number of cases is <span class="math inline">\(f(x)&gt;0\)</span>.</p>
<ol start="2" style="list-style-type: decimal">
<li><p>Suppose that RV <span class="math inline">\(X\)</span> can take on the values <span class="math inline">\(\{x_1,x_2,\dots,x_K\}\)</span>. Prove that <span class="math display">\[\sum_{k=1}^K f(x_k) = 1\]</span></p></li>
<li><p><strong>Bernoulli Distribution</strong>. Suppose that the random variable <span class="math inline">\(W\)</span> takes on the values <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span> with the probabilities <span class="math inline">\(Pr(W=1) = p\)</span> and <span class="math inline">\(Pr(W=0) = 1-p\)</span>. Then we say that <span class="math inline">\(W\)</span> has a Bernoulli distribution with probability of success <span class="math inline">\(p\)</span>, which I might write as <span class="math inline">\(W \sim Bernoulli(p)\)</span>. Show that for any interval <span class="math inline">\(C =\{c: a\le c \le b\}\)</span> in <span class="math inline">\(\mathbb{R}\)</span>, you can find <span class="math inline">\(Pr(W \in C)\)</span>.</p></li>
<li><p><strong>Uniform Distribution on Integers</strong>. Suppose that we have integers <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> such that <span class="math inline">\(a &lt; b\)</span>. Suppose that the RV <span class="math inline">\(X\)</span> is equally likely to be any of the consecutive integers <span class="math inline">\(a,\dots,b\)</span>. What is <span class="math inline">\(f(x)\)</span>? <em>(Make sure your definition applies to any <span class="math inline">\(x\in\mathbb{R}\)</span>)</em></p></li>
<li><strong>Binomial Distribution</strong> Suppose that we have an experiment that consists of <span class="math inline">\(n\)</span> independent Bernoulli(<span class="math inline">\(p\)</span>) trials. We are interested in the distribution of <span class="math inline">\(X=\)</span> # of successful trials. That is <span class="math inline">\(X\sim Binomial(n,p)\)</span>.
<ol style="list-style-type: lower-alpha">
<li>For any integer <span class="math inline">\(x \in \{0,1,\dots,n\}\)</span>, what is <span class="math inline">\(Pr(X=x)\)</span>?</li>
<li>Define <span class="math inline">\(f(x)\)</span> for all values of <span class="math inline">\(x \in \mathbb{R}\)</span>.</li>
</ol></li>
<li><p>Give two examples of random variables which have Bernoulli(<span class="math inline">\(p=1/2\)</span>) distributions. These two RVs should not the same RVs, but they have the same distribution. That is to say, RVs have distributions, but distributions are not RVs.</p></li>
<li><p>Suppose that two fair six-sided dice are rolled and the RV of interest is the absolute value of the difference between the dice. Give the probability distribution along with an illuminating graph.</p></li>
<li><p>Suppose that a box contains 7 red balls and 3 green balls. If five balls are selected at random, without replacement, determine the probability function of <span class="math inline">\(X\)</span> where <span class="math inline">\(X\)</span> is the number of red balls selected.</p></li>
<li><p>Suppose that a random variable <span class="math inline">\(X\)</span> has a discrete distribution with the following probability function: <span class="math display">\[f(x) = \begin{cases} 
  \frac{c}{2^x} \;\textrm{ for } x = 0, 1, 2, \dots \\
  0 \;\;\;\; \textrm{otherwise}
\end{cases}\]</span> Find the value for <span class="math inline">\(c\)</span> that forces the requirement that <span class="math display">\[\sum_{x=0}^\infty f(x) = 1\]</span> <em>Hint: This is a particular power series. Go to any Calculus book (or internet) for an appropriate result. Make sure you introduce the result and show why the result applies to <span class="math inline">\(f(x)\)</span> in your solution.</em></p></li>
</ol>
<p>For the binomial distribution (and many distributions we will consider this semester), it would be nice to not have to calculate various probabilities by hand. Most mathematical software packages include some way to calculate these probabilities.</p>
<table>
<colgroup>
<col width="14%" />
<col width="85%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">System</th>
<th align="left">Documentation Link or site link</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Matlab</td>
<td align="left"><a href="https://www.mathworks.com/help/stats/working-with-probability-distributions.html" class="uri">https://www.mathworks.com/help/stats/working-with-probability-distributions.html</a></td>
</tr>
<tr class="even">
<td align="center">Mathematica</td>
<td align="left"><a href="http://reference.wolfram.com/language/howto/WorkWithStatisticalDistributions.html" class="uri">http://reference.wolfram.com/language/howto/WorkWithStatisticalDistributions.html</a></td>
</tr>
<tr class="odd">
<td align="center">R</td>
<td align="left"><a href="https://dereksonderegger.github.io/570L/3-statistical-tables.html" class="uri">https://dereksonderegger.github.io/570L/3-statistical-tables.html</a></td>
</tr>
<tr class="even">
<td align="center">Web App</td>
<td align="left"><a href="https://ismay.shinyapps.io/ProbApp/" class="uri">https://ismay.shinyapps.io/ProbApp/</a></td>
</tr>
</tbody>
</table>
<ol start="10" style="list-style-type: decimal">
<li>Each time I ride home or to work, there is a <span class="math inline">\(p=0.1\)</span> probability that I will get stopped by a train. Let <span class="math inline">\(X\)</span> be the number of times I’m stopped by the train in the next 10 trips. Assume that the probability I’m stopped on trip <span class="math inline">\(i\)</span> is independent of all other trips <span class="math inline">\(j\)</span>.
<ol style="list-style-type: lower-alpha">
<li>What is the distribution of <span class="math inline">\(X\)</span>? Remember, to specify the distribution by name, you must specify the name and the value of all parameters.</li>
<li>What is <span class="math inline">\(Pr(X =6)\)</span>?</li>
<li>What is <span class="math inline">\(Pr(X &lt; 6)\)</span>?</li>
<li>What is <span class="math inline">\(Pr(X \ge 6)\)</span>?</li>
</ol></li>
</ol>
</div>
<div id="continuous-distributions" class="section level2">
<h2><span class="header-section-number">3.2</span> Continuous Distributions</h2>
<p>We wish to define something similar to the probability function for continuous RVs. However, because there are an uncountable number of values that the RV could take on, we have to be careful and define probability on intervals of the form <span class="math inline">\([a,b]\)</span>. We define the <em>probability density function</em> (usually denoted pdf) as the function <span class="math inline">\(f(x)\)</span> such that <span class="math display">\[Pr( a \le X \le b) = \int_a^b f(x) dx\]</span></p>
<p>We further define the support of the distribution as the closure of the set <span class="math inline">\({x: f(x)&gt;0}\)</span>. This is the second time we’ve defined the support as the <em>closure</em> of the set. In the discrete case, it didn’t really matter, but here it does. If we define the pdf on the set <span class="math inline">\([0,1]\)</span> versus <span class="math inline">\((0,1)\)</span>, we want the support to contain the end points of the interval, that is the support is the closure of <span class="math inline">\((0,1)\)</span> which is <span class="math inline">\([0,1]\)</span>.</p>
<ol style="list-style-type: decimal">
<li><p>Show that, for any <span class="math inline">\(a\in\mathbb{R}\)</span>, <span class="math inline">\(Pr( X = a ) = 0\)</span> is consistent with this definition of <span class="math inline">\(f(x)\)</span>. <em>Hint: What happens as <span class="math inline">\(b-a\)</span> gets small? Take the limit.</em></p></li>
<li><p>Prove that <span class="math inline">\(\int_{-\infty}^\infty f(x) dx = 1\)</span>.</p></li>
</ol>
<p>Further notice that <span class="math inline">\(f(x)\ge0\)</span> for all <span class="math inline">\(x\)</span> because otherwise that would imply that there are events with negative probabilities.</p>
<ol start="3" style="list-style-type: decimal">
<li><strong>Continuous Uniform</strong>. Suppose that the random variable <span class="math inline">\(X\)</span> will be a randomly chosen real value from the interval <span class="math inline">\([a,b]\)</span>. Suppose that for any sub interval <span class="math inline">\(d=[d_1,d_2]\)</span> where <span class="math inline">\(a \le d_1 \le d_2 \le b\)</span>, that <span class="math inline">\(Pr( X \in d)\)</span> is proportional to the length of the interval <span class="math inline">\(d\)</span>. What is the pdf of the distribution of <span class="math inline">\(X\)</span>?</li>
</ol>
<p>It is unfortunate that your book doesn’t introduce indicator functions at this point in time. We can define the following: <span class="math display">\[I( \textrm{logical test} ) = 
    \begin{cases} 1 \;\;\; \textrm{ if logical test is true } \\
                  0 \;\;\; \textrm{ if logical test is false } \end{cases}\]</span></p>
<ol start="4" style="list-style-type: decimal">
<li>Suppose that the pdf of a random variable <span class="math inline">\(X\)</span> is <span class="math display">\[f(x) = \frac{4}{3}\left(1-x^3\right) \;I(0&lt;x&lt;1)\]</span>
<ol style="list-style-type: lower-alpha">
<li>Create a graph of the pdf.</li>
<li>Find <span class="math inline">\(Pr\left( X\le \frac{1}{2} \right)\)</span>?</li>
<li>Find <span class="math inline">\(Pr\left( \frac{1}{4} &lt; X &lt; \frac{3}{4} \right)\)</span></li>
<li>Find <span class="math inline">\(Pr\left( X &gt; \frac{3}{4} \right)\)</span></li>
</ol></li>
<li>Suppose the random variable <span class="math inline">\(X\)</span> has pdf <span class="math display">\[f(x) = c\cdot e^{-5x}I(x\ge 0) = \begin{cases} c\cdot e^{-5 x} \;\;\; \textrm{ if } x \ge 0 \\
                                      0   \;\;\;\;\;\;\;\;\;\;\;\;\; \textrm{otherwise}
                        \end{cases}\]</span>
<ol style="list-style-type: lower-alpha">
<li>What is the value of <span class="math inline">\(c\)</span> that makes this a valid pdf? That is, what value of <span class="math inline">\(c\)</span> makes this function integrate to 1?</li>
<li>Graph this function. What is <span class="math inline">\(f(0)\)</span>? Many students dislike that <span class="math inline">\(f(0)&gt;1\)</span> is greater than one. Why isn’t this a problem?</li>
<li>What is the probability <span class="math inline">\(Pr(X \le 0.5)\)</span>?</li>
</ol></li>
<li>Suppose that the pdf of the random variable <span class="math inline">\(X\)</span> is <span class="math display">\[f(x)= c \cdot x \;\;I(0\le x \le 3)\]</span>
<ol style="list-style-type: lower-alpha">
<li>What value of <span class="math inline">\(c\)</span> makes this a valid pdf?</li>
<li>Find a value of <span class="math inline">\(t\)</span> such that <span class="math inline">\(Pr( X \le t ) = 0.25)\)</span></li>
<li>Find a value of <span class="math inline">\(t\)</span> such that <span class="math inline">\(Pr( X &gt; t) = 0.5)\)</span></li>
</ol></li>
<li>Given the same random variable <span class="math inline">\(X\)</span> described in problem 3.2.5, consider the random variable <span class="math inline">\(Y\)</span> which is the simply the nearest integer to <span class="math inline">\(X\)</span>. What is the pf of <span class="math inline">\(Y\)</span>?</li>
</ol>
<p>We now have defined both the probability function (pf) for discrete random variables and probability density function (pdf) for continuous random variables. We now try to make a physics analogy to describe the difference between the two. In both cases we want to think about probability as physical mass.</p>
<p><strong>Discrete Case</strong> In this case, all the probability mass is concentrated at precise points. So we have little nuggets of probability mass, at discrete points. If I want to know, say <span class="math inline">\(Pr(X \le 3)\)</span> then we have to sum the probability across all the discrete locations such that <span class="math inline">\(X \le 3\)</span>.</p>
<p><strong>Continuous Case</strong> In this case, the probability mass is spread out across <span class="math inline">\(\mathbb{R}\)</span> and the concentration of mass is not uniform, some spots have more concentrated mass. In this case, we don’t have a definite amount of mass at a particular point, but we do have a description of how dense the mass is at any point. In this case if we want to know <span class="math inline">\(Pr(X \le 3)\)</span> then we have to break up <span class="math inline">\(\mathbb{R}\)</span> into a bunch of intervals and then combine the length of the interval along with information about the average density of each interval.</p>
<p><img src="Mathematical_Statistics_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>Each bar has some probability mass (mass is the length of the interval times the average density along the interval) and then we just sum up the bars. If we take the limit as we make the bars narrower, then we end up with <span class="math display">\[Pr(X \le 3) = \int_{-\infty}^3 f(x) \, dx\]</span></p>
<p>You might be a little freaked out because typically you would think about mass being the <em>area</em> or <em>volume</em> times the density, but we need to start in the 1-dimension case before we address the 2-dimension and 3-dimension cases.</p>
</div>
<div id="cumulative-distribution-function" class="section level2">
<h2><span class="header-section-number">3.3</span> Cumulative Distribution Function</h2>
<p>It is somewhat annoying to mathematically describe distributions in two different ways, depending on if the random variable is discrete or continuous.</p>
<p><span class="math display">\[\begin{aligned}
  Pr \left( X=x \right) = f(x) \;\;\; &amp; \textrm{ if X is discrete RV } \\
  Pr \left(a \le X \le b\right) = \int_a^b f(x)\,dx \;\;\; &amp; \textrm{ if X is continuous RV } 
  \end{aligned}\]</span></p>
<p>While the probability function and probability density function are both useful functions, it is mathematically convenient to have a mathematical description of the distribution that has the same interpretation regardless of if the distribution is discrete or continuous.</p>
<p><em>Definition</em>: For a random variable <span class="math inline">\(X\)</span> (notice we don’t specify if it is continuous or discrete) the <strong>Cumulative Distribution Function</strong> (CDF) is defined as <span class="math display">\[F(x) = Pr( X \le x )\]</span> Notice that this is defined for all <span class="math inline">\(x\)</span> in <span class="math inline">\(\mathbb{R}\)</span>.</p>
<ol style="list-style-type: decimal">
<li>Suppose that random variable <span class="math inline">\(X\)</span> has a Uniform distribution on the integers <span class="math inline">\(1,2,3,4,5\)</span>. These are the only values that <span class="math inline">\(X\)</span> can take on, and <span class="math display">\[f(x) = \frac{1}{5} \, \cdot I\Big(x \in \{1,2,\dots,5\}\Big)\]</span> Draw a graph of <span class="math inline">\(F(x)\)</span> and make sure the graph demonstrates:
<ol style="list-style-type: lower-alpha">
<li>That <span class="math inline">\(F(x)\)</span> is defined for all <span class="math inline">\(x \in \mathbb{R}\)</span>.</li>
<li>That <span class="math inline">\(F(x)\)</span> is a step function.</li>
<li>That <span class="math inline">\(F(x)\)</span> is continuous from the right. <em>That is, for <span class="math inline">\(\epsilon &gt; 0\)</span>, <span class="math inline">\(\lim_{\epsilon \to 0} F(x+\epsilon) = F(x)\)</span>.</em></li>
<li>That <span class="math inline">\(\lim_{x\to -\infty} F(x) = 0\)</span>.</li>
<li>That <span class="math inline">\(\lim_{x\to \infty} F(x) = 1\)</span>.</li>
</ol></li>
</ol>
<p>Define <span class="math inline">\(B_x = \left\{s \textrm{ such that } X(s) \le x\right\}\)</span>. Then we have for <span class="math inline">\(x_1 &lt; x_2\)</span> that <span class="math inline">\(B_{x_1} \subset B_{x_2}\)</span> and therefore: <span class="math display">\[\lim_{x\to -\infty} F(x) = \lim_{x\to -\infty} Pr\left( B_{x} \right) = Pr( \emptyset ) = 0\]</span> <span class="math display">\[\lim_{x\to  \infty} F(x) = \lim_{x\to  \infty} Pr\left( B_{x} \right) = Pr( \mathcal{S} ) = 1\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li><p>Show that <span class="math inline">\(F(x)\)</span> must be non-decreasing. <em>Notice this allows for <span class="math inline">\(F(x)\)</span> to be a flat function, but cannot decrease.</em></p></li>
<li><p>Suppose that the r.v. <span class="math inline">\(X\)</span> has a Binomial(<span class="math inline">\(n=5\)</span>, <span class="math inline">\(p=0.8\)</span>) distribution. Sketch the CDF.</p></li>
<li><strong>Geometric Distribution</strong> Suppose that we flip a biased coin that has probability of heads as <span class="math inline">\(p \in [0,1]\)</span>. Let the r.v. <span class="math inline">\(X\)</span> be the number of coin flips until the first head is observed.
<ol style="list-style-type: lower-alpha">
<li>What values could <span class="math inline">\(X\)</span> take? Mathematically, we say, what is the support of <span class="math inline">\(X\)</span>?</li>
<li>Is <span class="math inline">\(X\)</span> a continuous or discrete random variable?</li>
<li>Find the probability function, <span class="math inline">\(f(x)\)</span>.</li>
<li>Show that cumulative distribution function is <span class="math inline">\(F(x) = 1 - (1-p)^x\)</span>. <em>Hint: Geometric Series!</em></li>
</ol></li>
</ol>
<p>For continuous random variables it is relatively easy to go back and forth from the cdf to the pdf (assuming the integration and differentiation isn’t too hard). <span class="math display">\[ F(x) = \int_\infty ^x f(u) \, du\]</span> <span class="math display">\[ f(x) = \frac{d}{dx}\, F(x)\]</span></p>
<ol start="5" style="list-style-type: decimal">
<li><strong>Exponential Distribution</strong> (Warning! There are two ways to parameterize the Exponential Distribution. Before you look anything up, make sure it is using the same parameterization you are.) Suppose that we have a continuous random variable <span class="math inline">\(X\)</span> with pdf <span class="math display">\[f(x) = \beta e ^{-\beta x} \;\cdot I(x &gt; 0)\]</span>
<ol style="list-style-type: lower-alpha">
<li>Find the cdf function <span class="math inline">\(F(x)\)</span>.</li>
<li>For <span class="math inline">\(\beta=2\)</span> sketch the pdf and cdf.</li>
<li>On the pdf and cdf graphs, represent <span class="math inline">\(Pr(X &lt; 1)\)</span>. <em>In the pdf it will be some shaded area, in the cdf it is something else.</em></li>
</ol></li>
<li>Suppose that the cdf of a random variable <span class="math inline">\(X\)</span> is as follows: <span class="math display">\[ F(x) = \begin{cases} 
  0 \;\;\;          &amp; \textrm{ for } x \le 0 \\
  \frac{1}{9} x^2   &amp; \textrm{ for } 0 \le x \le 3 \\
  1                 &amp; \textrm{ for } x &gt; 3 
  \end{cases}\]</span>
<ol style="list-style-type: lower-alpha">
<li>Find the pdf function <span class="math inline">\(f(x)\)</span>.</li>
<li>Sketch the pdf and cdf.</li>
<li>On the pdf and cdf graphs, represent <span class="math inline">\(Pr( X \le 2 )\)</span>.</li>
</ol></li>
<li>Suppose that a point in the <span class="math inline">\(xy\)</span>-plane is chosen at random from the interior of the unit circle, which is the circle centered at <span class="math inline">\((0,0)\)</span> with radius 1. Notice the probability that the chosen point belongs to a given region is proportional to the area of the region. Let the random variable <span class="math inline">\(Z\)</span> represent the distance of the point to the origin.
<ol style="list-style-type: lower-alpha">
<li>Find and sketch the cdf of <span class="math inline">\(Z\)</span>.</li>
<li>Find and sketch the pdf of <span class="math inline">\(Z\)</span>.</li>
<li>On the pdf and cdf graphs, represent <span class="math inline">\(Pr( Z \le 0.5 )\)</span>.</li>
</ol></li>
</ol>
<p>We think of the cdf as a function that takes some value <span class="math inline">\(x\)</span> and produces a probability. In the case where <span class="math inline">\(F(x)\)</span> is monotonically increasing, we could define <span class="math inline">\(F^{-1}(p)\)</span> which takes a probability and tells us what value of <span class="math inline">\(x\)</span> produces <span class="math inline">\(F(x)=p\)</span>.</p>
<p>The <em>quantile function</em> of a distribution generalizes the inverse function to work similarly for non-decreasing functions by defining <span class="math display">\[F^{-1}(p) = \min(x) \textrm{ such that } F(x) \ge p\]</span></p>
<ol start="8" style="list-style-type: decimal">
<li>Suppose that a point in the <span class="math inline">\(xy\)</span>-plane is chosen at random from the interior of the unit circle, which is the circle centered at <span class="math inline">\(\{0,0\}\)</span> with radius 1. Notice the probability that the chosen point belongs to a given region is proportional to the area of the region. Let the random variable <span class="math inline">\(Z\)</span> represent the distance of the point to the origin.
<ol style="list-style-type: lower-alpha">
<li>What is the median of the distribution? That is, find the value <span class="math inline">\(z\)</span> such that <span class="math inline">\(Pr(Z \le z)=0.5\)</span>.</li>
<li>Again sketch the pdf and cdf of this distribution and represent the median on both graphs along with pertinent information showing that indeed the value you found is the median.</li>
<li>What is the <span class="math inline">\(75\)</span>th percentile? Follow your previous steps in part (a) and (b).</li>
</ol></li>
<li><strong>Binomial Distribution</strong> Again we consider the binomial distribution but now with parameters <span class="math inline">\(n=6\)</span> and probability of success <span class="math inline">\(p=0.4\)</span>.
<ol style="list-style-type: lower-alpha">
<li>Find and graph the pdf and cdf of this distribution.</li>
<li>What is the median of this distribution?</li>
<li>What is the <span class="math inline">\(75\)</span>th percentile? What is the <span class="math inline">\(80\)</span>th percentile?</li>
</ol></li>
</ol>
</div>
<div id="bivariate-distributions" class="section level2">
<h2><span class="header-section-number">3.4</span> Bivariate Distributions</h2>
<p>We now consider the case of having two random variables. We can think of selecting an individual from a population and measuring two (or more!) variables. For example, we might select a NAU student and measure both their height and weight and we want to understand how those two measurements vary. It should be clear that there is a positive correlation (taller people also tend to weigh more) and we want to establish the mathematical framework to address questions such as this.</p>
<p>In general we will consider the distribution of 2 or more random variables and we will call this the <em>joint distribution</em>. In the bivariate case, we will consider the joint distribution of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p>
<div id="bivariate-discrete" class="section level3">
<h3><span class="header-section-number">3.4.1</span> Bivariate Discrete</h3>
<p>We first consider the case where both variables are discrete. In this case, the bivariate distribution can be defined by simply defining the probabilities <span class="math inline">\(f(x,y)=Pr(X=x, Y=y)\)</span>. Your book likes to emphasize the notation of an <span class="math inline">\((X,Y)\)</span> pair and writes these as <span class="math inline">\(Pr\Big( (X,Y) = (x,y)\Big)\)</span>, but I dislike so many parentheses.</p>
<ol style="list-style-type: decimal">
<li>Consider the experiment of rolling two six-sided fair dice. Define the discrete random variable <span class="math inline">\(X\)</span> as the number of ones we roll and <span class="math inline">\(Y\)</span> as the number of sixes.
<ol style="list-style-type: lower-alpha">
<li><p>Fill in the table of <span class="math inline">\(f(x,y)\)</span> values.</p>
<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(f(x,y)\)</span></th>
<th align="center"><span class="math inline">\(Y=0\)</span></th>
<th align="center"><span class="math inline">\(Y=1\)</span></th>
<th align="center"><span class="math inline">\(Y=2\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(X=0\)</span></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(X=1\)</span></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(X=2\)</span></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
</tbody>
</table></li>
<li><p>Find <span class="math inline">\(Pr( X\ge 1 \textrm{ and } Y \ge 1 )\)</span></p></li>
</ol></li>
<li>Suppose that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> have a discrete joint distribution for which the joint p.f. is defined as follows: <span class="math display">\[f(x,y) = \begin{cases}
  c|x+y| &amp; \textrm{ for } x \in \{-2, -1, 0, 1, 2 \} \\
     &amp; \textrm{ and } y \in \{-2, -1, 0, 1, 2 \} \\
  0      &amp; \textrm{ otherwise. }
  \end{cases}\]</span>
<ol style="list-style-type: lower-alpha">
<li>Find the value of the constant <span class="math inline">\(c\)</span>.</li>
<li>Find <span class="math inline">\(Pr(X = 0 \textrm{ and } Y = -2)\)</span>.</li>
<li>Find <span class="math inline">\(Pr(X=1)\)</span></li>
<li>Find <span class="math inline">\(Pr(|X−Y| \le 1)\)</span></li>
</ol></li>
</ol>
</div>
<div id="bivariate-continuous" class="section level3">
<h3><span class="header-section-number">3.4.2</span> Bivariate Continuous</h3>
<p>Two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> have a bivariate continuous distribution if there exists a non-negative function <span class="math inline">\(f(x,y)\)</span> such that for every subset <span class="math inline">\(C\)</span> of the <span class="math inline">\(xy\)</span>-plane <span class="math display">\[Pr\Big[ (X,Y)\in C\Big] = \iint_C f(x,y)\]</span></p>
<p>Unsurprisingly, the requirements for a function <span class="math inline">\(f(x,y)\)</span> to be a joint pdf are: <span class="math display">\[f(x,y) \ge 0 \textrm{ for all } (x,y) \textrm{ in } \mathbb{R}^2\]</span> <span class="math display">\[\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} f(x,y) \, dx\,dy = 1\]</span></p>
<p>Consider the case where we have the joint pdf <span class="math display">\[f(x,y) = 3x \, \cdot I(0\le y \le x \le 1)\]</span> We could visualize this in 3-D but I find it easier to visualize the <span class="math inline">\(xy\)</span>-plane and then let the height of the density function be represented by color.</p>
<p><img src="Mathematical_Statistics_files/figure-html/unnamed-chunk-3-1.jpeg" width="672" /></p>
<p>We might now ask question such as “What is the probability that <span class="math inline">\(X \le 0.5\)</span>?” or “What is the probability that <span class="math inline">\(Y&gt;X^2\)</span>? In both cases, we just need to integrate the density across the full area of interest.</p>
<p><span class="math display">\[\begin{aligned} Pr( X \le 0.5 ) 
  &amp;= \int_0^{0.5} \int_0^x f(x,y) \,dy\,dx \\
  &amp;= \int_0^{0.5} \int_0^x 3x \,dy\,dx \\
  &amp;= \int_0^{0.5}  3xy \rvert_{y=0}^x \,dx \\
  &amp;= \int_0^{0.5}  3x^2 \,dx \\
  &amp;= x^3 \rvert_{x=0}^{0.5} \\
  &amp;= 0.5^3 \\
  &amp;= \frac{1}{8}
  \end{aligned}\]</span></p>
<p>Notice that you could ask Wolfram Alpha for this by using the following:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Integrate[ Integrate[ <span class="dv">3</span><span class="op">*</span>x, {y,<span class="dv">0</span>,x}], {x,<span class="dv">0</span>,<span class="fl">0.5</span>} ] ]</code></pre></div>
<p>Similarly we could ask <span class="math display">\[\begin{aligned} Pr( Y &gt; X^2 ) 
  &amp;= \int_0^{1} \int_{x^2}^x f(x,y) \,dy\,dx \\
  &amp;= \int_0^{1} \int_{x^2}^x 3x \,dy\,dx \\
  &amp;= \int_0^{1}  3xy \rvert_{y=x^2}^x \,dx \\
  &amp;= \int_0^{1}  3x^2 - 3x^3 \,dx \\
  &amp;= x^3 - \frac{3}{4}x^4 \rvert_{x=0}^{1} \\
  &amp;= \frac{1}{4}
  \end{aligned}\]</span></p>
<p>and verify this via Wolfram…</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Integrate[ Integrate[ <span class="dv">3</span><span class="op">*</span>x, {y,x<span class="op">^</span><span class="dv">2</span>,x}], {x,<span class="dv">0</span>,<span class="dv">1</span>} ] ]</code></pre></div>
<p>The joint CDF of the bivariate distribution is defined as we would expect it: <span class="math display">\[F(x,y) = Pr( X\le x, \textrm{ and } Y\le y)\]</span></p>
<p>Just as in the univariate case, <span class="math inline">\(F(x,y)\)</span> is non-decreasing <span class="math inline">\(x\)</span> for every fixed value of <span class="math inline">\(y\)</span> and as well as non-decreasing in <span class="math inline">\(y\)</span> for every fixed value of <span class="math inline">\(x\)</span>.</p>
<p><strong>Claims</strong>: Suppose that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> have joint cdf <span class="math inline">\(F(x,y)\)</span>. Define the cdf of <span class="math inline">\(X\)</span> as <span class="math inline">\(F_1(x)\)</span> and the cdf of <span class="math inline">\(Y\)</span> as <span class="math inline">\(F_2(y)\)</span>. Then <span class="math display">\[F_1(x) = \lim_{y\to\infty} F(x,y)\]</span> <span class="math display">\[F_2(y) = \lim_{x\to\infty} F(x,y)\]</span> <span class="math display">\[F(x,y) = \int_{-\infty}^x \int_{-\infty}^y f(u,v) \, dv\,du\]</span> <span class="math display">\[f(x,y) = \frac{\partial^2 F(x,y)}{\partial x \partial y} = \frac{\partial^2 F(x,y)}{\partial y \partial x}\]</span> assuming the integrals and derivatives exist.</p>
<ol start="3" style="list-style-type: decimal">
<li>Suppose the random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> have joint pdf <span class="math display">\[f(x,y) = c x^2 y \,\cdot I(x^2 \le y \le 1)\]</span>
<ol style="list-style-type: lower-alpha">
<li>Draw the support of this distribution by drawing the parabola <span class="math inline">\(y=x^2\)</span> on the <span class="math inline">\(xy\)</span>-plane and shade in the area for which <span class="math inline">\(f(x,y)&gt;0\)</span>. Denote this region as <span class="math inline">\(D\)</span></li>
<li>Integrate <span class="math inline">\(f(x,y)\)</span> over its support. That is, find <span class="math inline">\(\iint_D f(x,y) \, dx\,dy\)</span>.</li>
<li>What is the value of <span class="math inline">\(c\)</span> such that <span class="math inline">\(\iint f(x,y) \, dx\,dy = 1\)</span>?</li>
<li>Find <span class="math inline">\(Pr( X&gt;0 \textrm{ and } Y&gt;X )\)</span> by shading in the area of the <span class="math inline">\(xy\)</span>-plane corresponding to this event and then integrating <span class="math inline">\(f(x,y)\)</span> over this region.</li>
</ol></li>
<li>Suppose that we have random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> that measure the lifespan of two components in a electronic system. Their joint pdf is <span class="math display">\[f(x,y) = \frac{1}{8} x e^{-(x+y)/2} \,\cdot I(x&gt;0,\,y&gt;0)\]</span>
<ol style="list-style-type: lower-alpha">
<li>Graph the support of this function along with contour lines or shading that indicates where the density is high and where is is near zero.</li>
<li>Find <span class="math inline">\(Pr(X&gt;1 \textrm{ and } Y&gt;1)\)</span></li>
</ol></li>
<li>Suppose that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> have a continuous joint distribution for which the joint pdf is defined as follows: <span class="math display">\[f(x,y)=c y^2 \,\cdot I( 0\le x \le 2, \;\; 0 \le y \le 1)\]</span>
<ol style="list-style-type: lower-alpha">
<li>Graph the support of this pdf along with contour lines or shading that indicates the relative value of the density.</li>
<li>Determine the value of the constant <span class="math inline">\(c\)</span>.</li>
<li>Find <span class="math inline">\(Pr( X+Y \ge 2 )\)</span></li>
<li>Find <span class="math inline">\(Pr( X \le 1 )\)</span></li>
</ol></li>
<li>Suppose that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> have a continuous joint distribution for which the joint pdf is defined as follows: <span class="math display">\[f(x,y)=c (x^2 + y \,\cdot I( 0 \le y \le 1-x^2)\]</span>
<ol style="list-style-type: lower-alpha">
<li>Graph the support of this pdf along with contour lines or shading that indicates the relative value of the density.</li>
<li>Determine the value of the constant <span class="math inline">\(c\)</span>.</li>
<li>Find <span class="math inline">\(Pr( X \le 1/2 )\)</span></li>
<li>Find <span class="math inline">\(Pr( Y \le X+1 )\)</span></li>
</ol></li>
</ol>
</div>
</div>
<div id="marginal-distributions" class="section level2">
<h2><span class="header-section-number">3.5</span> Marginal Distributions</h2>
<p>We might be given a joint distribution via its cdf <span class="math inline">\(F(x,y)\)</span> or pf/pdf <span class="math inline">\(f(x,y)\)</span>. Often we want to be able to ignore one or the other random variables and just consider <span class="math inline">\(X\)</span> or <span class="math inline">\(Y\)</span>. So we want to understand how to take the joint distribution and obtain the <em>marginal</em> distributions of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. We call them marginal because we are taking the <span class="math inline">\(xy\)</span>-plane (or table) and pushing all the probability density (or actual probability) to the <span class="math inline">\(X\)</span> or <span class="math inline">\(Y\)</span> margins.</p>
<div id="discrete-case" class="section level3">
<h3><span class="header-section-number">3.5.1</span> Discrete Case</h3>
<p><strong>Example</strong> Suppose we have the discrete pf <span class="math display">\[f(x,y) = \frac{3-x-y}{8} \;\cdot I(x\in \{0,1\}, \;y\in \{0,1\}\]</span>. Then the table of values for <span class="math inline">\(f(x,y)\)</span> is</p>
<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(f(x,y)\)</span></th>
<th align="center"><span class="math inline">\(x=0\)</span></th>
<th align="center"><span class="math inline">\(x=1\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(y=0\)</span></td>
<td align="center"><span class="math inline">\(\frac{3}{8}\)</span></td>
<td align="center"><span class="math inline">\(\frac{2}{8}\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(y=1\)</span></td>
<td align="center"><span class="math inline">\(\frac{2}{8}\)</span></td>
<td align="center"><span class="math inline">\(\frac{1}{8}\)</span></td>
</tr>
</tbody>
</table>
<p>We can easily calculate that <span class="math inline">\(Pr( X = 0 )\)</span> by simply summing the column of <span class="math inline">\(x=0\)</span> probabilities. In general we could write <span class="math display">\[f_1(x) = \sum_{\textrm{all }y} f(x,y)\]</span> We can do the same for <span class="math inline">\(Y\)</span> and calculate <span class="math display">\[f_2(y) = \sum_{\textrm{all }x} f(x,y)\]</span></p>
<p>In the table, the just means summing across the rows and columns:</p>
<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(f(x,y)\)</span></th>
<th align="center"><span class="math inline">\(x=0\)</span></th>
<th align="center"><span class="math inline">\(x=1\)</span></th>
<th align="center"><strong>Total</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(y=0\)</span></td>
<td align="center"><span class="math inline">\(\frac{3}{8}\)</span></td>
<td align="center"><span class="math inline">\(\frac{2}{8}\)</span></td>
<td align="center"><span class="math inline">\(\mathbf{\frac{5}{8}}\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(y=1\)</span></td>
<td align="center"><span class="math inline">\(\frac{2}{8}\)</span></td>
<td align="center"><span class="math inline">\(\frac{1}{8}\)</span></td>
<td align="center"><span class="math inline">\(\mathbf{\frac{3}{8}}\)</span></td>
</tr>
<tr class="odd">
<td align="center"><strong>Total</strong></td>
<td align="center"><span class="math inline">\(\mathbf{\frac{5}{8}}\)</span></td>
<td align="center"><span class="math inline">\(\mathbf{\frac{3}{8}}\)</span></td>
<td align="center"></td>
</tr>
</tbody>
</table>
<p>Similarly we could see that <span class="math display">\[\begin{aligned} F_1(x) 
  &amp;= Pr(X\le x) \\
  &amp;= Pr(X\le x, Y=\textrm{ anything }) \\
  &amp;=  \sum_{u \le x} \sum_{\textrm{all }y} f(u,y)
  \end{aligned}\]</span></p>
</div>
<div id="continuous-case" class="section level3">
<h3><span class="header-section-number">3.5.2</span> Continuous Case</h3>
<p>In the continuous case, we have a similar relationship, but we want to be careful and define things first using the cdf.</p>
<p>Again we define the marginal cdf of <span class="math inline">\(X\)</span> as</p>
<p><span class="math display">\[\begin{aligned} F_1(x) 
  &amp;= Pr( X \le x ) \\
  &amp;= Pr( X \le x, Y=\textrm{anything}) \\
  &amp;= \int_{-\infty}^x \int_{-\infty}^{\infty} f(u,y) \;dy\,du
  \end{aligned}\]</span></p>
<p>But recall how we initially defined the pdf of the distribution. It was whatever function you had to integrate such that you happily got the cdf. So therefore by that definition</p>
<p><span class="math display">\[\begin{aligned} F_1(x) 
  &amp;= \int_{-\infty}^x \underbrace{ \int_{-\infty}^{\infty} f(u,y) \;dy }_{f_1(u)}\;\;du
  \end{aligned}\]</span></p>
<p>And we can now see that <span class="math display">\[f_1(x) = \int_{-\infty}^{\infty} f(x,y) \, dy\]</span> and similarly <span class="math display">\[f_2(y) = \int_{-\infty}^{\infty} f(x,y) \, dx\]</span></p>
<ol style="list-style-type: decimal">
<li>Recall the joint pdf <span class="math display">\[f(x,y) = \frac{21}{4} x^2 y \,\cdot I(x^2 \le y \le 1)\]</span>
<ol style="list-style-type: lower-alpha">
<li>Graph the joint pdf <span class="math inline">\(f(x,y)\)</span> by graphing the support of the distribution on the <span class="math inline">\(xy\)</span>-plane and</li>
<li>Find and graph the marginal pdf of <span class="math inline">\(X\)</span>, <span class="math inline">\(f_1(x)\)</span>.</li>
<li>Find and graph the marginal pdf of <span class="math inline">\(Y\)</span>, <span class="math inline">\(f_2(y)\)</span>.</li>
</ol></li>
<li>Suppose we have the continuous joint distribution <span class="math display">\[f(x,y) = \frac{3}{2} y^2 \,\cdot I(0\le x \le 2, 0 \le y \le 1)\]</span>
<ol style="list-style-type: lower-alpha">
<li>Graph the joint pdf <span class="math inline">\(f(x,y)\)</span> by graphing the support of the distribution on the <span class="math inline">\(xy\)</span>-plane and</li>
<li>Find and graph the marginal pdf of <span class="math inline">\(X\)</span>, <span class="math inline">\(f_1(x)\)</span>.</li>
<li>Find and graph the marginal pdf of <span class="math inline">\(Y\)</span>, <span class="math inline">\(f_2(y)\)</span>.</li>
</ol></li>
<li><strong>Indentical marginal distrubutions don’t imply identical joint distributions.</strong> Create two discrete joint distributions that are different but yet have the same marginal distributions. Feel free to define the joint distributions by two tables of values instead of working out some functional form for <span class="math inline">\(f(x,y)\)</span>.</li>
</ol>
</div>
<div id="independence-1" class="section level3">
<h3><span class="header-section-number">3.5.3</span> Independence</h3>
<p>Recall that we had defined that events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are independent if <span class="math inline">\(Pr(A\cap B) = Pr(A)Pr(B)\)</span>. Equivalently we can define that random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent if <span class="math inline">\(Pr( X \le x, Y \le y) = Pr(X\le x)Pr(Y\le y)\)</span> for all <span class="math inline">\(x,y\)</span>. This is equivalent to <span class="math display">\[F(x,y) = F_1(x) F_2(y)\]</span></p>
<p>In the discrete case, it is clear that<br />
<span class="math display">\[Pr( X \le x, Y \le y) = Pr(X\le x)Pr(Y\le y) \textrm{ for all } x,y\]</span> is equivalent to <span class="math display">\[Pr( X = x, Y=y) = Pr(X=x)Pr(Y=y) \textrm{ for all } x,y\]</span> and therefore we could use an equivalent criteria for independence that <span class="math display">\[f(x,y) = f_1(x)f_2(y)\]</span></p>
<p>In the continuous case, we see that if <span class="math inline">\(F(x,y) = F_1(x) F_2(y)\)</span> then <span class="math display">\[\begin{aligned}f(x,y) 
  &amp;= \frac{\partial}{\partial x \partial y} F(x,y) \\
  &amp;= \frac{\partial}{\partial x \partial y} F_1(x) F_2(y) \\
  &amp;= \frac{\partial}{\partial x } F_1(x) \frac{\partial}{\partial y} F_2(y) \\
  &amp;= f_1(x) f_2(y)
  \end{aligned}\]</span> There is nothing in this sequence of equalities that can’t be done in reverse so we see this is another equivalent criteria for independence.</p>
<p>The critical aspect of this definition of independence is that the joint cdf or pf/pdf factors into two functions <span class="math inline">\(h_1(x)\)</span> and <span class="math inline">\(h_2(y)\)</span>. Typically we think of these as the marginal distributions, but they might be off by some constant value and that is acceptable. That is to say, <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent if and only if <span class="math display">\[f(x,y) = h_1(x) h_2(y)\]</span> where <span class="math inline">\(f_1(x) = c_1 h_1(x)\)</span> and <span class="math inline">\(f_2(y) = c_2 h_2(y)\)</span> for some <span class="math inline">\(c_1\)</span> and <span class="math inline">\(c_2\)</span>. We note this just so that we can show two variables are independent without bothering to figure out how the integrating constant gets divided up between the two marginal distributions.</p>
<p><strong>Example</strong> Suppose that <span class="math display">\[f(x,y) = \frac{3}{16} xy^2 \, \cdot I(0\le x \le 2, \; 0 \le y \le 2)\]</span> Then it is clear that we could factor <span class="math inline">\(f(x,y)\)</span> into <span class="math display">\[f(x,y) = \underbrace{\frac{3}{16} x \cdot I(0\le x \le 2)}_{h_1(x)} \;\;\;\underbrace{y^2 \cdot I(0 \le y \le 2)}_{h_2(y)}\]</span> but I know that <span class="math inline">\(h_2(y)\)</span> isn’t exactly the marginal distribution of <span class="math inline">\(Y\)</span> because it doesn’t integrate to 1. I’m too lazy to figure out how I need to break the <span class="math inline">\(3/16\)</span> multiplier into two pieces and distribute it to make <span class="math inline">\(h_1(x)\)</span> and <span class="math inline">\(h_2(y)\)</span> into the marginals, but I know that I could do it if I had to.</p>
<p>Notice that the indicator function has to be split up and appropriately grouped into the <span class="math inline">\(h_1(x)\)</span> and <span class="math inline">\(h_2(y)\)</span> terms. If that cannot happen, then the variables cannot be independent.</p>
<p>Finally notice that if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, then for any functions <span class="math inline">\(g_1(\cdot)\)</span> and <span class="math inline">\(g_2(\cdot)\)</span> we have that there is <span class="math inline">\(C_1 = \{x : g_1(x) \le u \}\)</span> and <span class="math inline">\(C_2 = \{ y : g_2(y) \le v\}\)</span> and therefore <span class="math display">\[\begin{aligned} Pr\Big( g_1(X) \le u, g_2(Y) \le v \Big) 
  &amp;= Pr( X \in C_1, Y \in C_2 ) \\
  &amp;= Pr(X \in C_1) \; Pr(Y \in C_2) \\
  &amp;= Pr( g_1(X) \le v ) \; Pr( g_2(Y) \le u )
  \end{aligned}\]</span> and therefore separate functions of independent random variables are also independent.</p>
<ol start="4" style="list-style-type: decimal">
<li>Suppose we have the continuous joint distribution <span class="math display">\[f(x,y) = \frac{3}{2} y^2 \,\cdot I(0\le x \le 2, 0 \le y \le 1)\]</span>
<ol style="list-style-type: lower-alpha">
<li>Are <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> independent?</li>
<li>Are the events <span class="math inline">\(\{X&lt;1\}\)</span> and <span class="math inline">\(\{Y \ge 1\}\)</span> independent?</li>
</ol></li>
<li>Suppose that the joint pdf of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is <span class="math display">\[f(x,y) = \frac{15}{4} x^2 \; \cdot I(0\le y \le 1-x^2)\]</span>
<ol style="list-style-type: lower-alpha">
<li>Sketch the region of support of the joint distribution.</li>
<li>Determine the marginal pdfs of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</li>
<li>Are <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> independent? Justify your answer.</li>
</ol></li>
<li>Suppose that the amount of sugar (in grams) I add to my tea each day has a pdf <span class="math display">\[f(x) = \frac{3}{8} x^2 \; \cdot I(0 \le x \le 2)\]</span> Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be the amount added on two successive days and suppose that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent.
<ol style="list-style-type: lower-alpha">
<li>Find the joint pdf of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</li>
<li>Find <span class="math inline">\(Pr( X &lt; Y )\)</span>.</li>
<li>Find <span class="math inline">\(Pr( X + Y \le 2)\)</span>.</li>
</ol></li>
<li>Suppose that a point <span class="math inline">\((X,Y)\)</span> is chosen randomly from the unit disc <span class="math display">\[S=\{(x,y) : x^2 + y^2 \le 1\}\]</span>. <em>(In general, when we say something like “a point is chosen randomly”, we mean that <span class="math inline">\(f(x,y) = c\)</span> for some constant <span class="math inline">\(c\)</span> for all values of <span class="math inline">\((x,y)\)</span> in the support. In other words, “at random” means “follows a uniform distribution on the support.”)</em>
<ol style="list-style-type: lower-alpha">
<li>Determine the joint and marginal pdfs of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</li>
<li>Are <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> independent?</li>
</ol></li>
<li>Suppose that Alice and Barbara plan to meet at Macy’s Coffee shop. Each person will arrive randomly between 5 pm and 6 pm. Each person will arrive and wait for the other for at most 15 minutes. What is the probability that they will meet up? <em>Hint: Draw the region of support and sub-region where they meet up. Because the density is constant, you could skip the integration and just do some geometry, but be careful!</em></li>
</ol>
</div>
</div>
<div id="conditional-distributions" class="section level2">
<h2><span class="header-section-number">3.6</span> Conditional Distributions</h2>
<p>The last distribution we wish to derive from the joint distribution is that <em>conditional distribution</em>. Recall for events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> we had <span class="math display">\[Pr( A | B ) = \frac{ Pr( A \cap B ) }{ Pr( B ) } \;\;\textrm{ if }Pr(B) &gt; 0\]</span></p>
<p>This definition immediately leads to the definition of the condition pf in the discrete case:</p>
<p><span class="math display">\[g_1(x | y) = Pr( X=x | Y=y ) = \frac{ f(x,y) }{f_2(y)} \;\;\textrm{ if }f_2(y) &gt; 0\]</span> and <span class="math display">\[g_2(y | x) = Pr( Y=y | X=x ) = \frac{ f(x,y) }{f_1(x)} \;\;\textrm{ if }f_1(x) &gt; 0\]</span></p>
<p><strong>Example</strong> Suppose we have the discrete pf <span class="math display">\[f(x,y) = \frac{3-x-y}{8} \;\cdot I(x\in \{0,1\}, \;y\in \{0,1\}\]</span> The table of values for <span class="math inline">\(f(x,y)\)</span> is</p>
<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(f(x,y)\)</span></th>
<th align="center"><span class="math inline">\(x=0\)</span></th>
<th align="center"><span class="math inline">\(x=1\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(y=0\)</span></td>
<td align="center"><span class="math inline">\(\frac{3}{8}\)</span></td>
<td align="center"><span class="math inline">\(\frac{2}{8}\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(y=1\)</span></td>
<td align="center"><span class="math inline">\(\frac{2}{8}\)</span></td>
<td align="center"><span class="math inline">\(\frac{1}{8}\)</span></td>
</tr>
</tbody>
</table>
<p>We might be interested in <span class="math display">\[Pr(Y=1 | X=0) = g_2(1|0) = \frac{f(0,1)}{f_1(0)} = \frac{2/8}{5/8} = \frac{2}{5}\]</span></p>
<p>For the continuous case… we’ll just <em>cheat and define</em> it as what we want: <span class="math display">\[g_1(x | y) =  \frac{ f(x,y) }{f_2(y)} \;\;\textrm{ if }f_2(y) &gt; 0\]</span> and <span class="math display">\[g_2(y | x) =  \frac{ f(x,y) }{f_1(x)} \;\;\textrm{ if }f_1(x) &gt; 0\]</span></p>
<p>In graduate level probability courses, this gets a bit more treatment and using measure theory we could appropriately derive this, but that is too much for now.</p>
<p>In the continuous case, we should at least check that <span class="math inline">\(g_1(x|y)\)</span> is a valid pdf, though.</p>
<ol style="list-style-type: decimal">
<li>For continuous random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> and <span class="math inline">\(y\)</span> such that <span class="math inline">\(f_2(y) &gt; 0\)</span>, show that <span class="math inline">\(g_1(x|y)\)</span> is a valid pdf by showing that:
<ol style="list-style-type: lower-alpha">
<li><span class="math inline">\(g_1(x|y) \ge 0\)</span> for all <span class="math inline">\(x\)</span>.</li>
<li><span class="math inline">\(\int_{-\infty}^{\infty} g_1(x|y)\, dx = 1\)</span></li>
</ol></li>
<li><p>The population distribution of blood donors in the United States based on race/ethnicity and blood type as reported by the American Red Cross is given here:</p>
<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(f(x,y)\)</span></th>
<th align="center">O</th>
<th align="center">A</th>
<th align="center">B</th>
<th align="center">AB</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>White</strong></td>
<td align="center">36%</td>
<td align="center">32.2%</td>
<td align="center">8.8%</td>
<td align="center">3.2%</td>
</tr>
<tr class="even">
<td align="center"><strong>Black</strong></td>
<td align="center">7%</td>
<td align="center">2.9%</td>
<td align="center">2.5%</td>
<td align="center">0.5%</td>
</tr>
<tr class="odd">
<td align="center"><strong>Asian</strong></td>
<td align="center">1.7%</td>
<td align="center">1.2%</td>
<td align="center">1%</td>
<td align="center">0.3%</td>
</tr>
<tr class="even">
<td align="center"><strong>Other</strong></td>
<td align="center">1.5%</td>
<td align="center">0.8%</td>
<td align="center">0.3%</td>
<td align="center">0.1%</td>
</tr>
</tbody>
</table>
<ol style="list-style-type: lower-alpha">
<li>What is the probability that a randomly selected donor will be Asian and have Type O blood?</li>
<li>What is the probability that a randomly selected donor is white?</li>
<li>What is the probability that a randomly selected donor has Type A blood?</li>
<li>What is the probability that a donor will have Type A blood given that the donor is white?</li>
</ol></li>
<li>Suppose that continuous random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> have joint pdf <span class="math display">\[f(x,y) = e^{-x} \; \cdot I( 0 \le y \le x )\]</span> This might be an appropriate model for the number of minutes it takes my daughter to put on her socks and shoes. Here <span class="math inline">\(Y\)</span> is the amount of time it takes to put on her socks, and <span class="math inline">\(X-Y\)</span> is the takes to put the shoes over the socks, so the whole process takes <span class="math inline">\(X\)</span> minutes.
<ol style="list-style-type: lower-alpha">
<li>Derive the marginal distribution for the total time to put on her shoes and socks.</li>
<li>Derive the conditional distribution for how long it took to put on her socks given the total amount of time taken. <em>Notice this should be a function of the total amount of time taken!</em></li>
<li>Derive the conditional distribution of how long the total process took given the amount of time it took to put on her socks.</li>
<li>What is the probability the process takes more that 5 minutes given that she took 2 minutes to put on her socks?</li>
</ol></li>
<li>Suppose we have a joint pdf of <span class="math display">\[f(x,y) = c(x+y^2) \; \cdot I( 0\le x \le 1, \; 0 \le y \le 1 )\]</span>
<ol style="list-style-type: lower-alpha">
<li>Determine the value of <span class="math inline">\(c\)</span> such that this is a valid joint pdf.</li>
<li>Find the conditional pdf <span class="math inline">\(g_1(x|y)\)</span></li>
<li>Find <span class="math inline">\(Pr( X \le 1/2 \;|\; Y=1/2 )\)</span></li>
</ol></li>
<li><strong>Law of Total Probability</strong> Prove that
<ol style="list-style-type: lower-alpha">
<li>If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are discrete RVs then <span class="math display">\[f_1(x) = \sum_y g_1(x|y) f_2(y)\]</span></li>
<li>If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are continuous RVs then <span class="math display">\[f_1(x) = \int_{-\infty}^{\infty} g_1(x|y) f_2(y)\;dy\]</span></li>
</ol></li>
<li><strong>Bayes Theorem</strong> Prove that
<ol style="list-style-type: lower-alpha">
<li>If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are discrete RVs then <span class="math display">\[g_1(y|x) = \frac{ g_1(x|y) f_2(y)}{ f_1(x) } = \frac{ g_1(x|y) f_2(y)}{ \sum_y g_1(x|y)f_2(y) }\]</span></li>
<li>If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are continuous RVs then <span class="math display">\[g_1(y|x) = \frac{ g_1(x|y) f_2(y)}{ f_1(x) } = \frac{ g_1(x|y) f_2(y)}{ \int_{-\infty}^{\infty} g_1(x|y)f_2(y) \;dy}\]</span></li>
</ol></li>
</ol>
<p>Notice that it was arbitrary which variable we conditioned on and we could similarly define <span class="math inline">\(f_2(x)\)</span> and <span class="math inline">\(g_2(y|x)\)</span>. Furthermore, all the formulas in this section work if one variable is continuous and the other is discrete with just the appropriate switch between summation and integration.</p>
<ol start="7" style="list-style-type: decimal">
<li>We are interested in modeling the rate of hits on a web page server. Let <span class="math inline">\(X\)</span> be the number of page requests in a minute and <span class="math inline">\(R\)</span> is the <em>rate</em> of page requests. Suppose that we model <span class="math display">\[f_2(r) = e^{-r} \; \cdot I(r &gt; 0)\]</span> and <span class="math display">\[g_1(x | r) = \frac{(2r)^x}{x!} e^{-2r}\;\cdot I\Big(x\in\{0,1,2,\dots\}\Big)\]</span>
<ol style="list-style-type: lower-alpha">
<li>Find the marginal pf of <span class="math inline">\(X\)</span>.</li>
<li>Find the conditional pdf of <span class="math inline">\(R\)</span> given <span class="math inline">\(X\)</span>, <span class="math inline">\(g_2(r|x)\)</span>.</li>
<li>Consider the cases where <span class="math inline">\(X=1\)</span> and <span class="math inline">\(X=2\)</span>. For what values of <span class="math inline">\(r\)</span> is <span class="math inline">\(g_2(r|1)\)</span> larger than <span class="math inline">\(g_2(r|2)\)</span>?</li>
</ol></li>
<li><strong>Beta-Binomial</strong> Suppose we are considering a model for seedling success of a tree. We will randomly select an adult tree to harvest seeds from and then plant <span class="math inline">\(n=40\)</span> seeds and observe how many of them germinate. We will model the number of germinating seeds as <span class="math inline">\(X \sim Binomial(n=40, \pi)\)</span> where <span class="math inline">\(\pi\)</span> is the probability a randomly selected seed will germinate. However because seed fitness depends on the parent tree, we allow <span class="math inline">\(\pi \sim Beta(\alpha=2, \beta=2)\)</span>. The beta distribution has the pdf: <span class="math display">\[\begin{aligned} f_2(\pi) 
  &amp;= \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)} \pi^{\alpha-1}(1-\pi)^{\beta-1} \,\cdot I(0&lt;\pi&lt;1) \\
  \end{aligned}\]</span> where <span class="math inline">\(\alpha&gt;0, \beta&gt;0\)</span> and <span class="math inline">\(\Gamma(\cdot)\)</span> is the gamma function (see the appendix). Recall that the <span class="math inline">\(Binomial(n,\pi)\)</span> distribution has pf <span class="math display">\[ g_1(x|\pi) = {n \choose x} \pi^x (1-\pi)^{n-x} \;\cdot I(x\in \{0,1,\dots,n\})\]</span> For this problem, we have <span class="math inline">\(n=40\)</span> and <span class="math inline">\(\alpha=\beta=2\)</span> and we are interested in random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(\Pi\)</span>.
<ol style="list-style-type: lower-alpha">
<li>Graph the marginal distribution of <span class="math inline">\(\Pi\)</span>.</li>
<li>Find the marginal distribution of <span class="math inline">\(X\)</span>. <em>Hint: Distributions must integrate/sum to one, and therefore distributions lists are a convenient set of integration/sumation identities. In particular from our definition of the Beta Distribution we have:</em> <span class="math display">\[\int_0^1 \pi^{a-1}(1-\pi)^{b-1} \, d\pi = \frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}\]</span></li>
<li>Find the conditional pdf of <span class="math inline">\(\pi\)</span> given <span class="math inline">\(X\)</span>, <span class="math inline">\(g_2(\pi |x)\)</span>.</li>
<li>Consider the cases where <span class="math inline">\(X=20\)</span> and <span class="math inline">\(X=30\)</span>. For what values of <span class="math inline">\(\pi\)</span> is <span class="math inline">\(g_2(\pi|x=20)\)</span> larger than <span class="math inline">\(g_2(\pi|x=30)\)</span>?</li>
</ol></li>
</ol>
</div>
<div id="functions-of-random-variables" class="section level2">
<h2><span class="header-section-number">3.7</span> Functions of Random Variables</h2>
<p>Often we have a distribution for the random variable <span class="math inline">\(X\)</span> but actually want to know aobut the random variable <span class="math inline">\(Y=r(X)\)</span>.</p>
<p><strong>Example</strong> Suppose that we know the distribution of <span class="math inline">\(X\)</span>, the number of customers per hour at a resturant. However, what we really want to know about is <span class="math inline">\(Y=r(X) = 60 / X\)</span> which is the typical number of minutes between customers.</p>
<p><strong>Example</strong> Suppose that I know the distribution of <span class="math inline">\(X\)</span> which is the number of docter appointments necessary for treatment of some issue. But what I really care about is <span class="math inline">\(Y=200+100*X\)</span> which is the amount of money total that these appointments will cost.</p>
<div id="cdf-method" class="section level3">
<h3><span class="header-section-number">3.7.1</span> CDF Method</h3>
<p>Of the two methods we’ll investigate in this section, the CDF method is by far the most general, but can be somewhat annoying to implement.</p>
<p>The idea is that <span class="math display">\[G(y) = Pr(Y \le y) = Pr( r(X) \le y ) = 
  \begin{cases} \sum_{x:r(x) \le y} f(x) \\ \\
                \int_{x:r(x) \le y} f(x)
  \end{cases}\]</span> and the only annoying part is figuring out what set of <span class="math inline">\(x\)</span> values forces the <span class="math inline">\(r(x)\le y\)</span> inequality to hold.</p>
<p><strong>Example</strong> Suppose that <span class="math inline">\(X\)</span>, the number of customers per hour, follows a distribution with CDF <span class="math display">\[F(x) = 1-e^{-6x}\;\cdot I(x&gt;0)\]</span> We are actually interested in <span class="math inline">\(Y=r(X) = 60 / X\)</span>. Then <span class="math display">\[\begin{aligned} G(y)  
  &amp;= Pr( Y \le y ) \\
  &amp;= Pr( r(X) \le y )  \\ \\
  &amp;= Pr\left( \frac{60}{X} \le y \right)  \\\\
  &amp;= Pr\left( X \ge \frac{60}{y} \right) \\
  &amp;= 1 - F\left( \frac{60}{y} \right) \\
  &amp;= \left[ 1 - \left( 1 - e^{-6(\frac{60}{y})} \right)\right] \;\cdot I(0 &lt; y)\\
  &amp;= e^{\frac{-360}{y}} \;\cdot I(0 &lt; y)
  \end{aligned} \]</span></p>
<p><strong>Example</strong> Suppose that the random variable <span class="math inline">\(X\)</span> is the amount of time waiting on a child to eat her breakfast before leaving the house and has pdf <span class="math display">\[f(x) = \left( \frac{x}{50} \right) \;\cdot I(0 \le x \le 10)\]</span> but what I really want to know about is the proportion of breatkfast time remaining <span class="math inline">\(Y=r(X) = 1-\frac{X}{10}\)</span>. Furthermore, suppose that I want the pdf of <span class="math inline">\(Y\)</span> and not the CDF. So my plan is:</p>
<p><span class="math display">\[f(x) \stackrel{Integrate}{\Longrightarrow} F(x) = Pr(X\le x)\stackrel{CDF\; Method}{\Longrightarrow}
  Pr(Y\le y)=G(y) \stackrel{Differentiate}{\Longrightarrow} g(y)\]</span></p>
<p>Step 1: Find <span class="math inline">\(F(x)\)</span>. We will do this by integrating <span class="math inline">\(f(x)\)</span>. <span class="math display">\[F(x) = \int_{-\infty}^x f(u) \,du = \int_{0}^x \frac{u}{50} \,du = \frac{u^2}{100}\]</span></p>
<p>Step 2: CDF method to find <span class="math inline">\(G(y)\)</span>. <span class="math display">\[\begin{aligned} G(y) 
  &amp;= Pr( Y \le y ) \\
  &amp;= Pr\left( 1-\frac{X}{10} \le y \right) \\
  &amp;= Pr\left(X \ge 10(1-y)\right) \\
  &amp;= 1-F\Big( 10(1-y) \Big) \\
  &amp;= 1 - \frac{ \Big( 10(1-y) \Big)^2 }{100} \\
  &amp;= 1-(1-y)^2  \cdot I(y&gt;0)
  \end{aligned}\]</span></p>
<p>Step 3: Differentiate <span class="math inline">\(G(y)\)</span> to obtain <span class="math inline">\(g(y)\)</span> <span class="math display">\[g(y) = \frac{d}{dy}G(y) = \frac{d}{dy} \left[ 1-(1-y)^2 \right]  = 2(1-y) \; \cdot I(0 \le y \le 1)\]</span></p>
</div>
<div id="pdf-method" class="section level3">
<h3><span class="header-section-number">3.7.2</span> pdf Method</h3>
<p>In some cases it is possible to go straight from <span class="math inline">\(f(x)\)</span> to <span class="math inline">\(g(y)\)</span>. In particular if <span class="math inline">\(r(x)\)</span> is a one-to-one and differentiable. If <span class="math inline">\(r(x)\)</span> is one-to-one, then the inverse function <span class="math inline">\(r^{-1}(y)\)</span> exists.</p>
<p><strong>Theorem</strong> <em>Suppose <span class="math inline">\(X\)</span> is a continuous random variable with pdf <span class="math inline">\(f(x)\)</span> such that <span class="math inline">\(Pr(a\le X\le b)=1\)</span> for some (possibly infinite) boundaries <span class="math inline">\(a\)</span>,<span class="math inline">\(b\)</span>. If the transformation <span class="math inline">\(r(x)\)</span> is a one-to-one and differentiable and has range <span class="math inline">\((\alpha,\beta)\)</span> for <span class="math inline">\(x\in(a,b)\)</span>, then</em> <span class="math display">\[g(y) = f\Big( r^{-1}(y) \Big) \cdot \Bigg\vert \frac{d}{dy}r^{-1}(y) \Bigg\vert\]</span> <em>where <span class="math inline">\(r^{-1}(y)\)</span> is the inverse function of <span class="math inline">\(r(x)\)</span>.</em></p>
<p><strong>Proof</strong> Suppose that <span class="math inline">\(r(x)\)</span> is an increasing function. Then its inverse function is also an increasing function and <span class="math display">\[G(y) = Pr[Y \le y] = Pr[ r(X) \le y ] = Pr[ X \le r^{-1}(y) ] = F[ r^{-1}(y) ]\]</span></p>
<p>Next we can take the derivative and see</p>
<p><span class="math display">\[g(y) = \frac{d}{dy} G(y) = \frac{d}{dy} F[ r^{-1}(y) ] = f[ r^{-1}(y) ] \frac{d}{dy} r^{-1}(y)\]</span> Because <span class="math inline">\(r^{-1}\)</span> is an increasing function, its derivative is positive, <span class="math inline">\(\frac{d}{dy} r^{-1}(y) = \vert \frac{d}{dy} r^{-1}(y) \vert\)</span>.</p>
<p><span class="math display">\[g(y) = f[ r^{-1}(y) ] \Big\vert \frac{d}{dy}  r^{-1}(y) \Big\vert\]</span></p>
<p>Now suppose that <span class="math inline">\(r(x)\)</span> is a decreasing function. The its inverse is also decreasing and we have <span class="math display">\[G(y) = Pr[Y \le y] = Pr[ r(X) \le y ] = Pr[ X \ge r^{-1}(y) ] = 1 - F[ r^{-1}(y) ]\]</span> Again we take the derivative and see <span class="math display">\[g(y) = \frac{d}{dy} G(y) = - f[ r^{-1}(y) ] \frac{d}{dy} r^{-1}(y)\]</span> But because <span class="math inline">\(r^{-1}(y)\)</span> is decreasing, its derivative is negative and the negative terms would cancel. So we could write <span class="math display">\[g(y) = f[ r^{-1}(y) ] \Big\vert \frac{d}{dy}  r^{-1}(y) \Big\vert\]</span> and cover both the increasing and deceasing <span class="math inline">\(r(x)\)</span> cases.</p>
<p><strong>Example</strong> Suppose the rate of growth of some bacteria is a random variable <span class="math inline">\(X\)</span> with pdf <span class="math display">\[f(x) = 3(1-x)^2\;\cdot I(0 \le x \le 1)\]</span> but what we are interested in is the amount of bacteria at time <span class="math inline">\(t\)</span> which is a function of the initial amount of bacteria <span class="math inline">\(\nu&gt;0\)</span> and time <span class="math inline">\(t&gt;0\)</span>. Suppose that <span class="math inline">\(\nu\)</span> and <span class="math inline">\(t\)</span> are known values, we are interested in the pdf of <span class="math display">\[Y = \nu e^{Xt}\]</span></p>
<p>Step 1: Is this function a one-to-one increasing or decreasing function on the support of <span class="math inline">\(X\)</span>? Yes it is so we can use the pdf method.</p>
<p>Step 2: Find the inverse function. <span class="math display">\[Y = \nu e^{Xt}\]</span> <span class="math display">\[ \log{ \left( \frac{Y}{\nu} \right) } = Xt\]</span> <span class="math display">\[X = \frac{1}{t} \log{ \left( \frac{Y}{\nu} \right) }\]</span></p>
<p>Step 3: Find the deriviative with respect to the new variable. <span class="math display">\[\frac{d}{dy} \Bigg[ \frac{1}{t} \log{ \left( \frac{Y}{\nu} \right) } \Bigg]= \frac{1}{tY}\]</span></p>
<p>Step 4: Stick these into the formula <span class="math display">\[g(y) = f\left( \frac{1}{t} \log \left( \frac{Y}{\nu} \right) \right) \; \Bigg\vert \frac{1}{tY}\Bigg\vert  
       =3 \left( 1-\frac{1}{t} \log \left( \frac{Y}{\nu} \right) \right) \; \frac{1}{tY} \cdot I(\nu \le y \le \nu e^t)\]</span></p>
<p><strong>Problems</strong></p>
<ol style="list-style-type: decimal">
<li>Suppose that random variable <span class="math inline">\(X\)</span> has pdf <span class="math display">\[f(x) = 3x^2 \; \cdot I(0 &lt; x &lt; 1)\]</span> Further suppose that <span class="math inline">\(Y=1-X^2\)</span>.
<ol style="list-style-type: lower-alpha">
<li>Find the pdf of <span class="math inline">\(Y\)</span> using the CDF method.</li>
<li>Find the pdf of <span class="math inline">\(Y\)</span> using the pdf method.</li>
</ol></li>
<li>Suppose that random variable <span class="math inline">\(X\)</span> has pdf <span class="math display">\[f(x) = 1\cdot I(0 \le x \le 1)\]</span> Further suppose that <span class="math inline">\(Y=X(1-X)\)</span>. Find the pdf of <span class="math inline">\(Y\)</span>.
<ol style="list-style-type: lower-alpha">
<li>Graph the function <span class="math inline">\(y = x(1-x)\)</span> on the support of <span class="math inline">\(x\)</span>. Is this a one-to-one function?</li>
<li>What is the support of the random variable <span class="math inline">\(Y\)</span>?</li>
<li>On your graph of <span class="math inline">\(y=x(1-x)\)</span>, pick a value for <span class="math inline">\(y\)</span> and find the region(s) of <span class="math inline">\(x\)</span> such that <span class="math inline">\(x(1-x)\le y\)</span>. <em>Hint: quadratic equation!</em></li>
<li>Find the CDF of <span class="math inline">\(Y\)</span> by integrating over the appropriate region(s).</li>
<li>Find the pdf of <span class="math inline">\(Y\)</span> by differentiating.</li>
</ol></li>
<li><p>Suppose that random variable <span class="math inline">\(X\)</span> has pdf <span class="math display">\[f(x) = e^{-x} \; \cdot I(0 &lt; x)\]</span> Further suppose that <span class="math inline">\(Y=\Big\vert\sqrt{X}\Big\vert\)</span>. Find the pdf of <span class="math inline">\(Y\)</span>.</p></li>
<li><p>Suppose that <span class="math inline">\(X\)</span> has a uniform distribution on <span class="math inline">\((a, b)\)</span>. Let <span class="math inline">\(c&gt;0\)</span>. Show that <span class="math inline">\(Y= cX+d\)</span> has a uniform distribution on <span class="math inline">\((ca+d, cb+d)\)</span>.</p></li>
</ol>
</div>
</div>
<div id="multivariate-transformations" class="section level2">
<h2><span class="header-section-number">3.8</span> Multivariate Transformations</h2>
<p>Often we have a bivariate or multivariate distribution and we wish to consider some transformation.</p>
<p><strong>CDF Method</strong> Our most widely applicable method for dealing with transformations is to consider how the tranformation can affect the CDF.</p>
<p><strong>Example</strong> Suppose that we have <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> that are independent with joint pdf <span class="math display">\[\beta^2 e^{-\beta (x_1+x_2)} \; \cdot I(x_1&gt;0, x_2&gt;0)\]</span> where <span class="math inline">\(\beta&gt;0\)</span>. What we really care about is the sum of the two variables <span class="math inline">\(Y = X_1 + X_2\)</span>. Then we could figure out the cdf of <span class="math inline">\(Y\)</span> by <span class="math display">\[\begin{aligned} G(Y) 
  = Pr( Y \le y ) 
  = Pr( X_1 + X_2 \le y ) 
  \end{aligned}\]</span> <img src="Mathematical_Statistics_files/figure-html/unnamed-chunk-6-1.jpeg" width="672" /></p>
<p><span class="math display">\[\begin{aligned} G(Y) 
  &amp;= Pr( X_1 + X_2 \le y ) \\
  &amp;= \int_0^y \int_0^{y-x_1} f(x_1,x_2) \,dx_2 \,dx_1 \\
  &amp;= \int_0^y \int_0^{y-x_1} \beta^2 e^{-\beta (x_1+x_2)} \,dx_2 \,dx_1 \\
  &amp;= \vdots \\
  &amp;= 1 - e^{-\beta y} - \beta y e^{-\beta y}
  \end{aligned}\]</span></p>
<p>We can now use this CDF to obtain the pdf by differentiating <span class="math display">\[\begin{aligned} g(y) = \frac{d}{dy} G(y) 
  &amp;= \frac{d}{dy} \left[ 1 - e^{-\beta y} - \beta y e^{-\beta y} \right] \\
  &amp;= 0 + \beta e^{-\beta y} - \beta  e^{-\beta y} + \beta^2 y e^{-\beta y} \\
  &amp;= \beta^2 y e^{-\beta y} \; \cdot I(0 &lt; y)
  \end{aligned}\]</span></p>
<p><strong>Convolutions</strong> We are often interested in the sum of two independent random variables. Suppose that <span class="math inline">\(X_1\)</span> has density function <span class="math inline">\(f_1(x_1)\)</span> and <span class="math inline">\(X_2\)</span> has density function <span class="math inline">\(f_2(x_2)\)</span> and <span class="math inline">\(X_1\)</span> is independent of <span class="math inline">\(X_2\)</span>. Define <span class="math inline">\(Y=X_1 + X_2\)</span>. We say that the distribution of <span class="math inline">\(Y\)</span> is the <em>convolution</em> of the distributions of <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>.</p>
<ol style="list-style-type: decimal">
<li>Suppose that <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> have support along the entire real line. Show that the density function of <span class="math inline">\(Y=X_1+X_2\)</span> is <span class="math display">\[g(y) = \int_{-\infty} ^{\infty} f_1(y-u)f_2(u) \, du = \int_{-\infty} ^{\infty} f_1(z)f_2(y-z) \, dz\]</span>
<ol style="list-style-type: lower-alpha">
<li>Define the set <span class="math inline">\(A_y\)</span> which is the set of <span class="math inline">\((x_1,x_2)\)</span> values such that <span class="math inline">\(x_1+x_2\le y\)</span>.</li>
<li>Define <span class="math inline">\(G(y)\)</span> by setting up the integration across the set <span class="math inline">\(A_y\)</span>.</li>
<li>Recognize that in the set up of <span class="math inline">\(G(y)\)</span> we have indirectly figured out what <span class="math inline">\(g(y)\)</span> is.</li>
</ol></li>
</ol>
<p><strong>Example</strong> Suppose we have independent random variables <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> with uniform distributions on <span class="math inline">\((0,1)\)</span> and we are interested in <span class="math inline">\(Y=X_1+X_2\)</span>. Because <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are constrained to <span class="math inline">\((0,1)\)</span> then <span class="math inline">\(Y\in(0,2)\)</span>. Because <span class="math inline">\(f_1(x_1)=I(0 &lt; x_1 &lt; 1)\)</span> and <span class="math inline">\(f_2(x_2)=I(0 &lt; x_2 &lt; 1)\)</span> then <span class="math display">\[\begin{aligned} g(y)
  &amp;=\int_{-\infty}^\infty f_1(y-u) f_2(u) \, du \\
  &amp;=\int_{-\infty}^\infty I( 0 &lt; y-u &lt; 1) I(0 &lt; u &lt; 1) \, du
  \end{aligned}\]</span> We’ll look at this in cases, <span class="math inline">\(y \in (0,1)\)</span> and <span class="math inline">\(y\in (1,2)\)</span>. In the first case where <span class="math inline">\(y \in (0,1)\)</span>, then <span class="math inline">\(u \in (0,y)\)</span>. In the second case where <span class="math inline">\(y \in (1,2)\)</span> then <span class="math inline">\(u\in (y-1, 1)\)</span>.</p>
<p>In the first case we have <span class="math display">\[g(y) = \int_0^y 1 \, du = y \;\;\;\;\; \textrm{ if } y \in (0,1)\]</span> and in the second case <span class="math display">\[g(y) = \int_{y-1}^1 1 \, du = 2-y \;\;\;\;\; \textrm{ if } y \in (1,2)\]</span> so all together <span class="math display">\[g(y) = \begin{cases}
  0   &amp; \textrm{ if }\; y \le 0       \\
  y   &amp; \textrm{ if }\; 0 &lt; y \le 1   \\
  2-y &amp; \textrm{ if }\; 1 &lt; y &lt; 2   \\
  0   &amp; \textrm{ if }\; 2 \le y       
  \end{cases}\]</span> <img src="Mathematical_Statistics_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<ol start="2" style="list-style-type: decimal">
<li><p>Suppose that we have independent random variables <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> with uniform distributions on <span class="math inline">\(a,b\)</span> where <span class="math inline">\(a&lt;b\)</span>. <span class="math display">\[f_1(x_1) = \frac{1}{b-a} \;\cdot I(a &lt; x_1 &lt; b)\]</span> <span class="math display">\[f_2(x_2) = \frac{1}{b-a} \;\cdot I(a &lt; x_2 &lt; b)\]</span> Find the pdf of <span class="math inline">\(Y=X_1+X_2\)</span>.</p></li>
<li><p>Suppose that we have independent random variables <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> with pdfs <span class="math display">\[f_1(x_1) = \beta e^{-\beta x_1} \;\cdot I(x_1&gt;0)\]</span> <span class="math display">\[f_2(x_2) = \beta e^{-\beta x_2} \;\cdot I(x_2&gt;0)\]</span> where <span class="math inline">\(\beta &gt; 0\)</span>. Show that the pdf of <span class="math inline">\(Y=X_1 + X_2\)</span> is <span class="math display">\[g(y) = \beta^2 y e^{-\beta y} \; \cdot I(y&gt;0)\]</span> <em>Hint: Don’t ignore the indicator functions.</em></p></li>
<li><strong>Distribution of the Maximum of a sample.</strong> Suppose that <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> are independent random variables, and all have a common distribution with CDF <span class="math inline">\(F(x)\)</span> and pdf <span class="math inline">\(f(x)\)</span>. Let <span class="math inline">\(Y_n\)</span> be the maximum value of these <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> values. <em>For convenience, let <span class="math inline">\(X_i\)</span> be the <span class="math inline">\(i\)</span>th of these.</em>
<ol style="list-style-type: lower-alpha">
<li>If for some <span class="math inline">\(y\)</span> we have that <span class="math inline">\(Y_n \le y\)</span>, what inequality must hold for each <span class="math inline">\(X_i\)</span>?</li>
<li>Write <span class="math inline">\(Pr( Y_n \le y )\)</span> as some probability statement involving all <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> independent variables.</li>
<li>Write down the formula for <span class="math inline">\(G_n(y)\)</span>.</li>
<li>Write down the formula for <span class="math inline">\(g_n(y)\)</span>.</li>
</ol></li>
<li><strong>Distribution of the Minimum of a sample.</strong> Suppose that <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> are independent continuous random variables, and all have a common distribution with CDF <span class="math inline">\(F(x)\)</span> and pdf <span class="math inline">\(f(x)\)</span>. Let <span class="math inline">\(Y_1\)</span> be the minimum value of these <span class="math inline">\(X_i\)</span> values.
<ol style="list-style-type: lower-alpha">
<li>If for some <span class="math inline">\(y\)</span> we have that <span class="math inline">\(Y_1 &gt; y\)</span>, what inequality must hold for each <span class="math inline">\(X_i\)</span>?</li>
<li>Write <span class="math inline">\(1 - Pr( Y_1 &gt; y )\)</span> as some probability statement involving all <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> independent variables.</li>
<li>Write down the formula for <span class="math inline">\(G_1(y)\)</span>.</li>
<li>Write down the formula for <span class="math inline">\(g_1(y)\)</span>.</li>
</ol></li>
<li><p>Suppose that we have independent random variables <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> with uniform distributions on <span class="math inline">\(a,b\)</span> where <span class="math inline">\(a&lt;b\)</span>. <span class="math display">\[f_1(x_1) = \frac{1}{b-a} \;\cdot I(a \le x_1 \le b)\]</span> <span class="math display">\[f_2(x_2) = \frac{1}{b-a} \;\cdot I(a \le x_2 \le b)\]</span> Find the pdf of the maximum of <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>.</p></li>
<li><p>Suppose that we have independent random variables <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> with pdfs <span class="math display">\[f_1(x_1) = \beta e^{-\beta x_1} \;\cdot I(x_1&gt;0)\]</span> <span class="math display">\[f_1(x_2) = \beta e^{-\beta x_2} \;\cdot I(x_2&gt;0)\]</span> where <span class="math inline">\(\beta &gt; 0\)</span>. Find the pdf of the minimum of <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>.</p></li>
</ol>
<p><strong>pdf method</strong> Just as in the univariate case, we might ask if there is a way to just go straight from the joint pdf to the pdf of the variable of interest. The answer is yes provided, we convert the distribution of <span class="math inline">\(X_1,X_2,\dots,X_n\)</span> into <span class="math inline">\(Y_1,Y_2,\dots,Y_n\)</span> via transformations <span class="math inline">\(r_1(),r_2(), \dots,r_n()\)</span> where <span class="math display">\[y_1 = r_1(x_1,x_2,\dots,x_n)\]</span> <span class="math display">\[y_2 = r_2(x_1,x_2,\dots,x_n)\]</span> <span class="math display">\[\vdots\]</span> <span class="math display">\[y_n = r_n(x_1,x_2,\dots,x_n)\]</span> and these functions are invertable so that there exists <span class="math display">\[x_1 = s_1(y_1, y_2, \dots, y_n)\]</span> <span class="math display">\[x_2 = s_2(y_1, y_2, \dots, y_n)\]</span> <span class="math display">\[\vdots\]</span> <span class="math display">\[x_n = s_n(y_1, y_2, \dots, y_n)\]</span></p>
<p>For notational compactness, denote <span class="math inline">\(s_1 = s_1(y_1, y_2, \dots, y_n)\)</span>. Then <span class="math display">\[g(y_1, y_2, \dots, y_n) = f(s_1, s_2, \dots, s_n) \cdot \big\vert J \big\vert\]</span> where <span class="math inline">\(|J|\)</span> is the absolute value of the determinant <span class="math display">\[J = \det \left[ \begin{array}{ccc} 
\frac{\partial s_1}{\partial y_1}   &amp; \dots  &amp; \frac{\partial s_1}{\partial y_n}\\
\vdots                              &amp; \ddots &amp; \vdots \\
\frac{\partial s_n}{\partial y_1}   &amp; \dots  &amp; \frac{\partial s_n}{\partial y_n}\\
\end{array}\right]\]</span></p>
<p><strong>Example</strong><br />
Suppose that we have independent random variables <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> with pdfs <span class="math display">\[f_1(x_1) = \beta e^{-\beta x_1} \;\cdot I(x_1&gt;0)\]</span> <span class="math display">\[f_2(x_2) = \beta e^{-\beta x_2} \;\cdot I(x_2&gt;0)\]</span> where <span class="math inline">\(\beta &gt; 0\)</span>. We will find the joint pdf of <span class="math inline">\(Y_1 = \frac{X_1}{X_1+X_2}\)</span> and <span class="math inline">\(Y_2= X_1+X_2\)</span>.</p>
<p>In this case <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are positive values, so <span class="math inline">\(Y_1 \in (0,1)\)</span> and <span class="math inline">\(Y_2 &gt; 0\)</span>. Doing the backsolving for <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> we have <span class="math display">\[x_1 = y_1 y_2\]</span> <span class="math display">\[x_2 = y_2 ( 1-y_1 )\]</span> and therefore <span class="math display">\[J = \det \left[ \begin{array}{cc} 
y_2 &amp; y_1 \\
-y_2 &amp; 1-y_1
\end{array}\right] = y_2(1-y_1) +y_1 y_2 = y_2\]</span></p>
<p>Therefore <span class="math display">\[\begin{aligned} g(y_1, y_2) 
  &amp;= f\Big( y_1 y_2,\; y_2(1-y_1) \Big)\,\cdot \big\vert J \big\vert \\
  &amp;= f_1( y_1 y_2) \, f_2( y_2(1-y_1) ) \vert y_2 \vert \\
  &amp;= \beta e^{-\beta y_1 y_2} \beta e^{-\beta y_2(1-y_2)} \\
  &amp;= \beta^2 e^{-\beta y_2} \; I(y_2 &gt; 0) \; I(0 &lt; y_1 &lt; 1) 
  \end{aligned}\]</span> Notice that this pdf is easily broken into the marginals of <span class="math inline">\(Y_1\)</span> and <span class="math inline">\(Y_2\)</span> and we see that we have confirmed our calculation for the pdf of <span class="math inline">\(Y_2\)</span> and conveniently discovered that <span class="math inline">\(Y_1\)</span> has a <span class="math inline">\(Uniform(0,1)\)</span> distribution.</p>
<ol start="8" style="list-style-type: decimal">
<li><p>Suppose we have random variables <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> with joint distribution <span class="math display">\[f(x_1, x_2) = 2(1-x_1)\; I(0\le x_1 \le 1) I(0\le x_2 \le 1)\]</span> Find the pdf of <span class="math inline">\(Y_2=X_1X_2\)</span>. <em>Because the pdf method of transformation requires having a <span class="math inline">\(Y_1\)</span> variable, we will consider a second random variable that is convenient, <span class="math inline">\(Y_1 = X_1\)</span>.</em></p></li>
<li>Suppose that <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> have a continuous joint distribution with pdf <span class="math display">\[f(x_1, x_2) = 8 x_1 x_2 \,\cdot I(0 &lt; x_1&lt; x_2 &lt; 1)\]</span> Define the random variables <span class="math inline">\(Y_1=X_1/X_2\)</span> and <span class="math inline">\(Y_2=X_2\)</span>
<ol style="list-style-type: lower-alpha">
<li>Find the joint pdf of <span class="math inline">\(Y_1\)</span> and <span class="math inline">\(Y_2\)</span>.</li>
<li>Show that <span class="math inline">\(Y_1\)</span> and <span class="math inline">\(Y_2\)</span> are independent.</li>
</ol></li>
</ol>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="2-conditional-probability.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="4-expectations.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/dereksonderegger/edit/master/474/03_Random_Variables.Rmd",
"text": "Edit"
},
"download": [["Probability.pdf", "PDF"], ["Probability.epub", "EPUB"]],
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
